## Stochastic Neighbor Embedding

Stochastic Neighbor Embedding (SNE) is a dimensionality reduction
technique used to project high-dimensional data into a
lower-dimensional space (often 2D or 3D) while preserving local
neighborhoods of points. It is particularly popular for visualization
tasks, helping to reveal clusters or groupings among similar
points. Key characteristics include:

- Unsupervised: It does not require labels, relying on similarity or
  distance metrics among data points.
- Probabilistic framework: Pairwise distances in the original space
  are interpreted as conditional probabilities, which SNE attempts to
  replicate in the lower-dimensional space.
- Common for exploratory data analysis: Especially useful for
  high-dimensional datasets such as images, text embeddings, or
  genetic data.


### Statistical Rationale

The core idea behind SNE is to preserve local neighborhoods of each
point in the data:

1. For each point $x_i$ in the high-dimensional space, SNE defines a
   conditional probability $p_{j|i}$ that represents how likely
   $x_j$ is a neighbor of $x_i$.
2. The probability $p_{j|i}$ is modeled using a Gaussian distribution
   centered on $x_i$:

   $$
   p_{j|i} = \frac{\exp\left(- \| x_i - x_j \|^2 / 2 \sigma_i^2\right)}{\sum_{k \neq i} \exp\left(- \| x_i - x_k \|^2 / 2 \sigma_i^2\right)},
   $$

   where $\sigma_i$ is a variance parameter controlling the
   neighborhood size.
3. Each point $x_i$ is mapped to a lower-dimensional counterpart
   $y_i$, and a corresponding probability $q_{j|i}$ is defined
   similarly in that space.
4. The objective function minimizes the Kullbackâ€“Leibler (KL)
   divergence between the high-dimensional and low-dimensional
   conditional probabilities, encouraging a faithful representation of
   local neighborhoods.


### t-SNE Variation

The t-SNE (t-distributed Stochastic Neighbor Embedding) addresses two
main issues in the original formulation of SNE:

- The crowding problem: In high dimensions, pairwise distances tend to
  spread out; in 2D or 3D, they can crowd together. t-SNE uses a
  Student t-distribution (with one degree of freedom) in the
  low-dimensional space, which has heavier tails than a Gaussian.
- Symmetric probabilities: t-SNE symmetrizes probabilities
  $p_{ij} = (p_{j|i} + p_{i|j}) / (2N)$, simplifying computation.
  

The Student t-distribution for low-dimensional similarity is given by:

$$
q_{ij} = \frac{\bigl(1 + \| y_i - y_j \|^2 \bigr)^{-1}}{\sum_{k \neq l} \bigl(1 + \| y_k - y_l \|^2 \bigr)^{-1}}.
$$

This heavier tail ensures that distant points are not forced too
close, thus reducing the crowding effect.


### Supervised Variation

Although SNE and t-SNE are fundamentally unsupervised, it is possible
to integrate label information. In a supervised variant, distances
between similarly labeled points may be reduced (or differently
weighted), and additional constraints can be imposed to promote class
separation in the lower-dimensional embedding. These approaches can
help when partial label information is available and you want to blend
supervised and unsupervised insights.


### Demonstration with a Subset of the NIST Digits Data

Below is a brief example in Python using t-SNE on a small subset of
the MNIST digits (which is itself a curated subset of the original
NIST data).


```{python}
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

mnist = fetch_openml('mnist_784', version=1)
X = mnist.data[:2000]
y = mnist.target[:2000]

tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', 
            init='random', random_state=42)
X_embedded = tsne.fit_transform(X)

# Create a separate scatter plot for each digit to show a legend
plt.figure()
digits = np.unique(y)
for digit in digits:
    idx = (y == digit)
    plt.scatter(
        X_embedded[idx, 0],
        X_embedded[idx, 1],
        label=f"Digit {digit}",
        alpha=0.5
    )
plt.title("t-SNE on a Subset of MNIST Digits (by class)")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.legend()
plt.show()
```

In the visualization:

+ Points belonging to the same digit typically cluster together.
+ Ambiguous or poorly written digits often end up bridging two
  clusters.
+ Some digits, such as 3 and 5, may be visually similar and can appear
  partially overlapping in the 2D space.
