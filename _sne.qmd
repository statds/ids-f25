## Stochastic Neighbor Embedding

Stochastic Neighbor Embedding (SNE) is a dimensionality reduction
technique used to project high-dimensional data into a
lower-dimensional space (often 2D or 3D) while preserving local
neighborhoods of points. It is particularly popular for visualization
tasks, helping to reveal clusters or groupings among similar
points. Key characteristics include:

- Unsupervised: It does not require labels, relying on similarity or
  distance metrics among data points.
- Probabilistic framework: Pairwise distances in the original space
  are interpreted as conditional probabilities, which SNE attempts to
  replicate in the lower-dimensional space.
- Common for exploratory data analysis: Especially useful for
  high-dimensional datasets such as images, text embeddings, or
  genetic data.


### Statistical Rationale

SNE aims to construct a low-dimensional representation that preserves
the local neighborhood relationships observed in the original
high-dimensional data. The method achieves this by matching
probabilistic similarities between points across the two spaces.

1. For each point $x_i$ in the high-dimensional space, SNE defines a
   conditional probability $p_{j|i}$ that measures how likely $x_j$
   would be chosen as a neighbor of $x_i$. The probability is modeled
   as

   $$
   p_{j|i} =
   \frac{\exp\!\left(-\|x_i - x_j\|^2 / 2\sigma_i^2\right)}
        {\sum_{k \neq i}\exp\!\left(-\|x_i - x_k\|^2 / 2\sigma_i^2\right)},
   $$

   where the bandwidth $\sigma_i$ controls the neighborhood size and
   is typically determined to match a specified perplexity.

1. Each high-dimensional point $x_i$ is mapped to a low-dimensional
   coordinate $y_i$, from which a corresponding similarity
   distribution is defined:

   $$
   q_{j|i} =
   \frac{\exp\!\left(-\|y_i - y_j\|^2\right)}
        {\sum_{k \neq i}\exp\!\left(-\|y_i - y_k\|^2\right)}.
   $$

1. Denote by $P_i = \{p_{j|i}\}_{j \neq i}$ and
   $Q_i = \{q_{j|i}\}_{j \neq i}$ the conditional probability
   distributions for point $i$ in the high- and low-dimensional
   spaces, respectively. 
   The optimal embedding $\{y_i\}$ minimizes the Kullback–Leibler (KL)
   divergence between the two conditional distributions:

   $$
   C = \sum_i \text{KL}(P_i \| Q_i)
     = \sum_i \sum_j p_{j|i} \log\!\frac{p_{j|i}}{q_{j|i}}.
   $$

   This optimization encourages points that are close in the
   high-dimensional space to remain close in the lower-dimensional
   map, thus preserving local structure.


The asymmetry of the KL divergence in SNE plays a central role in how
local structure is preserved. Because the divergence
$\text{KL}(P_i \| Q_i)$ penalizes situations where a true neighbor
$x_j$ (with high $p_{j|i}$) is assigned a low similarity $q_{j|i}$ in
the low-dimensional map, the optimization strongly discourages breaking
apart neighborhoods that exist in the original space. In contrast,
pairs of distant points with small $p_{j|i}$ values contribute little to
the objective even if they are mapped close together. This asymmetry
biases the optimization toward accurately reproducing local
relationships rather than global geometry, emphasizing cluster fidelity
over large-scale distances. While this focus on local preservation is
desirable for visualizing complex data manifolds, it can also cause
points from different neighborhoods to become compressed together, a
phenomenon known as the *crowding problem*.


The later *t*-SNE algorithm addresses this issue by symmetrizing
the joint probabilities and replacing the Gaussian kernel in the
low-dimensional space with a Student-$t$ distribution to allow for
heavier tails and better separation of clusters.


### t-SNE Variation


The t-distributed Stochastic Neighbor Embedding (t-SNE) modifies SNE
to overcome two major limitations: the crowding problem and the
asymmetry of conditional probabilities. These refinements improve both
optimization stability and visual interpretability of the resulting
low-dimensional map.


The crowding problem arises because, in high-dimensional spaces,
pairwise distances are more evenly distributed than can be represented
in two or three dimensions. As a result, points that are moderately
distant in the original space may become crowded together in the
low-dimensional map. To mitigate this issue, t-SNE replaces the
Gaussian kernel in the low-dimensional space with a Student-$t$
distribution with one degree of freedom, which has heavier tails and
allows distant points to remain farther apart.


To simplify computation and ensure that similarity is mutual, t-SNE
also defines symmetric joint probabilities as
$p_{ij} = (p_{j|i} + p_{i|j}) / (2N)$, producing a symmetric
similarity matrix.

The corresponding similarity in the low-dimensional space is defined
as

$$
q_{ij} =
\frac{\bigl(1 + \|y_i - y_j\|^2\bigr)^{-1}}
     {\sum_{k \neq l}\bigl(1 + \|y_k - y_l\|^2\bigr)^{-1}}.
$$

t-SNE minimizes the Kullback–Leibler divergence between the joint
probabilities $\{p_{ij}\}$ and $\{q_{ij}\}$ with respect to the
embedding coordinates $\{y_i\}$:

$$
C = \sum_{i}\sum_{j} p_{ij}\log\!\frac{p_{ij}}{q_{ij}}.
$$

By combining symmetric similarities and heavy-tailed distributions,
t-SNE preserves local structure while providing enough flexibility to
separate distant clusters in the low-dimensional representation.


### Supervised Variation

While SNE and t-SNE are fundamentally unsupervised methods, they can
be extended to incorporate available label information. In supervised
or semi-supervised variants, the similarity structure is adjusted so
that points sharing the same label are drawn closer together, whereas
points from different classes are pushed apart through modified
distance weights or additional penalty terms in the loss function.
These extensions preserve the local manifold structure of the data
while simultaneously promoting class separation in the
low-dimensional embedding. Such hybrid formulations are particularly
useful when partial label information is available and the goal is to
combine supervised guidance with unsupervised neighborhood discovery.


### An Example with the NIST Digits Data

Below is a brief example using t-SNE on a small subset of the
MNIST digits, which is itself a curated subset of the original NIST
handwritten digits data. This example illustrates how t-SNE projects
high-dimensional image data—each digit represented by a 784-dimensional
vector—onto a two-dimensional space. The resulting visualization reveals
clusters corresponding to different digits, providing an intuitive view
of how similar handwriting patterns group together in the embedding
space.


```{python}
#| label: fig-tsne-mnist
#| fig-cap: |
#|   t-SNE visualization of a subset of MNIST digits.
#|   Points belonging to the same digit cluster together,
#|   while visually similar digits such as 3 and 5 show
#|   partial overlap in the 2D embedding.
#| fig-format: svg

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
X = mnist.data[:2000]
y = mnist.target[:2000].astype(int)

tsne = TSNE(
    n_components=2,
    perplexity=30,
    learning_rate='auto',
    init='random',
    random_state=42
)
X_embedded = tsne.fit_transform(X)

plt.figure()
digits = np.unique(y)
for digit in digits:
    idx = (y == digit)
    plt.scatter(
        X_embedded[idx, 0],
        X_embedded[idx, 1],
        label=f"Digit {digit}",
        alpha=0.5
    )
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.legend()
plt.tight_layout()
```


As shown in Figure @fig:tsne-mnist, digits with similar
handwriting styles tend to cluster together, forming visually distinct
regions in the embedding space. Overlaps occur where digits share
structural features, such as 3 and 5 or 4 and 9, reflecting the
inherent ambiguity in handwritten data. A few scattered points often
correspond to atypically written digits that t-SNE places between
clusters. Overall, the visualization provides an interpretable
two-dimensional summary of the high-dimensional digit images,
illustrating how t-SNE preserves local similarity while maintaining
global diversity in the data.

