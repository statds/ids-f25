<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Supervised Learning – Introduction to Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./11-unsupervised.html" rel="next">
<link href="./09-classification.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-supervised.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Project Management</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducible Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python Refreshment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-manipulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Data Exploration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-supervised.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./90-advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Advanced Topics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./95-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">10.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="header-section-number">10.1.1</span> Supervised Learning</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning"><span class="header-section-number">10.1.2</span> Unsupervised Learning</a></li>
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning"><span class="header-section-number">10.1.3</span> Reinforcement Learning</a></li>
  </ul></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="header-section-number">10.2</span> Decision Trees</a>
  <ul class="collapse">
  <li><a href="#recursive-partition-algorithm" id="toc-recursive-partition-algorithm" class="nav-link" data-scroll-target="#recursive-partition-algorithm"><span class="header-section-number">10.2.1</span> Recursive Partition Algorithm</a></li>
  <li><a href="#search-space-for-possible-splits" id="toc-search-space-for-possible-splits" class="nav-link" data-scroll-target="#search-space-for-possible-splits"><span class="header-section-number">10.2.2</span> Search Space for Possible Splits</a></li>
  <li><a href="#metrics" id="toc-metrics" class="nav-link" data-scroll-target="#metrics"><span class="header-section-number">10.2.3</span> Metrics</a></li>
  <li><a href="#ames-housing-example" id="toc-ames-housing-example" class="nav-link" data-scroll-target="#ames-housing-example"><span class="header-section-number">10.2.4</span> Ames Housing Example</a></li>
  </ul></li>
  <li><a href="#gradient-boosted-models" id="toc-gradient-boosted-models" class="nav-link" data-scroll-target="#gradient-boosted-models"><span class="header-section-number">10.3</span> Gradient-Boosted Models</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">10.3.1</span> Introduction</a></li>
  <li><a href="#gradient-boosting-process" id="toc-gradient-boosting-process" class="nav-link" data-scroll-target="#gradient-boosting-process"><span class="header-section-number">10.3.2</span> Gradient Boosting Process</a></li>
  <li><a href="#boosted-trees-with-ames-housing" id="toc-boosted-trees-with-ames-housing" class="nav-link" data-scroll-target="#boosted-trees-with-ames-housing"><span class="header-section-number">10.3.3</span> Boosted Trees with Ames Housing</a></li>
  <li><a href="#xgboost-extreme-gradient-boosting" id="toc-xgboost-extreme-gradient-boosting" class="nav-link" data-scroll-target="#xgboost-extreme-gradient-boosting"><span class="header-section-number">10.3.4</span> XGBoost: Extreme Gradient Boosting</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="header-section-number">10.4</span> Random Forest</a>
  <ul class="collapse">
  <li><a href="#random-forest-algorithm" id="toc-random-forest-algorithm" class="nav-link" data-scroll-target="#random-forest-algorithm"><span class="header-section-number">10.4.1</span> Random Forest Algorithm</a></li>
  <li><a href="#randomforestclassifier-parameters" id="toc-randomforestclassifier-parameters" class="nav-link" data-scroll-target="#randomforestclassifier-parameters"><span class="header-section-number">10.4.2</span> RandomForestClassifier Parameters</a></li>
  <li><a href="#random-forests-in-python-ames-data" id="toc-random-forests-in-python-ames-data" class="nav-link" data-scroll-target="#random-forests-in-python-ames-data"><span class="header-section-number">10.4.3</span> Random Forests in Python (Ames Data)</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">10.4.4</span> Further Reading</a></li>
  </ul></li>
  <li><a href="#synthetic-minority-over-sampling-technique-smote" id="toc-synthetic-minority-over-sampling-technique-smote" class="nav-link" data-scroll-target="#synthetic-minority-over-sampling-technique-smote"><span class="header-section-number">10.5</span> Synthetic Minority Over-sampling Technique (SMOTE)</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2"><span class="header-section-number">10.5.1</span> Introduction</a></li>
  <li><a href="#smote" id="toc-smote" class="nav-link" data-scroll-target="#smote"><span class="header-section-number">10.5.2</span> SMOTE</a></li>
  <li><a href="#example-created-synthetic-dataset" id="toc-example-created-synthetic-dataset" class="nav-link" data-scroll-target="#example-created-synthetic-dataset"><span class="header-section-number">10.5.3</span> Example: Created Synthetic Dataset</a></li>
  <li><a href="#variations-and-extensions-of-smote" id="toc-variations-and-extensions-of-smote" class="nav-link" data-scroll-target="#variations-and-extensions-of-smote"><span class="header-section-number">10.5.4</span> Variations and Extensions of SMOTE</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10.5.5</span> Conclusion</a></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings"><span class="header-section-number">10.5.6</span> Further Readings</a></li>
  </ul></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="header-section-number">10.6</span> Naive Bayes</a>
  <ul class="collapse">
  <li><a href="#theoretical-basis" id="toc-theoretical-basis" class="nav-link" data-scroll-target="#theoretical-basis"><span class="header-section-number">10.6.1</span> Theoretical basis</a></li>
  <li><a href="#types-of-naive-bayes" id="toc-types-of-naive-bayes" class="nav-link" data-scroll-target="#types-of-naive-bayes"><span class="header-section-number">10.6.2</span> Types of Naive Bayes</a></li>
  <li><a href="#naive-bayesian-application-nyc-crash-data" id="toc-naive-bayesian-application-nyc-crash-data" class="nav-link" data-scroll-target="#naive-bayesian-application-nyc-crash-data"><span class="header-section-number">10.6.3</span> Naive Bayesian application: NYC Crash Data</a></li>
  </ul></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks"><span class="header-section-number">10.7</span> Neural Networks</a>
  <ul class="collapse">
  <li><a href="#structure-of-a-neural-network" id="toc-structure-of-a-neural-network" class="nav-link" data-scroll-target="#structure-of-a-neural-network"><span class="header-section-number">10.7.1</span> Structure of a Neural Network</a></li>
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation"><span class="header-section-number">10.7.2</span> Mathematical Formulation</a></li>
  <li><a href="#network-diagram" id="toc-network-diagram" class="nav-link" data-scroll-target="#network-diagram"><span class="header-section-number">10.7.3</span> Network Diagram</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">10.7.4</span> Activation Functions</a></li>
  <li><a href="#training-neural-networks" id="toc-training-neural-networks" class="nav-link" data-scroll-target="#training-neural-networks"><span class="header-section-number">10.7.5</span> Training Neural Networks</a></li>
  <li><a href="#regularization-and-overfitting" id="toc-regularization-and-overfitting" class="nav-link" data-scroll-target="#regularization-and-overfitting"><span class="header-section-number">10.7.6</span> Regularization and Overfitting</a></li>
  <li><a href="#example-a-simple-feed-forward-network" id="toc-example-a-simple-feed-forward-network" class="nav-link" data-scroll-target="#example-a-simple-feed-forward-network"><span class="header-section-number">10.7.7</span> Example: A Simple Feed-Forward Network</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">10.1</span> Introduction</h2>
<p>Machine Learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed. At its core, machine learning algorithms aim to identify patterns in data and use those patterns to make decisions or predictions.</p>
<p>Machine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type differs in the data it uses and the learning tasks it performs, addressing addresses different tasks and problems. Supervised learning aims to predict outcomes based on labeled data, unsupervised learning focuses on discovering hidden patterns within the data, and reinforcement learning centers around learning optimal actions through interaction with an environment.</p>
<p>Let’s define some notations to introduce them:</p>
<ul>
<li><p><span class="math inline">\(X\)</span>: A set of feature vectors representing the input data. Each element <span class="math inline">\(X_i\)</span> corresponds to a set of features or attributes that describe an instance of data.</p></li>
<li><p><span class="math inline">\(Y\)</span>: A set of labels or rewards associated with outcomes. In supervised learning, <span class="math inline">\(Y\)</span> is used to evaluate the correctness of the model’s predictions. In reinforcement learning, <span class="math inline">\(Y\)</span> represents the rewards that guide the learning process.</p></li>
<li><p><span class="math inline">\(A\)</span>: A set of possible actions in a given context. In reinforcement learning, actions <span class="math inline">\(A\)</span> represent choices that can be made in response to a given situation, with the goal of maximizing a reward.</p></li>
</ul>
<section id="supervised-learning" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">10.1.1</span> Supervised Learning</h3>
<p>Supervised learning is the most widely used type of machine learning. In supervised learning, we have both feature vectors <span class="math inline">\(X\)</span> and their corresponding labels <span class="math inline">\(Y\)</span>. The objective is to train a model that can predict <span class="math inline">\(Y\)</span> based on <span class="math inline">\(X\)</span>. This model is trained on labeled examples, where the correct outcome is known, and it adjusts its internal parameters to minimize the error in its predictions, which occurs as part of the cross-validation process.</p>
<p>Key tasks in supervised learning include:</p>
<ul>
<li>Classification: Assigning data points to predefined categories or classes.</li>
<li>Regression: Predicting a continuous value based on input data.</li>
</ul>
<p>In supervised learning, the data consists of both feature vectors <span class="math inline">\(X\)</span> and labels <span class="math inline">\(Y\)</span>, namely, <span class="math inline">\((X, Y)\)</span>.</p>
</section>
<section id="unsupervised-learning" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="unsupervised-learning"><span class="header-section-number">10.1.2</span> Unsupervised Learning</h3>
<p>Unsupervised learning involves learning patterns from data without any associated labels or outcomes. The objective is to explore and identify hidden structures in the feature vectors <span class="math inline">\(X\)</span>. Since there are no ground-truth labels <span class="math inline">\(Y\)</span> to guide the learning process, the algorithm must discover patterns on its own. This is particularly useful when subject matter experts are unsure of common properties within a data set.</p>
<p>Common tasks in unsupervised learning include:</p>
<ul>
<li><p>Clustering: Grouping similar data points together based on certain features.</p></li>
<li><p>Dimension Reduction: Simplifying the input data by reducing the number of features while preserving essential patterns.</p></li>
</ul>
<p>In unsupervised learning, the data consists solely of feature vectors <span class="math inline">\(X\)</span>.</p>
</section>
<section id="reinforcement-learning" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="reinforcement-learning"><span class="header-section-number">10.1.3</span> Reinforcement Learning</h3>
<p>Reinforcement learning involves learning how to make a sequence of decisions to maximize a cumulative reward. Unlike supervised learning, where the model learns from a static dataset of labeled examples, reinforcement learning involves an agent that interacts with an environment by taking actions <span class="math inline">\(A\)</span>, receiving feedback in the form of rewards <span class="math inline">\(Y\)</span>, and learning over time which actions lead to the highest cumulative reward.</p>
<p>The process in reinforcement learning involves:</p>
<ul>
<li><p>States: The context or environment the agent is in, represented by feature vectors <span class="math inline">\(X\)</span>.</p></li>
<li><p>Actions: The set of possible choices the agent can make in response to the current state, denoted as <span class="math inline">\(A\)</span>.</p></li>
<li><p>Rewards: Feedback the agent receives after taking an action, which guides the learning process.</p></li>
</ul>
<p>In reinforcement learning, the data consists of feature vectors <span class="math inline">\(X\)</span>, actions <span class="math inline">\(A\)</span>, and rewards <span class="math inline">\(Y\)</span>, namely, <span class="math inline">\((X, A, Y)\)</span>.</p>
</section>
</section>
<section id="decision-trees" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">10.2</span> Decision Trees</h2>
<p>Decision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed <span class="citation" data-cites="breiman1984classification">(<a href="99-references.html#ref-breiman1984classification" role="doc-biblioref">Breiman et al., 1984</a>)</span>. Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.</p>
<section id="recursive-partition-algorithm" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="recursive-partition-algorithm"><span class="header-section-number">10.2.1</span> Recursive Partition Algorithm</h3>
<p>The core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node <span class="math inline">\(m\)</span>, the data subset is denoted as <span class="math inline">\(Q_m\)</span> with a sample size of <span class="math inline">\(n_m\)</span>. The objective is to find a candidate split <span class="math inline">\(\theta\)</span>, defined as a threshold for a given feature, that minimizes an impurity or loss measure <span class="math inline">\(H\)</span>.</p>
<p>When a split is made at node <span class="math inline">\(m\)</span>, the data is divided into two subsets: <span class="math inline">\(Q_{m,l}\)</span> (left node) with sample size <span class="math inline">\(n_{m,l}\)</span>, and <span class="math inline">\(Q_{m,r}\)</span> (right node) with sample size <span class="math inline">\(n_{m,r}\)</span>. The split quality, measured by <span class="math inline">\(G(Q_m, \theta)\)</span>, is given by:</p>
<p><span class="math display">\[
G(Q_m, \theta) = \frac{n_{m,l}}{n_m} H(Q_{m,l}(\theta)) +
\frac{n_{m,r}}{n_m} H(Q_{m,r}(\theta)).
\]</span></p>
<p>The algorithm aims to identify the split that minimizes the impurity:</p>
<p><span class="math display">\[
\theta^* = \arg\min_{\theta} G(Q_m, \theta).
\]</span></p>
<p>This process is applied recursively at each child node until a stopping condition is met.</p>
<ul>
<li><strong>Stopping Criteria:</strong> The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.</li>
<li><strong>Pruning:</strong> Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves generalization.</li>
</ul>
</section>
<section id="search-space-for-possible-splits" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="search-space-for-possible-splits"><span class="header-section-number">10.2.2</span> Search Space for Possible Splits</h3>
<p>At each node, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the feature values. For a given feature, the algorithm considers each of its unique value in the current node as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring effective partition.</p>
<p>Formally, let the feature set be <span class="math inline">\(\{X_1, X_2, \ldots, X_p\}\)</span>, where <span class="math inline">\(p\)</span> is the total number of features, and let the unique values of feature <span class="math inline">\(X_j\)</span> at node <span class="math inline">\(m\)</span> be denoted by <span class="math inline">\(\{v_{m,j,1}, v_{m,j,2}, \ldots, v_{m,j,k_{mj}}\}\)</span>. The search space at node <span class="math inline">\(m\)</span> includes:</p>
<ul>
<li>Feature candidates: <span class="math inline">\(\{X_1, X_2, \ldots, X_p\}\)</span>.</li>
<li>Threshold candidates for <span class="math inline">\(X_j\)</span>: <span class="math display">\[
\left\{ \frac{v_{m,j,i} + v_{m,j,i+1}}{2} \mid 1 \leq i &lt; k_{mj} \right\}.
\]</span></li>
</ul>
<p>While the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.</p>
</section>
<section id="metrics" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="metrics"><span class="header-section-number">10.2.3</span> Metrics</h3>
<section id="classification" class="level4" data-number="10.2.3.1">
<h4 data-number="10.2.3.1" class="anchored" data-anchor-id="classification"><span class="header-section-number">10.2.3.1</span> Classification</h4>
<p>In classification, the split quality metric measures how pure the resulting nodes are after a split. A pure node contains observations that predominantly belong to a single class.</p>
<ul>
<li><p><strong>Gini Index</strong>: The Gini index measures node impurity by the probability that two observations randomly drawn from the node belong to different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node <span class="math inline">\(m\)</span>, the Gini index is <span class="math display">\[
H(Q_m) = \sum_{k=1}^{K} p_{mk} (1 - p_{mk})
= 1 - \sum_{k=1}^n p_{mk}^2,
\]</span> where <span class="math inline">\(p_{mk}\)</span> is the proportion of samples of class <span class="math inline">\(k\)</span> at node <span class="math inline">\(m\)</span>; and <span class="math inline">\(K\)</span> is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including <code>sklearn</code>.</p>
<p>The Gini index originates from the Gini coefficient, introduced by Corrado Gini in 1912 to quantify inequality in income distributions. In that context, the Gini coefficient measures how unevenly a quantity (such as wealth) is distributed across a population. Decision tree algorithms adapt this concept of inequality to measure the impurity of a node: instead of wealth, the distribution concerns class membership. A perfectly pure node, where all observations belong to the same class, represents complete equality and yields a Gini index of zero. As class proportions become more mixed, inequality in class membership increases, leading to higher impurity values. Thus, the Gini index used in decision trees can be viewed as a statistical measure of diversity or heterogeneity derived from Gini’s original work on inequality.</p></li>
<li><p><strong>Entropy (Information Gain):</strong> Derived from information theory, entropy quantifies the disorder of the data at a node. Lower entropy means higher purity. At node <span class="math inline">\(m\)</span>, it is defined as <span class="math display">\[
H(Q_m) = - \sum_{k=1}^{K} p_{mk} \log p_{mk}.
\]</span> Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.</p></li>
<li><p><strong>Misclassification Error:</strong> Misclassification error focuses on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node <span class="math inline">\(m\)</span>, it is defined as <span class="math display">\[
H(Q_m) = 1 - \max_k p_{mk},
\]</span> where <span class="math inline">\(\max_k p_{mk}\)</span> is the largest proportion of samples belonging to any class <span class="math inline">\(k\)</span>.</p></li>
</ul>
</section>
<section id="regression-criteria" class="level4" data-number="10.2.3.2">
<h4 data-number="10.2.3.2" class="anchored" data-anchor-id="regression-criteria"><span class="header-section-number">10.2.3.2</span> Regression Criteria</h4>
<p>In regression, the goal is to minimize the spread or variance of the target variable within each node.</p>
<ul>
<li><p><strong>Mean Squared Error (MSE):</strong> MSE is the average squared difference between observed and predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} (y_i - \bar{y}_m)^2,
\]</span> where</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the actual value for sample <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\bar{y}_m\)</span> is the mean value of the target at node <span class="math inline">\(m\)</span>;</li>
<li><span class="math inline">\(n_m\)</span> is the number of samples at node <span class="math inline">\(m\)</span>.</li>
</ul>
<p>MSE works well when the target is continuous and normally distributed.</p></li>
<li><p><strong>Half Poisson Deviance:</strong> Used for count target, the Poisson deviance measures the variance in the number of occurrences of an event. At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \sum_{i=1}^{n_m} \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right),
\]</span> where <span class="math inline">\(\hat{y}_i\)</span> is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.</p></li>
<li><p><strong>Mean Absolute Error (MAE):</strong> MAE aims to minimize the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} |y_i - \bar{y}_m|.
\]</span> MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.</p></li>
</ul>
</section>
</section>
<section id="ames-housing-example" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="ames-housing-example"><span class="header-section-number">10.2.4</span> Ames Housing Example</h3>
<p>The Ames Housing data are used to illustrate a regression tree model for predicting log house price.</p>
<p>As before, we retrive the data from <code>OpenML</code>.</p>
<div id="42095621" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openml</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Ames Housing dataset (OpenML ID 42165)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> openml.datasets.get_dataset(<span class="dv">42165</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df, <span class="op">*</span>_ <span class="op">=</span> dataset.get_data()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"LogPrice"</span>] <span class="op">=</span> np.log(df[<span class="st">"SalePrice"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A decision tree partitions the feature space into regions where the average log price is relatively constant.</p>
<div id="7fe39031" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotnine <span class="im">as</span> gg</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>numeric_features <span class="op">=</span> [</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"OverallQual"</span>, <span class="st">"GrLivArea"</span>, <span class="st">"GarageCars"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TotalBsmtSF"</span>, <span class="st">"YearBuilt"</span>, <span class="st">"FullBath"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>categorical_features <span class="op">=</span> [<span class="st">"KitchenQual"</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ColumnTransformer([</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"num"</span>, <span class="st">"passthrough"</span>, numeric_features),</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"cat"</span>, OneHotEncoder(drop<span class="op">=</span><span class="st">"first"</span>), categorical_features)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[numeric_features <span class="op">+</span> categorical_features]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"LogPrice"</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>depths <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">11</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> [</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    cross_val_score(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        Pipeline([</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"pre"</span>, preprocessor),</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"model"</span>, DecisionTreeRegressor(max_depth<span class="op">=</span>d, random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        ]),</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        X, y, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">"r2"</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    ).mean()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> depths</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(depths, cv_scores))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>[(2, np.float64(0.594134995704976)),
 (3, np.float64(0.6712972027857058)),
 (4, np.float64(0.7141792234890973)),
 (5, np.float64(0.748794485599919)),
 (6, np.float64(0.7587964739851765)),
 (7, np.float64(0.7343953839481492)),
 (8, np.float64(0.7186324525934304)),
 (9, np.float64(0.7112790937242873)),
 (10, np.float64(0.6938966980572193))]</code></pre>
</div>
</div>
<p>Cross-validation identifies an appropriate tree depth that balances fit and generalization. A too-deep tree overfits, while a shallow tree misses structure.</p>
<div id="68a9e15d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> Pipeline([</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"pre"</span>, preprocessor),</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"model"</span>, DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>dt.fit(X, y)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>df_pred <span class="op">=</span> pd.DataFrame({<span class="st">"Observed"</span>: y, <span class="st">"Predicted"</span>: y_pred})</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>(gg.ggplot(df_pred, gg.aes(x<span class="op">=</span><span class="st">"Observed"</span>, y<span class="op">=</span><span class="st">"Predicted"</span>)) <span class="op">+</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a> gg.geom_point(alpha<span class="op">=</span><span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a> gg.geom_abline(slope<span class="op">=</span><span class="dv">1</span>, intercept<span class="op">=</span><span class="dv">0</span>, linetype<span class="op">=</span><span class="st">"dashed"</span>) <span class="op">+</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a> gg.labs(title<span class="op">=</span><span class="st">"Decision Tree Regression on Ames Housing"</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>         x<span class="op">=</span><span class="st">"Observed Log Price"</span>, y<span class="op">=</span><span class="st">"Predicted Log Price"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-4-output-1.png" width="672" height="480" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plot shows predicted versus observed log prices. A well-fitted model has points close to the diagonal. The decision tree naturally captures nonlinear effects and interactions, though its predictions are piecewise constant, producing visible step patterns.</p>
</section>
</section>
<section id="gradient-boosted-models" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="gradient-boosted-models"><span class="header-section-number">10.3</span> Gradient-Boosted Models</h2>
<p>Gradient boosting is a powerful ensemble technique in machine learning that combines multiple weak learners into a strong predictive model. Unlike bagging methods, which train models independently, gradient boosting fits models sequentially, with each new model correcting errors made by the previous ensemble <span class="citation" data-cites="friedman2001greedy">(<a href="99-references.html#ref-friedman2001greedy" role="doc-biblioref">Friedman, 2001</a>)</span>. While decision trees are commonly used as weak learners, gradient boosting can be generalized to other base models. This iterative method optimizes a specified loss function by repeatedly adding models designed to reduce residual errors.</p>
<section id="introduction-1" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">10.3.1</span> Introduction</h3>
<p>Gradient boosting builds on the general concept of boosting, aiming to construct a strong predictor from an ensemble of sequentially trained weak learners. The weak learners are often shallow decision trees (stumps), linear models, or generalized additive models <span class="citation" data-cites="hastie2009elements">(<a href="99-references.html#ref-hastie2009elements" role="doc-biblioref">Hastie et al., 2009</a>)</span>. Each iteration adds a new learner focusing primarily on the data points poorly predicted by the existing ensemble, thereby progressively enhancing predictive accuracy.</p>
<p>Gradient boosting’s effectiveness stems from:</p>
<ul>
<li>Error Correction: Each iteration specifically targets previous errors, refining predictive accuracy.</li>
<li>Weighted Learning: Iteratively focuses more heavily on difficult-to-predict data points.</li>
<li>Flexibility: Capable of handling diverse loss functions and various types of predictive tasks.</li>
</ul>
<p>The effectiveness of gradient-boosted models has made them popular across diverse tasks, including classification, regression, and ranking. Gradient boosting forms the foundation for algorithms such as XGBoost <span class="citation" data-cites="chen2016xgboost">(<a href="99-references.html#ref-chen2016xgboost" role="doc-biblioref">Chen &amp; Guestrin, 2016</a>)</span>, LightGBM <span class="citation" data-cites="ke2017lightgbm">(<a href="99-references.html#ref-ke2017lightgbm" role="doc-biblioref">Ke et al., 2017</a>)</span>, and CatBoost <span class="citation" data-cites="prokhorenkova2018catboost">(<a href="99-references.html#ref-prokhorenkova2018catboost" role="doc-biblioref">Prokhorenkova et al., 2018</a>)</span>, known for their high performance and scalability.</p>
</section>
<section id="gradient-boosting-process" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="gradient-boosting-process"><span class="header-section-number">10.3.2</span> Gradient Boosting Process</h3>
<p>Gradient boosting builds an ensemble by iteratively minimizing the residual errors from previous models. This iterative approach optimizes a loss function, <span class="math inline">\(L(y, F(x))\)</span>, where <span class="math inline">\(y\)</span> represents the observed target variable and <span class="math inline">\(F(x)\)</span> the model’s prediction for a given feature vector <span class="math inline">\(x\)</span>.</p>
<p>Key concepts:</p>
<ul>
<li>Loss Function: Guides model optimization, such as squared error for regression or logistic loss for classification.</li>
<li>Learning Rate: Controls incremental updates, balancing training speed and generalization.</li>
<li>Regularization: Reduces overfitting through tree depth limitation, subsampling, and L1/L2 penalties.</li>
</ul>
<section id="model-iteration" class="level4" data-number="10.3.2.1">
<h4 data-number="10.3.2.1" class="anchored" data-anchor-id="model-iteration"><span class="header-section-number">10.3.2.1</span> Model Iteration</h4>
<p>The gradient boosting algorithm proceeds as follows:</p>
<ol type="1">
<li><p>Initialization: Define a base model <span class="math inline">\(F_0(x)\)</span>, typically the mean of the target variable for regression or the log-odds for classification.</p></li>
<li><p>Iterative Boosting: At each iteration <span class="math inline">\(m\)</span>:</p>
<ul>
<li><p>Compute pseudo-residuals representing the negative gradient of the loss function at the current predictions. For each observation <span class="math inline">\(i\)</span>: <span class="math display">\[
r_i^{(m)} = -\left.\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right|_{F(x)=F_{m-1}(x)},
\]</span> where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> denote the feature vector and observed value for the <span class="math inline">\(i\)</span>-th observation, respectively. The residuals represent the direction of steepest descent in function space, so fitting a learner to them approximates a gradient descent step minimizing <span class="math inline">\(L(y, F(x))\)</span>.</p></li>
<li><p>Fit a new weak learner <span class="math inline">\(h_m(x)\)</span> to these residuals.</p></li>
<li><p>Update the model: <span class="math display">\[
F_m(x) = F_{m-1}(x) + \eta \, h_m(x),
\]</span> where <span class="math inline">\(\eta\)</span> is a small positive learning rate (e.g., 0.01–0.1), controlling incremental improvement and reducing overfitting.</p></li>
</ul>
<p>In some implementations, the update step includes an additional multiplier determined by a one-dimensional line search that minimizes the loss function at each iteration. Specifically, the optimal step length is defined as <span class="math display">\[
\gamma_m = \arg\min_\gamma \sum_{i=1}^n
L\bigl(y_i,\, F_{m-1}(x_i) + \gamma\, h_m(x_i)\bigr),
\]</span> leading to an updated model of the form <span class="math display">\[
F_m(x) = F_{m-1}(x) + \eta\, \gamma_m\, h_m(x),
\]</span> where <span class="math inline">\(\eta\)</span> remains a shrinkage factor controlling the overall rate of learning, while <span class="math inline">\(\gamma_m\)</span> adjusts the step size adaptively at each iteration.</p></li>
<li><p>Final Model: After <span class="math inline">\(M\)</span> iterations, the ensemble model is: <span class="math display">\[
  F_M(x) = F_0(x) + \sum_{m=1}^M \eta \, h_m(x).
  \]</span></p></li>
</ol>
<p>Stochastic gradient boosting is a variant that enhances gradient boosting by introducing randomness through subsampling at each iteration, selecting a random fraction of data points (typically 50%–80%) to fit the model . This randomness helps reduce correlation among trees, improve model robustness, and lower the risk of overfitting.</p>
</section>
</section>
<section id="boosted-trees-with-ames-housing" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="boosted-trees-with-ames-housing"><span class="header-section-number">10.3.3</span> Boosted Trees with Ames Housing</h3>
<p>Boosted trees apply the gradient boosting framework to decision trees. They build an ensemble of shallow trees, each trained to correct the residual errors of the preceding ones. By sequentially emphasizing observations that are difficult to predict, the model progressively improves its overall predictive accuracy. We now apply gradient boosting using the same preprocessed features. Boosting combines many shallow trees, each correcting the residual errors of its predecessors, to improve predictive accuracy.</p>
<div id="5333af3d" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a range of tree counts for tuning</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>n_estimators_list <span class="op">=</span> [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>cv_scores_gb <span class="op">=</span> [</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    cross_val_score(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        Pipeline([</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"pre"</span>, preprocessor),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"model"</span>, GradientBoostingRegressor(</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                n_estimators<span class="op">=</span>n,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>                learning_rate<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>                max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        ]),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        X, y, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">"r2"</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    ).mean()</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> n_estimators_list</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(n_estimators_list, cv_scores_gb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>[(50, np.float64(0.8199019428994425)),
 (100, np.float64(0.8437789746745888)),
 (200, np.float64(0.8451433195728157)),
 (400, np.float64(0.8405078307788265))]</code></pre>
</div>
</div>
<p>Cross-validation shows how increasing the number of boosting rounds initially improves performance but eventually risks overfitting when too many trees are added.</p>
<div id="a1469b2d" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>gb <span class="op">=</span> Pipeline([</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"pre"</span>, preprocessor),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"model"</span>, GradientBoostingRegressor(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>gb.fit(X, y)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>y_pred_gb <span class="op">=</span> gb.predict(X)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>df_pred_gb <span class="op">=</span> pd.DataFrame({<span class="st">"Observed"</span>: y, <span class="st">"Predicted"</span>: y_pred_gb})</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>(gg.ggplot(df_pred_gb, gg.aes(x<span class="op">=</span><span class="st">"Observed"</span>, y<span class="op">=</span><span class="st">"Predicted"</span>)) <span class="op">+</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a> gg.geom_point(alpha<span class="op">=</span><span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a> gg.geom_abline(slope<span class="op">=</span><span class="dv">1</span>, intercept<span class="op">=</span><span class="dv">0</span>, linetype<span class="op">=</span><span class="st">"dashed"</span>) <span class="op">+</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a> gg.labs(title<span class="op">=</span><span class="st">"Gradient-Boosted Regression on Ames Housing"</span>,</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>         x<span class="op">=</span><span class="st">"Observed Log Price"</span>, y<span class="op">=</span><span class="st">"Predicted Log Price"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-6-output-1.png" width="672" height="480" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The boosted model produces predictions that are generally closer to the 45-degree line than a single tree, reflecting improved accuracy and smoother response across the feature space.</p>
<p>Gradient-boosted trees introduce several parameters that govern model complexity, learning stability, and overfitting control:</p>
<ul>
<li><code>n_estimators</code>: the number of trees (boosting rounds). More trees can reduce bias but increase computation and risk of overfitting. learning_rate — the shrinkage parameter <span class="math inline">\(\eta\)</span> controlling the contribution of each new tree. Smaller values (e.g., 0.05 or 0.01) require more trees but often yield better generalization.</li>
<li><code>max_depth</code>: the maximum depth of each individual tree, limiting the model’s ability to overfit local noise. Shallow trees (depth 2–4) are typical weak learners.</li>
<li><code>subsample</code>: the fraction of data used in each iteration. Values below 1.0 introduce randomness (stochastic boosting), improving robustness and reducing correlation among trees.</li>
<li><code>min_samples_split</code> and <code>min_samples_leaf</code>: minimum numbers of observations required for splitting or forming leaves. These control tree granularity and help regularize the model.</li>
</ul>
<p>In practice, moderate learning rates with a sufficiently large number of estimators and shallow trees often perform best, balancing bias, variance, and computational cost.</p>
</section>
<section id="xgboost-extreme-gradient-boosting" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="xgboost-extreme-gradient-boosting"><span class="header-section-number">10.3.4</span> XGBoost: Extreme Gradient Boosting</h3>
<p>XGBoost is a scalable and efficient implementation of gradient-boosted decision trees <span class="citation" data-cites="chen2016xgboost">(<a href="99-references.html#ref-chen2016xgboost" role="doc-biblioref">Chen &amp; Guestrin, 2016</a>)</span>. It has become one of the most widely used machine learning methods for structured data due to its high predictive performance, regularization capabilities, and speed. XGBoost builds an ensemble of decision trees in a stage-wise fashion, minimizing a regularized objective that balances training loss and model complexity.</p>
<p>The core idea of XGBoost is to fit each new tree to the <em>gradient</em> of the loss function with respect to the model’s predictions. Unlike traditional boosting algorithms like AdaBoost, which use only first-order gradients, XGBoost optionally uses second-order derivatives (Hessians), enabling better convergence and stability <span class="citation" data-cites="friedman2001greedy">(<a href="99-references.html#ref-friedman2001greedy" role="doc-biblioref">Friedman, 2001</a>)</span>.</p>
<p>XGBoost is widely used in data science competitions and real-world applications. It supports regularization (L1 and L2), handles missing values internally, and is designed for distributed computing.</p>
<p>XGBoost builds upon the same foundational idea as gradient boosted machines—sequentially adding trees to improve the predictive model— but introduces a number of enhancements:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 45%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Traditional GBM</th>
<th>XGBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Implementation</td>
<td>Basic gradient boosting</td>
<td>Optimized, regularized boosting</td>
</tr>
<tr class="even">
<td>Regularization</td>
<td>Shrinkage only</td>
<td>L1 and L2 regularization</td>
</tr>
<tr class="odd">
<td>Loss Optimization</td>
<td>First-order gradients</td>
<td>First- and second-order</td>
</tr>
<tr class="even">
<td>Missing Data</td>
<td>Requires manual imputation</td>
<td>Handled automatically</td>
</tr>
<tr class="odd">
<td>Tree Construction</td>
<td>Depth-wise</td>
<td>Level-wise (faster)</td>
</tr>
<tr class="even">
<td>Parallelization</td>
<td>Limited</td>
<td>Built-in</td>
</tr>
<tr class="odd">
<td>Sparsity Handling</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Objective Functions</td>
<td>Few options</td>
<td>Custom supported</td>
</tr>
<tr class="odd">
<td>Cross-validation</td>
<td>External via <code>GridSearchCV</code></td>
<td>Built-in <code>xgb.cv</code></td>
</tr>
</tbody>
</table>
<p>XGBoost is therefore more suitable for large-scale problems and provides better generalization performance in many practical tasks.</p>
<!-- This section is for random forest. -->
</section>
</section>
<section id="random-forest" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">10.4</span> Random Forest</h2>
<p>This section was prepared by Alex Plotnikov, first-year master’s student in statistics.</p>
<section id="random-forest-algorithm" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="random-forest-algorithm"><span class="header-section-number">10.4.1</span> Random Forest Algorithm</h3>
<p>Random forests are an ensemble method that was developed by Leo Breiman in <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">2001</a> to improve the performance of tree-based models. This model creates multiple trees in parallel. There are a few key differences in how trees in a random forest are constructed compared to the decision tree algorithm. The main differences are:</p>
<ul>
<li>Sample of data used for each tree.</li>
<li>Number of features selected at each node.</li>
<li>Trees are allowed to grow deep in random forests.</li>
<li>Prediction is made based on aggregating results from all trees.</li>
</ul>
<div id="15bd946a" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-7-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="632" height="232"></p>
</figure>
</div>
</div>
</div>
<section id="bagging-bootstrap-aggregation" class="level4" data-number="10.4.1.1">
<h4 data-number="10.4.1.1" class="anchored" data-anchor-id="bagging-bootstrap-aggregation"><span class="header-section-number">10.4.1.1</span> Bagging (Bootstrap Aggregation)</h4>
<p>Unlike decision trees which are trained on the entire dataset, random forests use bagging to create a sample for each tree. Afterwards, the tree is trained on that specific sample.</p>
<p>The term bagging comes from bootstrap aggregation. This process involves creating a sample set with the same number of records as the original by sampling from the original dataset <em>with</em> replacement. This means that observations may appear multiple times in the sample dataset.</p>
<p>For one draw:</p>
<p><span class="math display">\[
P(\text{not selected}) = 1 - \frac{1}{N}.
\]</span></p>
<p>For N draws (since we sample N times with replacement):</p>
<p><span class="math display">\[
P(\text{not selected in entire sample}) = \left(1 - \frac{1}{N}\right)^N.
\]</span></p>
<p>As N approaches infinity:</p>
<p><span class="math display">\[
\left(1 - \frac{1}{N}\right)^N \approx e^{-1} \approx 0.3679.
\]</span></p>
<p>So, about <strong>36.8%</strong> of the original observations are <em>not included</em> in a given bootstrap sample.</p>
<p>Therefore, the remaining portion is:</p>
<p><span class="math display">\[
1 - e^{-1} \approx 0.632.
\]</span></p>
<p>On average, 63.2% of the values in each sample are non-repeated. Bagging helps reduce overfitting to noise and reducing variance of the outcome for a more stable model.</p>
</section>
<section id="feature-sampling-and-tree-depth" class="level4" data-number="10.4.1.2">
<h4 data-number="10.4.1.2" class="anchored" data-anchor-id="feature-sampling-and-tree-depth"><span class="header-section-number">10.4.1.2</span> Feature Sampling and Tree Depth</h4>
<p>Random forests also calculate impurity on a subset of all features rather than all features as in a decision tree. At each node, the algorithm selects a <em>random</em> subset of features and calculates the impurity metric for these features. At the next node, the algorithm takes another sample of the entire feature set and calculates the metric on this new set of features.</p>
<p>For <span class="math inline">\(p\)</span> features, the algorithm uses (<span class="math inline">\(\sqrt{p}\)</span>) features for classification and (<span class="math inline">\(\frac{p}{3}\)</span>) features for regression.</p>
<p>Trees in random forests are also usually allowed to grow to max depth. The goal of having multiple trees is reducing variance of output and overfitting, and this is achieved through averaging of trees.</p>
</section>
<section id="prediction" class="level4" data-number="10.4.1.3">
<h4 data-number="10.4.1.3" class="anchored" data-anchor-id="prediction"><span class="header-section-number">10.4.1.3</span> Prediction</h4>
<p>Random forests make predictions by aggregating results from all trees.</p>
<p>For classification, this is done by each tree predicting a label and then the forest combines predictions from all trees to output a label based on a majority vote.</p>
<p>For regression, the forest takes the numerical value predicted by each tree and then calculates the average for the output.</p>
</section>
<section id="algorithm-comparison" class="level4" data-number="10.4.1.4">
<h4 data-number="10.4.1.4" class="anchored" data-anchor-id="algorithm-comparison"><span class="header-section-number">10.4.1.4</span> Algorithm Comparison</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 34%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Decision Trees</th>
<th>Random Forests</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data Set</strong></td>
<td>Entire dataset</td>
<td>Bootstrap sample for each tree</td>
</tr>
<tr class="even">
<td><strong>Features</strong></td>
<td>All features</td>
<td>Random subset at each node</td>
</tr>
<tr class="odd">
<td><strong>Tree Size</strong></td>
<td>Based on input</td>
<td>Can be modified with input but usually until tree fully grows</td>
</tr>
<tr class="even">
<td><strong>Prediction</strong></td>
<td>Leaf where new data lands</td>
<td>Majority vote of all trees for classification, average for regression</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="randomforestclassifier-parameters" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="randomforestclassifier-parameters"><span class="header-section-number">10.4.2</span> RandomForestClassifier Parameters</h3>
<p>These concepts can be adjusted as parameters in the RandomForestClassifier class in Python. Not all parameters are shown here as they are similar to decision tree parameters:</p>
<ul>
<li><code>n_estimators</code>: number of trees to train in forest.</li>
<li><code>max_depth</code>: max depth of the tree in the forest, default value is set to <code>None</code> which allows the tree to grow to max depth.</li>
<li><code>max_features</code>: number of features to use at each node. Default value is <code>'sqrt'</code> and can also be set to <code>'log2'</code> and <code>None</code>.</li>
<li><code>bootstrap</code>: whether or not to use bootstrap for each tree, if <code>False</code> then each tree will use the entire dataset.</li>
<li><code>max_samples</code>: the maximum number of samples for each tree can be adjusted rather than the same size as the dataset.</li>
</ul>
</section>
<section id="random-forests-in-python-ames-data" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="random-forests-in-python-ames-data"><span class="header-section-number">10.4.3</span> Random Forests in Python (Ames Data)</h3>
<p>The below example shows a random forest regression on the Ames data. We construct a plot to show the observed log price against the predicted log price.</p>
<div id="4ba07259" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openml</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier <span class="co"># For classification</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor <span class="co"># For regression</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, mean_squared_error <span class="co"># For evaluation</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotnine <span class="im">as</span> gg</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Ames Housing (OpenML ID 42165)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> openml.datasets.get_dataset(<span class="dv">42165</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>df, <span class="op">*</span>_ <span class="op">=</span> dataset.get_data()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"LogPrice"</span>] <span class="op">=</span> np.log(df[<span class="st">"SalePrice"</span>])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>ind_columns <span class="op">=</span> df.columns.drop([<span class="st">'Id'</span>,<span class="st">'SalePrice'</span>,<span class="st">'LogPrice'</span>]).tolist()</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> pd.get_dummies(df[ind_columns], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_encoded </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'LogPrice'</span>]</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>                                    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co"># # For Classification</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># model = RandomForestClassifier(n_estimators=100, random_state=42)</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co"># model.fit(X_train, y_train)</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># For Regression (if 'target' was continuous)</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="co"># For graph</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>dt.fit(X, y)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt.predict(X)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>df_pred <span class="op">=</span> pd.DataFrame({<span class="st">"Observed"</span>: y, <span class="st">"Predicted"</span>: y_pred})</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>(gg.ggplot(df_pred, gg.aes(x<span class="op">=</span><span class="st">"Observed"</span>, y<span class="op">=</span><span class="st">"Predicted"</span>)) <span class="op">+</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a> gg.geom_point(alpha<span class="op">=</span><span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a> gg.geom_abline(slope<span class="op">=</span><span class="dv">1</span>, intercept<span class="op">=</span><span class="dv">0</span>, linetype<span class="op">=</span><span class="st">"dashed"</span>) <span class="op">+</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a> gg.labs(title<span class="op">=</span><span class="st">"Random Forest Regression on Ames Housing"</span>,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>         x<span class="op">=</span><span class="st">"Observed Log Price"</span>, y<span class="op">=</span><span class="st">"Predicted Log Price"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-8-output-1.png" width="672" height="480" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">10.4.4</span> Further Reading</h3>
<ul>
<li>Classification and Regression Trees. <a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-olshen-charles-stone">Brieman, Friedman, Olshen, Stone 1986.</a></li>
<li>Bagging Predictors. <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">Brieman 1994.</a></li>
<li>Random Forests. <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Brieman 2001.</a></li>
<li>Random Forest Class - <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">scikit</a></li>
</ul>
<!-- This section is for SMOTE. -->
</section>
</section>
<section id="synthetic-minority-over-sampling-technique-smote" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="synthetic-minority-over-sampling-technique-smote"><span class="header-section-number">10.5</span> Synthetic Minority Over-sampling Technique (SMOTE)</h2>
<p>This section was written by Hannah Levine, a junior majoring in Applied Data Analysis.</p>
<section id="introduction-2" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="introduction-2"><span class="header-section-number">10.5.1</span> Introduction</h3>
<p>In classification problems, imbalanced data refers to skewed class distribution. Classification problems use numerical variables to produce a categorical outcome, like diagnosing a disease based on clinical data or detecting fraudulent transactions. In a simple example, you have 2 classes for a medical diagnosis, “healthy” and “not healthy”. For rare diseases, there can be a large difference in the amount of individuals who are “healthy” and who are “not healthy”, with the “healthy” far outnumbering the “not healthy”. The “not healthy” class is known as the minority class.</p>
<p>This is important because machine learning models can become biased towards the majority class, and be innacurate in predicting for the minority class. This is especially apparent in problems with multiple classes. Adressing this class imbalance helps to improve the accuracy of the model. An easy solution is to just oversample the minority class, creating duplicates. However, this doesn’t really add any new information to the model. Another solution is undersampling of the majority class, but then important information might be left out. A better technique is Synthetic Minority Over-Sampling Technique, or SMOTE.</p>
</section>
<section id="smote" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="smote"><span class="header-section-number">10.5.2</span> SMOTE</h3>
<p>While random oversampling creates exact replicas of minority observations, SMOTE creates synthetic samples based on the existing observations in the minority class.</p>
<ol type="1">
<li>SMOTE identifies minority class(es) in the dataset</li>
<li>For each observation in the minority class, SMOTE identifies the k nearest neighbors, with k being specified prior (usually k=5).</li>
<li>A line is drawn between the minority observation and one of its k nearest neighbors.</li>
<li>The difference is found, multiplied by a random number between 0 and 1, then added to the initial observation to generate the synthetic sample.</li>
<li>This process is repeated until the desired ratio is reached, usually when the minority class equals the size of the majority class.</li>
</ol>
<p>This process is shown by the figure below:</p>
<div id="90c329d1" class="cell" data-fig.show="true" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-9-output-1.png" width="934" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The initial majority and minority classes are imbalanced, with the minority only having 2 points and the majority having 8. After SMOTE is applied, the total of minority and synthetic points equals the number of majority points. You can see how the synthetic points are all between the initial 2 minority points. In larger datasets, this can look almost like lines drawn between multiple sets of minority points. The resulting dataset is more balanced, but there’s less of a chance of overlapping data points with the originals because it’s not duplicates.</p>
<p>This can also be demonstrated by: <span class="math display">\[
x_{new} = x_{i} + \lambda(x_{neighbor} - x_{i}),
\]</span> where <span class="math inline">\(x_{i}\)</span> is the selected minority observation, <span class="math inline">\(x_{new}\)</span> is the generated synthetic sample, <span class="math inline">\(\lambda\)</span> is a random number between 0 and 1 sampled from a uniform distribution, and <span class="math inline">\(x_{neighbor}\)</span> is the randomly chosen neighbor out of the identified k neearest neighbors.</p>
</section>
<section id="example-created-synthetic-dataset" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="example-created-synthetic-dataset"><span class="header-section-number">10.5.3</span> Example: Created Synthetic Dataset</h3>
<p>We can generate a random dataset using the <code>make_classification</code> function from <code>sklearn.datasets</code>.</p>
<div id="67030e12" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># create imbalanced dataset</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    weights<span class="op">=</span>[<span class="fl">0.99</span>, <span class="fl">0.01</span>],</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">1234</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original class distribution: </span><span class="sc">{</span>Counter(y)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original class distribution: Counter({np.int64(0): 983, np.int64(1): 17})</code></pre>
</div>
</div>
<div id="9ba315ec" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply SMOTE</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> smote.fit_resample(X, y)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After SMOTE class distribution:"</span>, Counter(y_resampled))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After SMOTE class distribution: Counter({np.int64(0): 983, np.int64(1): 983})</code></pre>
</div>
</div>
<p>For the application of SMOTE, I only specify a random state to ensure reproducibility. Other parameters that can be used are <code>sampling_strategy</code> and <code>k_neighbors</code>.</p>
<p>Now we can visualize the distribution of the data.</p>
<div id="bee437aa" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>orig_counts <span class="op">=</span> Counter(y)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>smote_counts <span class="op">=</span> Counter(y_resampled)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>ax1.bar(orig_counts.keys(), orig_counts.values(), color<span class="op">=</span>[<span class="st">'#DEA8FF'</span>, <span class="st">'#A3007D'</span>])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">"Class Distribution Before SMOTE"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">"Class"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">"Count"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>ax1.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="st">"Majority (0)"</span>, <span class="st">"Minority (1)"</span>])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>ax2.bar(smote_counts.keys(), smote_counts.values(), color<span class="op">=</span>[<span class="st">'#DEA8FF'</span>, <span class="st">'#A3007D'</span>])</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">"Class Distribution After SMOTE"</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">"Class"</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">"Count"</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>ax2.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="st">"Majority (0)"</span>, <span class="st">"Minority (1)"</span>])</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10-supervised_files/figure-html/cell-12-output-1.png" width="1142" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the bar charts, you can see the difference in class size before and after using SMOTE. To see if the application of SMOTE actually made a difference, we can look at models for the original dataset and the after SMOTE dataset.</p>
<div id="78c790e2" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">1234</span>, stratify<span class="op">=</span>y</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># original</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>model_original <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>model_original.fit(X_train, y_train)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>y_pred_original <span class="op">=</span> model_original.predict(X_test)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Model trained on ORIGINAL data ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_original, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Confusion Matrix:"</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred_original))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>=== Model trained on ORIGINAL data ===

              precision    recall  f1-score   support

           0       0.98      1.00      0.99       295
           1       0.00      0.00      0.00         5

    accuracy                           0.98       300
   macro avg       0.49      0.50      0.50       300
weighted avg       0.97      0.98      0.98       300


Confusion Matrix:
[[295   0]
 [  5   0]]</code></pre>
</div>
</div>
<p>Looking at the report, 98% of the cases that were predicted to be class 0 were predicted correctly. 100% of the cases that were actually class 0 were predicted to be class 0. Meanwhile, 0% of the cases were even predicted to be class 1. This is super unrealistic, and it seems like the model completely ignored the minority class. The confusion matrix suppports this, it shows that no cases were predicted to be class 1 at all, while 295 cases were correctly predicted to be class 0 (True Positive), and 5 cases were incorrectly predicted to be class 0 (False Positive).</p>
<div id="3df742b0" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SMOTE</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>X_train_resampled, y_train_resampled <span class="op">=</span> smote.fit_resample(X_train, y_train)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>model_smote <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>model_smote.fit(X_train_resampled, y_train_resampled)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>y_pred_smote <span class="op">=</span> model_smote.predict(X_test)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Model trained with SMOTE ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_smote))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Confusion Matrix:"</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred_smote))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>=== Model trained with SMOTE ===

              precision    recall  f1-score   support

           0       1.00      0.76      0.86       295
           1       0.05      0.80      0.10         5

    accuracy                           0.76       300
   macro avg       0.52      0.78      0.48       300
weighted avg       0.98      0.76      0.85       300


Confusion Matrix:
[[223  72]
 [  1   4]]</code></pre>
</div>
</div>
<p>Looking at the model trained with SMOTE data, the report is much more realistic. 80% of the cases that were in class 1 were correctly predicted to be class 1. However, only 5% of the cases that were predicted to be class 1 actually were class 1. The rest were actually class 0. Looking at the confusion matrix, 223 cases were correctly predicted to be class 0 and only 1 case was incorrectly predicted to be class 0.</p>
</section>
<section id="variations-and-extensions-of-smote" class="level3" data-number="10.5.4">
<h3 data-number="10.5.4" class="anchored" data-anchor-id="variations-and-extensions-of-smote"><span class="header-section-number">10.5.4</span> Variations and Extensions of SMOTE</h3>
<p>SMOTE is very helpful in addressing class imbalance, but with larger datasets and higher-dimensional class spaces other extensions have been developed to help handle those larger scenarios. This includes Borderline SMOTE, ADASYN, SMOTE-ENN, SMOTE+TOMEK, and SMOTE-NC.</p>
<p><strong>Borderline SMOTE</strong> was desgined to address the misclassification of minority class observations that are near or on the borderline between classes. Because they are on or near the borderlines, they are harder to classify and more likely to be mislabeled. Borderline SMOTE generates more synthetic samples near the borderlines between the majority and minority classes, providing more challenging observations to classify in hopes that it will improve the performance of classifiers.</p>
<p><strong>ADASYN</strong>, or Adaptive Synthetic Sampling Approach, focuses on finding and handling regions where the imbalance is more severe. For each minority class observation, ADASYN measures how many of its k nearest neighbors belong to the majority class. If the minority observation has more nearest neighbors that are a part of the majority class, it is considered a high density area. The algorithm then generates more synthetic samples in the higher density regions, which are the regions with a greater imbalance. This allows the dataset to become more uniform by balancing more severely imbalanced areas.</p>
<p><strong>SMOTE-ENN</strong>, or SMOTE Edited Nearest Neighbors, combines SMOTE with the ENN rule. The ENN rule is used to remove misclassified samples. The point of combining these techniques is to improve the overall quality of the dataset. SMOTE is applied first to generate synthetic samples, then ENN is used to remove both synthetic and original samples that have a majority of their nearest neighbors that are the opposite class. This provides a cleaner dataset with more distinct classes.</p>
<p><strong>SMOTE-TOMEK</strong> links combine SMOTE and TOMEK links. TOMEK links are pairs of very close instances that are in opposite classes. By removing these links, overlap between classes is reduced and the classes are more distinct. For every observation in the dataset, the nearest neighbor from the same class and the nearest neighbor from the opposite class is found. Each pair of observations is analyzed to see if it forms a TOMEK link. If a link is found, the two observations are marked for removal. Once all TOMEK links have been found, the observations that are involved are removed. This technique also reduces overlap between classes, and improves the distinction of classes.</p>
<p><strong>SMOTE-NC</strong>, or SMOTE Nominal Continuous, is used for datasets that are mixed nominal (categorical) and continuous variables. SMOTE is great when dealing with datasets with only numerical variables, but runs into problems when there are categorical variables. SMOTE operates in the numerical differences between observations and their neighbors, so categorical variables don’t work with it. SMOTE-NC addresses this by seperately considering the categorical variables, and ensuring the categorical features of the synthetic samples align with those of the original data.</p>
</section>
<section id="conclusion" class="level3" data-number="10.5.5">
<h3 data-number="10.5.5" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.5.5</span> Conclusion</h3>
<p>SMOTE is a great tool to handle imbalanced classes in datasets. By creating synthetic samples from real observations in the minority class, the dataset is balanced while still having a good range of information and not overfitting. It’s important to note that SMOTE does not guaruntee that a model will perform well. It can help with prediction on the minority class, but it doesn’t always mean that the overall performance of the model improves. It also can create noisy samples in dense or overlapping regions. It’s important to analyze the problem to determine if SMOTE is worth applying, or if another method might be better.</p>
</section>
<section id="further-readings" class="level3" data-number="10.5.6">
<h3 data-number="10.5.6" class="anchored" data-anchor-id="further-readings"><span class="header-section-number">10.5.6</span> Further Readings</h3>
<p>Gudluri, S. (2025) <a href="https://www.geeksforgeeks.org/machine-learning/smote-for-imbalanced-classification-with-python/">SMOTE for Imbalanced Classification With Python</a></p>
<p>Brownlee, J. (2020) <a href="https://machinelearningmastery.com/what-is-imbalanced-classification/">A Gentle Introduction to Imbalanced Classification</a></p>
<p>Devanathan, H. (2024) <a href="https://towardsdatascience.com/creating-smote-oversampling-from-scratch-64af1712a3be/">Creating SMOTE Oversampling from Scratch</a></p>
<p>SMOTE <a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html">imbalanced-learn, over-sampling methods</a></p>
<!-- This section is for naive Bayes. -->
</section>
</section>
<section id="naive-bayes" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">10.6</span> Naive Bayes</h2>
<p>Naive Bayes classifier is a supervised learning algorithm based on <strong>Bayes’ Theorem</strong> for solving classification problems <span class="citation" data-cites="domingos1997optimal">(<a href="99-references.html#ref-domingos1997optimal" role="doc-biblioref">Domingos &amp; Pazzani, 1997</a>)</span>. This method determines which category a sample belongs to by calculating the posterior probability of each category, and assumes that each feature is independent of each other under the conditions of the given category.</p>
<p>This “naive” assumption greatly simplifies the model computation and makes naive Bayes a classical algorithm that is both fast and stable.</p>
<p>In statistical learning and machine learning, naive Bayes is widely used for tasks such as text classification, medical diagnosis, credit risk assessment, and anomaly detection. Despite its relatively strict assumptions, it still performs well in high dimensional data scenarios due to its robustness, low computational cost, and good interpretability <span class="citation" data-cites="rish2001empirical">(<a href="99-references.html#ref-rish2001empirical" role="doc-biblioref">Rish, 2001</a>)</span>.</p>
<section id="theoretical-basis" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="theoretical-basis"><span class="header-section-number">10.6.1</span> Theoretical basis</h3>
<section id="bayes-theorem" class="level4" data-number="10.6.1.1">
<h4 data-number="10.6.1.1" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">10.6.1.1</span> Bayes theorem</h4>
<p>The theoretical basis of Naive Bayes classifier is <strong>Bayes’ Theorem</strong>. It describes how to update our level of trust in an event after new observation information is available.</p>
<section id="basic-form" class="level5" data-number="10.6.1.1.1">
<h5 data-number="10.6.1.1.1" class="anchored" data-anchor-id="basic-form"><span class="header-section-number">10.6.1.1.1</span> Basic form</h5>
<p>The basic form of Bayes theorem is:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}.
\]</span></p>
<p>Among them:</p>
<ul>
<li><span class="math inline">\(P(A)\)</span>: <strong>Prior Probability</strong> of event <span class="math inline">\(A\)</span>, representing the initial belief about event <span class="math inline">\(A\)</span> in the absence of any additional information.</li>
<li><span class="math inline">\(P(B|A)\)</span>: <strong>Likelihood</strong>, denotes the probability of observing <span class="math inline">\(B\)</span> given the occurrence of <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(A|B)\)</span>: <strong>Posterior Probability</strong>, which is the updated probability of event <span class="math inline">\(A\)</span> occurring after the observation of <span class="math inline">\(B\)</span>.</li>
<li><span class="math inline">\(P(B)\)</span>: <strong>Marginal Probability</strong> of event <span class="math inline">\(B\)</span>, which represents the overall probability of observing <span class="math inline">\(B\)</span> under all possible scenarios.</li>
</ul>
</section>
<section id="application-to-classification-problems" class="level5" data-number="10.6.1.1.2">
<h5 data-number="10.6.1.1.2" class="anchored" data-anchor-id="application-to-classification-problems"><span class="header-section-number">10.6.1.1.2</span> Application to classification problems</h5>
<p>In the classification task, we let:</p>
<ul>
<li><span class="math inline">\(Y\)</span> is the category variable (e.g., “Is it a serious accident”, “is it spam”);</li>
<li><span class="math inline">\(X = (x_1, x_2, \ldots, x_n)\)</span> is the observed eigenvector.</li>
</ul>
<p>Bayes theorem can be rewritten as follows.</p>
<p><span class="math display">\[
P(Y|X) = \frac{P(X|Y)\,P(Y)}{P(X)}.
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(P(Y)\)</span>: prior probability, reflecting the distribution of each category in the population sample;</li>
<li><span class="math inline">\(P(X|Y)\)</span>: likelihood function, which represents the probability of occurrence of feature <span class="math inline">\(X\)</span> under the category <span class="math inline">\(Y\)</span>;</li>
<li><span class="math inline">\(P(X)\)</span>: the normalization term, which is used to ensure that the posterior probabilities of all categories sum to one;</li>
<li><span class="math inline">\(P(Y|X)\)</span>: posterior probability, which represents the likelihood that the sample after a given feature <span class="math inline">\(X\)</span> belongs to the class <span class="math inline">\(Y\)</span>.</li>
</ul>
<p>Because <span class="math inline">\(P(X)\)</span> is the same for all categories, it can be ignored when comparing which category has a greater probability. The prediction criteria of the model are therefore simplified as follows.</p>
<p><span class="math display">\[
\hat{Y} = \arg\max_Y P(Y|X) = \arg\max_Y P(X|Y)\,P(Y).
\]</span></p>
<p>In other words: Instead of calculating all probabilities, we can simply compare the “prior × likelihood” of each category which is greater.</p>
</section>
<section id="intuitive-explanation" class="level5" data-number="10.6.1.1.3">
<h5 data-number="10.6.1.1.3" class="anchored" data-anchor-id="intuitive-explanation"><span class="header-section-number">10.6.1.1.3</span> Intuitive explanation</h5>
<p>Bayes’ theorem can be understood as a “belief update formula” : When we obtain new observational information <span class="math inline">\(X\)</span>, our belief about a certain category <span class="math inline">\(Y\)</span> changes.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>stage</th>
<th>concept</th>
<th>meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prior <span class="math inline">\(P(Y)\)</span></td>
<td>Our original knowledge of the category <span class="math inline">\(Y\)</span></td>
<td>For example: 30% <br> of serious accidents</td>
</tr>
<tr class="even">
<td>Likelihood <span class="math inline">\(P(X|Y)\)</span></td>
<td>probability of feature <span class="math inline">\(X\)</span> under category <span class="math inline">\(Y\)</span></td>
<td>For example: a higher proportion of <br> serious accidents in nighttime accidents</td>
</tr>
<tr class="odd">
<td>A posterior <span class="math inline">\(P(Y|X)\)</span></td>
<td>update probability after combining observations</td>
<td>For example, if it is at night, <br> the probability of a serious accident rises to about 56%</td>
</tr>
</tbody>
</table>
<p>In other words, Bayes’ theorem lets us <strong>update beliefs</strong> with data. It is this updating mechanism that naive Bayes uses for classification.</p>
</section>
</section>
<section id="conditional-independence-assumption" class="level4" data-number="10.6.1.2">
<h4 data-number="10.6.1.2" class="anchored" data-anchor-id="conditional-independence-assumption"><span class="header-section-number">10.6.1.2</span> Conditional independence assumption</h4>
<p>Naive Bayes’ “naive” comes from a key assumption: Given the category <span class="math inline">\(Y\)</span>, the individual features <span class="math inline">\(x_i\)</span> are <strong>conditionally independent</strong> among each other <span class="citation" data-cites="domingos1997optimal">(<a href="99-references.html#ref-domingos1997optimal" role="doc-biblioref">Domingos &amp; Pazzani, 1997</a>)</span>. Namely:</p>
<p><span class="math display">\[
P(X|Y) = \prod_{i=1}^{n} P(x_i|Y)
\]</span></p>
<p>This assumption greatly simplifies the calculation of the joint distribution. Originally we needed to estimate <span class="math inline">\(P(x_1,x_2,... x_n|Y)\)</span> such a high-dimensional joint probability, But with the independence assumption, one only needs to estimate the conditional probability <span class="math inline">\(P(x_i|Y)\)</span> of each individual feature.</p>
<p>This is not always true in real data, but in most scenarios it is still satisfactory, This is especially the case when the correlation between features is weak or the sample size is large enough.</p>
</section>
<section id="parameter-estimates-with-likelihood-functions" class="level4" data-number="10.6.1.3">
<h4 data-number="10.6.1.3" class="anchored" data-anchor-id="parameter-estimates-with-likelihood-functions"><span class="header-section-number">10.6.1.3</span> Parameter estimates with likelihood functions</h4>
<p>The core of model training is the estimation of two types of parameters:</p>
<ol type="I">
<li><strong>Prior probability</strong> <span class="math inline">\(P(Y)\)</span></li>
</ol>
<p>Represents the proportion of categories occurring in the population:</p>
<p><span class="math display">\[
    \hat{P}(Y_k) = \frac{N_k}{N},
\]</span></p>
<p>Where <span class="math inline">\(N_k\)</span> is the number of samples for category <span class="math inline">\(k\)</span> and <span class="math inline">\(N\)</span> is the total number of samples.</p>
<ol start="2" type="I">
<li><strong>Conditional probability</strong> <span class="math inline">\(P(x_i|Y)\)</span></li>
</ol>
<ul>
<li>If the feature is discrete:</li>
</ul>
<p><span class="math display">\[
    \hat{P}(x_i|Y) = \frac{\text{count}(x_i,Y)}{\text{count}(Y)}.
\]</span></p>
<ul>
<li>If the feature is continuous, assume that it follows a Gaussian distribution under each category:</li>
</ul>
<p><span class="math display">\[
    P(x_i|Y) = \frac{1}{\sqrt{2\pi\sigma_{Y,i}^2}}
    \exp\! \left(-\frac{(x_i-\mu_{Y,i})^2}{2\sigma_{Y,i}^2}\right)
\]</span></p>
<p>where <span class="math inline">\(\mu_{Y,i}\)</span> and <span class="math inline">\(\sigma_{Y,i}^2\)</span> are the mean and variance of feature <span class="math inline">\(x_i\)</span> among all samples in class <span class="math inline">\(Y\)</span>, respectively.</p>
<p>These parameters are usually obtained by Maximum Likelihood Estimation (MLE).</p>
</section>
<section id="class-prediction-and-log-form" class="level4" data-number="10.6.1.4">
<h4 data-number="10.6.1.4" class="anchored" data-anchor-id="class-prediction-and-log-form"><span class="header-section-number">10.6.1.4</span> Class Prediction and Log Form</h4>
<p>The goal of model prediction is to find the class that maximizes the posterior probability:</p>
<p><span class="math display">\[
\hat{Y} = \arg\max_Y P(Y) \prod_{i=1}^n P(x_i|Y)
\]</span></p>
<p>However, in practice, because continuous multiplication can lead to numerical underflow, We usually take logarithms and turn multiplication into addition:</p>
<p><span class="math display">\[
\log P(Y|X) = \log P(Y) + \sum_{i=1}^{n} \log P(x_i|Y)
\]</span></p>
<p>The final output of the model is:</p>
<p><span class="math display">\[
\hat{Y} = \arg\max_Y \big[\log P(Y) + \sum_i \log P(x_i|Y)\big].
\]</span></p>
<p>In this way, not only accuracy problems can be avoided, but also the contribution of each feature to the classification result can be observed more intuitively.</p>
</section>
<section id="theoretical-performance-and-advantages-and-limitations" class="level4" data-number="10.6.1.5">
<h4 data-number="10.6.1.5" class="anchored" data-anchor-id="theoretical-performance-and-advantages-and-limitations"><span class="header-section-number">10.6.1.5</span> Theoretical Performance and Advantages and Limitations</h4>
<ol type="I">
<li><strong>Advantages</strong></li>
</ol>
<ul>
<li>The algorithm is simple, the computational complexity is linear, and the training and prediction speed is extremely fast;</li>
<li>Good performance on high dimensional data (such as bag-of-words model);</li>
<li>Good performance on small sample data; The model results are highly interpretable; Certain robustness against noisy data and missing data.</li>
</ul>
<ol start="2" type="I">
<li><strong>Limitations</strong></li>
</ol>
<p>Conditional independence assumption is often not true, and performance may decrease when feature correlation is strong; Inability to capture feature interaction effects; For the values of features that have not appeared before, there may be a zero probability problem; Output probability estimates are not necessarily reliable (often biased toward 0 or 1).</p>
</section>
<section id="laplace-smoothing" class="level4" data-number="10.6.1.6">
<h4 data-number="10.6.1.6" class="anchored" data-anchor-id="laplace-smoothing"><span class="header-section-number">10.6.1.6</span> Laplace Smoothing</h4>
<p>In naive Bayes, if a feature value never occurs in the training set, its corresponding probability is estimated to be zero. As a result, the whole product is 0 and the classifier outputs the wrong result. To avoid this situation, <strong>Laplace Smoothing</strong> is introduced.</p>
<p>That is to add a constant <span class="math inline">\(\alpha &gt; 0\)</span> (usually 1) to the probability estimate:</p>
<p><span class="math display">\[
\hat{P}(x_i|Y) =
\frac{\text{count}(x_i,Y) + \alpha}
{\sum_{x_i'}(\text{count}(x_i',Y) + \alpha)}.
\]</span></p>
<p>So even if a value doesn’t appear in the training set, the probability is not zero, The model is thus more stable and robust.</p>
</section>
<section id="surprisingly-good-performance" class="level4" data-number="10.6.1.7">
<h4 data-number="10.6.1.7" class="anchored" data-anchor-id="surprisingly-good-performance"><span class="header-section-number">10.6.1.7</span> Surprisingly Good Performance</h4>
<p>Although the conditional independence assumption is often not strictly true, naive Bayes still performs surprisingly well in practice. This is mainly due to the following:</p>
<ul>
<li><p><strong>Robustness</strong> : Even if there is some correlation between features, the classification result is still accurate as long as the dependence between different categories is not very different.</p></li>
<li><p><strong>Error Compensation</strong> : The bias caused by conditional independence assumption often cancerates each other when multiple features are combined.</p></li>
<li><p><strong>Implicit Regularization</strong> : Feature independence makes the model complexity low and can effectively prevent overfitting.</p></li>
<li><p><strong>High-dimensional Advantage</strong> : In scenarios where the number of features is much larger than the number of samples, such as text classification, naive Bayes is almost always more stable than linear models.</p></li>
</ul>
</section>
</section>
<section id="types-of-naive-bayes" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="types-of-naive-bayes"><span class="header-section-number">10.6.2</span> Types of Naive Bayes</h3>
<p>There is not just one form of naive Bayesian algorithm. According to the type of feature variable (continuous or discrete), the value characteristics of the feature (Boolean, count, frequency, etc.) and the distribution assumption of the data, naive Bayes can be divided into the following main types:</p>
<section id="gaussian-naive-bayes" class="level4" data-number="10.6.2.1">
<h4 data-number="10.6.2.1" class="anchored" data-anchor-id="gaussian-naive-bayes"><span class="header-section-number">10.6.2.1</span> Gaussian Naive Bayes</h4>
<p>When the data feature is <strong>continuous variable</strong>, each feature is assumed to follow <strong>Gaussian distribution</strong> under a given category:</p>
<p><span class="math display">\[
P(x_i|Y) = \frac{1}{\sqrt{2\pi\sigma_{Y,i}^2}}
\exp\! \left(-\frac{(x_i - \mu_{Y,i})^2}{2\sigma_{Y,i}^2}\right)
\]</span></p>
<p>The advantages of Gaussian naive Bayes are:</p>
<ul>
<li>suitable for continuous characteristics (e.g.&nbsp;age, temperature, price, etc.).</li>
<li>Computationally efficient.</li>
<li>Parameter estimation only requires sample mean and variance.</li>
<li>Good results can still be achieved for nonlinear separable data.</li>
</ul>
</section>
<section id="categorical-naive-bayes" class="level4" data-number="10.6.2.2">
<h4 data-number="10.6.2.2" class="anchored" data-anchor-id="categorical-naive-bayes"><span class="header-section-number">10.6.2.2</span> Categorical Naive Bayes</h4>
<p>Categorical naive Bayes is used when the feature is <strong>discrete (categorical) variable</strong>. Assume that each feature follows a <strong>categorical distribution</strong> under the category <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
P(x_i|Y) = \frac{\text{count}(x_i,Y) + \alpha}
{\sum_{x_i'} (\text{count}(x_i',Y) + \alpha)}
\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is the smoothing coefficient (usually 1) and is used to avoid the zero-probability problem. It is often used to analyze categorical characteristic data, such as region, color, occupation type, etc.</p>
</section>
<section id="bernoulli-naive-bayes" class="level4" data-number="10.6.2.3">
<h4 data-number="10.6.2.3" class="anchored" data-anchor-id="bernoulli-naive-bayes"><span class="header-section-number">10.6.2.3</span> Bernoulli Naive Bayes</h4>
<p>Bernoulli naive Bayes is used when the features take only two values, such as 0/1, True/False. It assumes that each feature follows a <strong>Bernoulli Distribution</strong> under the category <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
P(x_i|Y) = P_i^{x_i} (1-P_i)^{(1-x_i)},
\]</span></p>
<p>Where <span class="math inline">\(P_i = P(x_i=1|Y)\)</span>. Often used in “word presence or absence” models for text classification (e.g., spam detection). Compared with the polynomial model, the Bernoulli model pays more attention to “whether a certain feature is included” rather than “the number of occurrences of features”.</p>
</section>
<section id="multinomial-naive-bayes" class="level4" data-number="10.6.2.4">
<h4 data-number="10.6.2.4" class="anchored" data-anchor-id="multinomial-naive-bayes"><span class="header-section-number">10.6.2.4</span> Multinomial Naive Bayes</h4>
<p>Polynomial naive Bayes is often used for text analysis or document classification tasks. It assumes that features are represented as discrete frequencies or counts such as word frequency TF. Conditional probabilities are calculated as follows:</p>
<p><span class="math display">\[
P(X|Y) = \frac{(\sum_i x_i)! }{\prod_i x_i! } \prod_i P(x_i|Y)^{x_i},
\]</span></p>
<p>Where <span class="math inline">\(x_i\)</span> is the number of occurrences of the <span class="math inline">\(i\)</span>word in the document. This model is particularly suitable for “Bag-of-Words” models, such as news classification, comment sentiment analysis, etc.</p>
</section>
<section id="complement-naive-bayes" class="level4" data-number="10.6.2.5">
<h4 data-number="10.6.2.5" class="anchored" data-anchor-id="complement-naive-bayes"><span class="header-section-number">10.6.2.5</span> Complement Naive Bayes</h4>
<p>Complement NB is an improvement of the polynomial model, It is mainly used for <strong>class imbalanced</strong> datasets <span class="citation" data-cites="zhang2004complement">(<a href="99-references.html#ref-zhang2004complement" role="doc-biblioref">Zhang, 2004</a>)</span>. It estimates conditional probabilities by considering “samples outside of one category”, reducing the bias of the model by mainstream categories:</p>
<p><span class="math display">\[
P(x_i|\bar{Y}) = \frac{\text{count}(x_i,\bar{Y}) + \alpha}
{\sum_{x_i'}(\text{count}(x_i',\bar{Y}) + \alpha)}
\]</span></p>
<p>This method works well when there are large differences in category proportions, For example, it is often used in tasks such as “accident severity classification” and “fraud detection”.</p>
</section>
<section id="model-comparison-and-application-scenario-summary" class="level4" data-number="10.6.2.6">
<h4 data-number="10.6.2.6" class="anchored" data-anchor-id="model-comparison-and-application-scenario-summary"><span class="header-section-number">10.6.2.6</span> Model comparison and application scenario summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 24%">
<col style="width: 32%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>Model type</th>
<th>Feature type</th>
<th>Common applications</th>
<th>Distributional assumptions</th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gaussian NB</strong></td>
<td>Continuous variables</td>
<td>medical diagnosis, price prediction</td>
<td>Gaussian distribution</td>
<td>more sensitive to outliers</td>
</tr>
<tr class="even">
<td><strong>Categorical NB</strong></td>
<td>Categorical variables</td>
<td>Questionnaire, demographic analysis</td>
<td>Categorical distribution</td>
<td>fit discrete features</td>
</tr>
<tr class="odd">
<td><strong>Bernoulli NB</strong></td>
<td>Binary variables</td>
<td>Spam classification, whether the text <br> contains keywords</td>
<td>Bernoulli distribution</td>
<td>Focus on “presence or absence”</td>
</tr>
<tr class="even">
<td><strong>Multinomial NB</strong></td>
<td>Count / frequency</td>
<td>text classification, sentiment analysis</td>
<td>Multinomial distribution</td>
<td>Fit for high dimensional sparse data</td>
</tr>
<tr class="odd">
<td><strong>Complement NB</strong></td>
<td>Imbalanced sample</td>
<td>fraud detection, traffic accident prediction</td>
<td>Polynomial complementary distribution</td>
<td>Improved imbalanced problem</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="naive-bayesian-application-nyc-crash-data" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="naive-bayesian-application-nyc-crash-data"><span class="header-section-number">10.6.3</span> Naive Bayesian application: NYC Crash Data</h3>
<p>In this section, we will use the accident data set provided in the course to demonstrate how naive Bayesian models can be used to predict whether an accident is a “serious accident”.</p>
<p>The goal here is to classify an original traffic accident record into:</p>
<ul>
<li>severe (severe = 1) : Someone is injured or killed</li>
<li>No severe (severe = 0) : no injuries or deaths</li>
</ul>
<p>This is a typical real world task: we want to quickly determine the severity of an accident as soon as it occurs or is recorded, so that we can direct resources, do risk monitoring, and observe patterns.</p>
<section id="modeling-objectives" class="level4" data-number="10.6.3.1">
<h4 data-number="10.6.3.1" class="anchored" data-anchor-id="modeling-objectives"><span class="header-section-number">10.6.3.1</span> Modeling objectives</h4>
<p>The task we are going to complete is a <strong>binary classification problem</strong> :</p>
<ul>
<li><p><strong>Response variable (Y)</strong> : ‘severe_flag’ Definition rule: If ‘persons_injured &gt; 0’ or ‘persons_killed &gt; 0’, it is considered a serious accident and marked as 1, otherwise it is 0.</p></li>
<li><p><strong>Independent variable (X)</strong> : We will use some features that can be extracted/discretized from accident records such as <code>borough</code>,<code>zip_code</code>,<code>crash_datetime</code>, <code>vehicle_type_code_1</code>,<code>contributing_factor_vehicle_1</code>… Most of these features are categorical factors, which are suitable for conditional probability based modeling in naive Bayes.</p></li>
<li><p>Load packages</p></li>
</ul>
<div id="09c99b1c" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> CategoricalNB</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Read our data</li>
</ul>
<div id="79e60021" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_feather(<span class="st">"data/nyc_crashes_cleaned.feather"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Create the target variable severe_flag (severe accident =1, otherwise =0)</li>
</ul>
<div id="7701aadb" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'number_of_persons_injured'</span>] <span class="op">=</span> df[<span class="st">'number_of_persons_injured'</span>].fillna(<span class="dv">0</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'number_of_persons_killed'</span>] <span class="op">=</span> df[<span class="st">'number_of_persons_killed'</span>].fillna(<span class="dv">0</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'severe_flag'</span>] <span class="op">=</span> np.where(</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'number_of_persons_injured'</span>] <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">|</span> (df[<span class="st">'number_of_persons_killed'</span>] <span class="op">&gt;</span> <span class="dv">0</span>),</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Extract hours from time and divide periods</li>
</ul>
<div id="f975edf7" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'crash_datetime'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'crash_datetime'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'hour'</span>] <span class="op">=</span> df[<span class="st">'crash_datetime'</span>].dt.hour</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> time_period(hour):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">6</span>:</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Late Night"</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">6</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">12</span>:</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Morning"</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">12</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">18</span>:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Afternoon"</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">18</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">24</span>:</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Evening"</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"Unknown"</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'time_category'</span>] <span class="op">=</span> df[<span class="st">'hour'</span>].<span class="bu">apply</span>(time_period)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Select features</li>
</ul>
<div id="55428dbc" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># borough: What borough is it in</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># zip_code/zip_filled: Location code information</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># time_category: Approximate time period when the accident occurred</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># contributing_factor_vehicle_1: The primary contributing_factor for vehicle 1</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># vehicle_type_code_1: Type of vehicle involved</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: We do not directly use the latitude and longitude, street names, these </span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># are high base text/continuous values, not suitable for direct feeding naive </span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayes for classification.</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>model_df <span class="op">=</span> df[[</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'borough'</span>,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'zip_filled'</span>,</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time_category'</span>,</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'contributing_factor_vehicle_1'</span>,</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'vehicle_type_code_1'</span>,</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'severe_flag'</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>]].dropna()</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Label encode</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>label_encoders <span class="op">=</span> {}</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> model_df.columns:</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model_df[col].dtype <span class="op">==</span> <span class="st">'object'</span>:</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        le <span class="op">=</span> LabelEncoder()</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        model_df[col] <span class="op">=</span> le.fit_transform(model_df[col])</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        label_encoders[col] <span class="op">=</span> le</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Split data into training/testing sets and Train the Naive Bayes model.</li>
</ul>
<div id="a2685284" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> model_df.drop(<span class="st">'severe_flag'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> model_df[<span class="st">'severe_flag'</span>]</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Make predictions and evaluate the model.</li>
</ul>
<div id="c8a9d85e" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>nb_model <span class="op">=</span> CategoricalNB(alpha<span class="op">=</span><span class="dv">1</span>)   <span class="co"># laplace smoothing</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>nb_model.fit(X_train, y_train)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> nb_model.predict(X_test)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>y_prob <span class="op">=</span> nb_model.predict_proba(X_test)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[163  51]
 [ 94  97]]
              precision    recall  f1-score   support

           0       0.63      0.76      0.69       214
           1       0.66      0.51      0.57       191

    accuracy                           0.64       405
   macro avg       0.64      0.63      0.63       405
weighted avg       0.64      0.64      0.64       405
</code></pre>
</div>
</div>
<section id="interpretation-of-our-model-results" class="level5" data-number="10.6.3.1.1">
<h5 data-number="10.6.3.1.1" class="anchored" data-anchor-id="interpretation-of-our-model-results"><span class="header-section-number">10.6.3.1.1</span> Interpretation of our model results</h5>
<p>From the results, the overall performance of our Naive Bayes model is only moderate. The accuracy of the model is about <strong>64%</strong>, which means that it is better than random guess, but the predictive power is not strong. Especially in distinguishing serious and non-serious accidents, the model identifies non-serious accidents relatively well (0), but its ability to capture serious accidents (1) is relatively weak, and many real serious accidents still cannot be successfully identified.</p>
<p>It can also be seen from the average values of precision, recall and F1 that the overall classification ability of the model is about 0.63 to 0.64, which is relatively general. In general, the Naive Bayes model can provide a certain reference, but it is not enough to establish a high-precision accident severity prediction system. More complex or stronger models may be needed to improve performance in practical applications.</p>
<p>The main reasons for this may include:</p>
<ul>
<li><p><strong>Imbalanced Data</strong> : The samples of non-serious accidents are far more than those of serious accidents, resulting in the model being biased towards the majority class.</p></li>
<li><p><strong>Naive Assumption</strong> : Naive Bayes assumes that each feature is independent of each other, but this assumption is often not true in real traffic accident data.</p></li>
<li><p><strong>Limited feature information</strong> : The variables used (such as time period, vehicle type, causative factors, etc.) are relatively macroscopic and lack continuity or quantitative characteristics, which limits the accuracy of the model.</p></li>
</ul>
<p>Nevertheless, the naive Bayesian model has the following advantages:</p>
<ul>
<li>fast training and prediction.</li>
<li>strong interpretation of parameters</li>
<li>Suitable as a baseline model (baseline) for more complex models (e.g.&nbsp; Random Forest, XGBoost).</li>
</ul>
<p>Therefore, the results of this model can be used as a reference basis for subsequent improvements, and further improvements in model performance are expected after the introduction of more features or balanced samples.</p>
<!-- This section is for nueral network. -->
</section>
</section>
</section>
</section>
<section id="neural-networks" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="neural-networks"><span class="header-section-number">10.7</span> Neural Networks</h2>
<p>Neural networks learn flexible, nonlinear relationships by composing simple computational units. Each unit (or <em>neuron</em>) applies an affine transformation to its inputs followed by a nonlinearity; stacking layers of such units yields a <strong>feed-forward</strong> mapping from features to predictions. A helpful anchor is logistic regression: it can be viewed as a one-layer neural network with a sigmoid activation. Hidden layers generalize this idea by transforming inputs into progressively more useful internal representations, enabling the model to capture complex patterns beyond linear decision boundaries.</p>
<p>Historically, the field traces back to the early formal model of a neuron by <span class="citation" data-cites="mcculloch1943logical">McCulloch &amp; Pitts (<a href="99-references.html#ref-mcculloch1943logical" role="doc-biblioref">1943</a>)</span>, followed by Rosenblatt’s perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="99-references.html#ref-rosenblatt1958perceptron" role="doc-biblioref">Rosenblatt, 1958</a>)</span>, which was the first trainable linear classifier. The limitations of single-layer perceptrons, notably their inability to model nonlinearly separable functions, were rigorously analyzed by <span class="citation" data-cites="minsky1969perceptrons">Minsky &amp; Papert (<a href="99-references.html#ref-minsky1969perceptrons" role="doc-biblioref">1969</a>)</span>, leading to a temporary decline in interest. The introduction of the backpropagation algorithm by <span class="citation" data-cites="rumelhart1986learning">Rumelhart et al. (<a href="99-references.html#ref-rumelhart1986learning" role="doc-biblioref">1986</a>)</span> revived the study of neural networks in the 1980s. With the growth of data, advances in hardware, and improved optimization methods, neural networks became the foundation for modern deep learning. Multilayer networks are universal function approximators in principle; in practice, their success depends on careful architectural design, efficient optimization, and appropriate regularization.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dimensions and Shapes
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an input vector <span class="math inline">\(x \in \mathbb{R}^d\)</span>, a hidden layer with <span class="math inline">\(m\)</span> neurons computes <span class="math inline">\(h = \sigma(Wx + b)\)</span> through an activation function <span class="math inline">\(\sigma\)</span> <span class="math inline">\(W \in \mathbb{R}^{m\times d}\)</span> and <span class="math inline">\(b \in \mathbb{R}^m\)</span>. The output layer applies another affine map (and possibly a final activation) to <span class="math inline">\(h\)</span>. Keeping track of these shapes prevents many implementation bugs.</p>
</div>
</div>
<p>The figure below shows an input layer, one hidden layer, and an output layer with directed connections. (We will formalize the mathematics in the next subsection.)</p>
<div id="cell-fig-basic-network" class="cell" data-fig-height="5" data-fig-width="7" data-execution_count="21">
<div class="cell-output cell-output-display">
<div id="fig-basic-network" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-basic-network-output-1.png" width="540" height="364" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: A minimal feed-forward neural network with an input layer (3 nodes), one hidden layer (4 nodes), and an output layer (1 node). Edges indicate the direction of information flow.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="structure-of-a-neural-network" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="structure-of-a-neural-network"><span class="header-section-number">10.7.1</span> Structure of a Neural Network</h3>
<p>A neural network is composed of <strong>layers</strong> of interconnected processing units called <em>neurons</em> or <em>nodes</em>. Each neuron receives inputs, applies a linear transformation, and passes the result through a nonlinear <strong>activation function</strong>. The layers are arranged sequentially so that information flows from input features to intermediate representations and finally to the output.</p>
</section>
<section id="mathematical-formulation" class="level3" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">10.7.2</span> Mathematical Formulation</h3>
<p>Let the input vector be <span class="math inline">\(x \in \mathbb{R}^{d}\)</span>. A hidden layer with <span class="math inline">\(m\)</span> neurons computes</p>
<p><span class="math display">\[
h = \sigma(Wx + b),
\]</span></p>
<p>where <span class="math inline">\(W \in \mathbb{R}^{m \times d}\)</span> is the weight matrix, <span class="math inline">\(b \in \mathbb{R}^{m}\)</span> is the bias vector, and <span class="math inline">\(\sigma(\cdot)\)</span> is an activation function applied element-wise.<br>
Subsequent layers take <span class="math inline">\(h\)</span> as input and repeat the same computation, producing successively transformed representations.<br>
If the output layer contains <span class="math inline">\(k\)</span> neurons with linear activations, the network computes</p>
<p><span class="math display">\[
\hat{y} = Vh + c,
\]</span></p>
<p>where <span class="math inline">\(V \in \mathbb{R}^{k \times m}\)</span> and <span class="math inline">\(c \in \mathbb{R}^{k}\)</span> are the output weights and biases.<br>
Collectively, these parameters define the model’s trainable structure.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feed-Forward Flow
</div>
</div>
<div class="callout-body-container callout-body">
<p>During a forward pass, inputs propagate layer by layer: <span class="math inline">\(x \rightarrow h^{(1)} \rightarrow h^{(2)} \rightarrow \cdots
\rightarrow \hat{y}\)</span>.<br>
Backward propagation of gradients (discussed later) updates all weights based on prediction error.</p>
</div>
</div>
</section>
<section id="network-diagram" class="level3" data-number="10.7.3">
<h3 data-number="10.7.3" class="anchored" data-anchor-id="network-diagram"><span class="header-section-number">10.7.3</span> Network Diagram</h3>
<p><a href="#fig-deep-network" class="quarto-xref">Figure&nbsp;<span>10.2</span></a> illustrates a feed-forward neural network with two hidden layers. Arrows indicate the direction of information flow from one layer to the next. Each connection carries a learnable weight, and nonlinear activations transform the signals as they propagate forward.</p>
<div id="cell-fig-deep-network" class="cell" data-fig-format="svg" data-fig-height="4" data-fig-width="8" data-execution_count="22">
<div class="cell-output cell-output-display">
<div id="fig-deep-network" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deep-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-deep-network-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="458" height="334">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deep-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: A feed-forward neural network with two hidden layers. Directed arrows represent weighted connections; one example weight <span class="math inline">\(w^{(1)}_{23}\)</span> is annotated to indicate its origin and destination neurons.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Each connection (edge) has an associated weight adjusted during training. The depth of a network refers to the number of hidden layers, and the width refers to the number of neurons per layer.</p>
</div>
</div>
</section>
<section id="activation-functions" class="level3" data-number="10.7.4">
<h3 data-number="10.7.4" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">10.7.4</span> Activation Functions</h3>
<p>Activation functions introduce <strong>nonlinearity</strong> into neural networks. Without them, the entire network would collapse into a single linear transformation, regardless of the number of layers. Nonlinear activations allow networks to approximate arbitrary complex functions, enabling them to model curved decision boundaries and hierarchical representations.</p>
<p>Let <span class="math inline">\(z\)</span> denote the input to a neuron before activation. The most common choices are:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 22%">
<col style="width: 18%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Formula</th>
<th>Range</th>
<th>Key Property</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sigmoid</strong></td>
<td><span class="math inline">\(\displaystyle \sigma(z) = \frac{1}{1 + e^{-z}}\)</span></td>
<td><span class="math inline">\((0,1)\)</span></td>
<td>Smooth, bounded; used in early networks and binary outputs</td>
</tr>
<tr class="even">
<td><strong>Tanh</strong></td>
<td><span class="math inline">\(\displaystyle \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\)</span></td>
<td><span class="math inline">\((-1,1)\)</span></td>
<td>Zero-centered; stronger gradients than sigmoid</td>
</tr>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td><span class="math inline">\(\displaystyle \mathrm{ReLU}(z) = \max(0, z)\)</span></td>
<td><span class="math inline">\([0, \infty)\)</span></td>
<td>Sparse activations; efficient to compute</td>
</tr>
<tr class="even">
<td><strong>Leaky ReLU</strong></td>
<td><span class="math inline">\(\displaystyle \mathrm{LReLU}(z) = \max(0.01z, z)\)</span></td>
<td><span class="math inline">\((-\infty, \infty)\)</span></td>
<td>Avoids “dead” neurons of ReLU</td>
</tr>
</tbody>
</table>
<p><a href="#fig-activation-curves" class="quarto-xref">Figure&nbsp;<span>10.3</span></a> shows the shape of these activation functions. Notice how ReLU sharply truncates negative values while sigmoid and tanh saturate for large magnitudes.</p>
<div id="cell-fig-activation-curves" class="cell" data-fig-format="svg" data-fig-height="4" data-fig-width="7" data-execution_count="23">
<div class="cell-output cell-output-display">
<div id="fig-activation-curves" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-activation-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-activation-curves-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="587" height="376">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-activation-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Activation functions commonly used in neural networks. ReLU and its variants introduce sparsity, while sigmoid and tanh are smooth and saturating.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="training-neural-networks" class="level3" data-number="10.7.5">
<h3 data-number="10.7.5" class="anchored" data-anchor-id="training-neural-networks"><span class="header-section-number">10.7.5</span> Training Neural Networks</h3>
<p>Training a neural network means adjusting its parameters so that its predictions <span class="math inline">\(\hat{y}\)</span> align closely with the true outputs <span class="math inline">\(y\)</span>. This is done by minimizing a <strong>loss function</strong> that quantifies prediction error. Optimization proceeds iteratively through <strong>forward</strong> and <strong>backward</strong> passes.</p>
<section id="loss-functions" class="level4" data-number="10.7.5.1">
<h4 data-number="10.7.5.1" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">10.7.5.1</span> Loss Functions</h4>
<p>The choice of loss depends on the task:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 54%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Typical Loss Function</th>
<th>Expression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>Mean squared error (MSE)</td>
<td><span class="math inline">\(\displaystyle L = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span></td>
</tr>
<tr class="even">
<td>Binary classification</td>
<td>Binary cross-entropy</td>
<td><span class="math inline">\(\displaystyle L = -\frac{1}{n}\sum_{i=1}^{n} \left[y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)\right]\)</span></td>
</tr>
<tr class="odd">
<td>Multiclass classification</td>
<td>Categorical cross-entropy</td>
<td><span class="math inline">\(\displaystyle L = -\frac{1}{n}\sum_{i=1}^{n} \sum_{k=1}^{K} y_{ik}\log \hat{y}_{ik}\)</span></td>
</tr>
</tbody>
</table>
<p>The loss function <span class="math inline">\(L(\theta)\)</span> depends on all network parameters <span class="math inline">\(\theta = \{W, b, V, c, \ldots\}\)</span> and guides the optimization.</p>
</section>
<section id="gradient-descent" class="level4" data-number="10.7.5.2">
<h4 data-number="10.7.5.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">10.7.5.2</span> Gradient Descent</h4>
<p>Neural networks are trained by <strong>gradient descent</strong>, which updates parameters in the opposite direction of the gradient of the loss:</p>
<p><span class="math display">\[
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta L(\theta^{(t)}),
\]</span></p>
<p>where <span class="math inline">\(\eta &gt; 0\)</span> is the <strong>learning rate</strong> controlling the step size. Choosing <span class="math inline">\(\eta\)</span> too small leads to slow convergence; too large can cause divergence.</p>
<p>Variants such as <em>stochastic</em> and <em>mini-batch</em> gradient descent compute gradients on subsets of data to speed learning and improve generalization.</p>
<p><a href="#fig-loss-landscape" class="quarto-xref">Figure&nbsp;<span>10.4</span></a> gives a two-dimensional analogy of gradient descent. In practice, a neural network may contain millions of parameters, meaning the loss function is defined over an extremely high-dimensional space. The contours shown here merely represent a projection of that space onto two dimensions for illustration.<br>
The true optimization surface is much more complex—full of flat regions, sharp valleys, and numerous local minima—but the intuition of moving “downhill” along the loss gradient remains valid.</p>
<div id="cell-fig-loss-landscape" class="cell" data-fig-format="svg" data-fig-height="4" data-fig-width="6" data-execution_count="24">
<div class="cell-output cell-output-display">
<div id="fig-loss-landscape" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-loss-landscape-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="517" height="361">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.4: Conceptual illustration of gradient descent. The optimizer iteratively moves parameters down the loss surface toward a local minimum.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="regularization-and-overfitting" class="level3" data-number="10.7.6">
<h3 data-number="10.7.6" class="anchored" data-anchor-id="regularization-and-overfitting"><span class="header-section-number">10.7.6</span> Regularization and Overfitting</h3>
<p>Because neural networks contain large numbers of parameters, they can easily <strong>overfit</strong> training data—capturing noise or random fluctuations rather than generalizable patterns. A model that overfits performs well on the training set but poorly on new data. Regularization techniques introduce constraints or randomness that help the network generalize.</p>
<section id="the-biasvariance-trade-off" class="level4" data-number="10.7.6.1">
<h4 data-number="10.7.6.1" class="anchored" data-anchor-id="the-biasvariance-trade-off"><span class="header-section-number">10.7.6.1</span> The Bias–Variance Trade-Off</h4>
<p>Adding model complexity (more layers or neurons) reduces bias but increases variance. The goal of regularization is to find a balance between these forces. <a href="#fig-overfitting-illustration" class="quarto-xref">Figure&nbsp;<span>10.5</span></a> provides a schematic view: the unregularized model follows every data fluctuation, while a regularized model captures only the underlying trend.</p>
<div id="cell-fig-overfitting-illustration" class="cell" data-fig-format="svg" data-fig-height="4" data-fig-width="6" data-execution_count="25">
<div class="cell-output cell-output-display">
<div id="fig-overfitting-illustration" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overfitting-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-overfitting-illustration-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="526" height="376">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-overfitting-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.5: Illustration of overfitting and regularization. The unregularized model follows noise in the data, whereas the regularized model captures the broader pattern.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="common-regularization-techniques" class="level4" data-number="10.7.6.2">
<h4 data-number="10.7.6.2" class="anchored" data-anchor-id="common-regularization-techniques"><span class="header-section-number">10.7.6.2</span> Common Regularization Techniques</h4>
<p>Among the simplest and most widely used techniques is <strong>L2 weight decay</strong>, which adds a penalty term to discourage large weights:</p>
<p><span class="math display">\[
L_{\text{total}} = L_{\text{data}} + \lambda \sum_{l}\sum_{i,j} (w_{ij}^{(l)})^2,
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> controls the strength of the constraint. Other general approaches include <strong>dropout</strong>, which randomly deactivates neurons during training to prevent reliance on any single pathway, and <strong>early stopping</strong>, which halts training once the validation loss stops improving. These ideas apply to networks of all depths and form the foundation for more advanced regularization strategies discussed later in the deep learning section.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Regularization Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>Regularization methods restrict how freely the model parameters can adapt to training data. This constraint encourages smoother mappings, reduces sensitivity to noise, and improves generalization to unseen inputs.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Tips
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Start with small <span class="math inline">\(\lambda\)</span> for L2 weight decay.<br>
</li>
<li>Use dropout (e.g., <span class="math inline">\(p = 0.2\)</span>–<span class="math inline">\(0.5\)</span>) between dense layers.<br>
</li>
<li>Always track training vs.&nbsp;validation loss curves to detect overfitting.<br>
</li>
</ul>
</div>
</div>
</section>
</section>
<section id="example-a-simple-feed-forward-network" class="level3" data-number="10.7.7">
<h3 data-number="10.7.7" class="anchored" data-anchor-id="example-a-simple-feed-forward-network"><span class="header-section-number">10.7.7</span> Example: A Simple Feed-Forward Network</h3>
<p>To illustrate how a neural network operates in practice, we train a simple <strong>multilayer perceptron (MLP)</strong> on a two-dimensional nonlinear dataset. The model learns a curved decision boundary that cannot be captured by linear classifiers.</p>
<p>The <em>two-moons</em> dataset consists of two interleaving half-circles, forming a pattern that resembles a pair of crescents. Each point represents an observation with two features <span class="math inline">\((x_1, x_2)\)</span>, and the color indicates its class. The two classes overlap slightly due to added random noise.</p>
<p>A linear classifier such as logistic regression would draw a straight line through the plane, misclassifying many points. In contrast, a neural network can learn the <strong>nonlinear boundary</strong> that curves along the interface between the moons. This problem, although simple, captures the essence of nonlinear learning in higher dimensions.</p>
<p>We generate the data using <code>make_moons()</code> from <code>scikit-learn</code> and train a feed-forward neural network with two hidden layers. The model’s task is to assign each observation to one of the two moon shapes.</p>
<div id="924f729b" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">500</span>, noise<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">1023</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define and train a small neural network</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>),</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>                    solver<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>                    alpha<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>                    max_iter<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>                    random_state<span class="op">=</span><span class="dv">1023</span>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>mlp.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(alpha=0.001, hidden_layer_sizes=(10, 5), max_iter=2000,
              random_state=1023)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>MLPClassifier</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                
<table class="parameters-table caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">hidden_layer_sizes&nbsp;</td>
<td class="value">(10, ...)</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">activation&nbsp;</td>
<td class="value">'relu'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">solver&nbsp;</td>
<td class="value">'adam'</td>
</tr>
<tr class="user-set even">
<td><em></em></td>
<td class="param">alpha&nbsp;</td>
<td class="value">0.001</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">batch_size&nbsp;</td>
<td class="value">'auto'</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">learning_rate&nbsp;</td>
<td class="value">'constant'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">learning_rate_init&nbsp;</td>
<td class="value">0.001</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">power_t&nbsp;</td>
<td class="value">0.5</td>
</tr>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">max_iter&nbsp;</td>
<td class="value">2000</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">shuffle&nbsp;</td>
<td class="value">True</td>
</tr>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">random_state&nbsp;</td>
<td class="value">1023</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">tol&nbsp;</td>
<td class="value">0.0001</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">verbose&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">warm_start&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">momentum&nbsp;</td>
<td class="value">0.9</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">nesterovs_momentum&nbsp;</td>
<td class="value">True</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">early_stopping&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">validation_fraction&nbsp;</td>
<td class="value">0.1</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">beta_1&nbsp;</td>
<td class="value">0.9</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">beta_2&nbsp;</td>
<td class="value">0.999</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">epsilon&nbsp;</td>
<td class="value">1e-08</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">n_iter_no_change&nbsp;</td>
<td class="value">10</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">max_fun&nbsp;</td>
<td class="value">15000</td>
</tr>
</tbody>
</table>

            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script>
</div>
</div>
<p>Afer the model is trained, we evaluate its accuracy.</p>
<div id="cell-fig-mlp-training" class="cell" data-fig-format="svg" data-fig-height="5" data-fig-width="6" data-execution_count="27">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate accuracy</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>acc_train <span class="op">=</span> mlp.score(X_train, y_train)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>acc_test <span class="op">=</span> mlp.score(X_test, y_test)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare grid for decision boundary</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">300</span>),</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>                     np.linspace(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">2</span>, <span class="dv">300</span>))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> mlp.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>ax.contourf(xx, yy, Z, cmap<span class="op">=</span><span class="st">"coolwarm"</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">"coolwarm"</span>, edgecolor<span class="op">=</span><span class="st">"k"</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$x_2$"</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f"Two-Moons Classification (Train Acc: </span><span class="sc">{</span>acc_train<span class="sc">:.2f}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>acc_test<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mlp-training" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="10-supervised_files/figure-html/fig-mlp-training-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="530" height="449">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.6: Decision boundary learned by a simple multilayer perceptron on the two-moons dataset. The network captures the nonlinear separation between the classes.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The trained network achieves high accuracy on both the training and test sets, demonstrating good generalization. The contour regions in Figure <a href="#fig-mlp-training" class="quarto-xref">Figure&nbsp;<span>10.6</span></a> reveal the curved boundary the network has learned. Each hidden layer transforms the input coordinates into a new representation where the two classes become more separable. After a few layers of nonlinear transformations, the output layer can classify the points with a simple linear threshold.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even this small network, with only a few dozen parameters, can learn a highly nonlinear decision surface. This illustrates the expressive power of multilayer neural networks, even in low-dimensional spaces.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Try It Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Increase the noise level in the data to test robustness. Change the hidden layer sizes (e.g., (3,), (20,10)), or the activation function (tanh, logistic). Adjust the regularization parameter alpha to see how it affects smoothness of the boundary.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-breiman1984classification" class="csl-entry" role="listitem">
Breiman, L., Friedman, J. H., Olshen, R., &amp; Stone, C. J. (1984). <em>Classification and regression trees</em>. Wadsworth.
</div>
<div id="ref-chen2016xgboost" class="csl-entry" role="listitem">
Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785–794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>
</div>
<div id="ref-domingos1997optimal" class="csl-entry" role="listitem">
Domingos, P., &amp; Pazzani, M. (1997). On the optimality of the simple <span>B</span>ayesian classifier under zero-one loss. <em>Machine Learning</em>, <em>29</em>(2-3), 103–130.
</div>
<div id="ref-friedman2001greedy" class="csl-entry" role="listitem">
Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. <em>The Annals of Statistics</em>, <em>29</em>(5), 1189–1232.
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of statistical learning: Data mining, inference, and prediction</em>. Springer.
</div>
<div id="ref-ke2017lightgbm" class="csl-entry" role="listitem">
Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &amp; Liu, T.-Y. (2017). <span>LightGBM</span>: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, 3146–3154.
</div>
<div id="ref-mcculloch1943logical" class="csl-entry" role="listitem">
McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. <em>The Bulletin of Mathematical Biophysics</em>, <em>5</em>(4), 115–133.
</div>
<div id="ref-minsky1969perceptrons" class="csl-entry" role="listitem">
Minsky, M., &amp; Papert, S. A. (1969). <em>Perceptrons: An introduction to computational geometry</em>. MIT Press.
</div>
<div id="ref-prokhorenkova2018catboost" class="csl-entry" role="listitem">
Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., &amp; Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. <em>Advances in Neural Information Processing Systems</em>, 6638–6648.
</div>
<div id="ref-rish2001empirical" class="csl-entry" role="listitem">
Rish, I. (2001). An empirical study of the naive <span>B</span>ayes classifier. <em>IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence</em>, 41–46.
</div>
<div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological Review</em>, <em>65</em>(6), 386.
</div>
<div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, <em>323</em>(6088), 533–536. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>
</div>
<div id="ref-zhang2004complement" class="csl-entry" role="listitem">
Zhang, H. (2004). The optimality of naive <span>B</span>ayes. <em>AAAI Conference on Artificial Intelligence</em>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./09-classification.html" class="pagination-link" aria-label="Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Classification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./11-unsupervised.html" class="pagination-link" aria-label="Unsupervised Learning">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>