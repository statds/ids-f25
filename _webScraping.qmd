## Web Scraping

This section was prepared by Sahil Patel, an undergraduate junior majoring in 
Computer Science, Statistical Data Science, and Economics.

### An Introduction to Web Scraping

#### What is Web Scraping

Web scraping is a way to automatically gather information from websites. Instead 
of spending hours copying and pasting data, you can use scripts to collect large 
amounts of structured information quickly and consistently. Once you have the 
data, it can be analyzed, stored, or used for research and decision making.

Web scraping is particularly useful because most web data is created for humans 
to read, not for machines to process. By scraping, we can turn that information 
into a format that computers can work with, allowing us to explore trends, track 
changes, or gain insights that would be really hard to collect manually.


#### Use Cases of Web Scraping
Here are some of the use cases of web scarping:

- Market Analysis
    - Companies use web scraping to track products, prices, and customer reviews, 
    helping them understand market trends and stay competitive.

- Finance and Investment
    - Analysts gather stock prices, financial news, and reports to make informed 
    investment decisions or to analyze market sentiment.

- Academic Research
    - Researchers collect articles, datasets, and public records to support 
    studies and uncover insights across a variety of fields.

- Social Media and Marketing
    - Marketers and analysts monitor trends, hashtags, and audience engagement 
    to understand consumer behavior and improve campaigns.


#### Static versus Dynamic Web Pages
Before scraping, it is important to understand the kind of web pages you are 
working with:

- Static Web Pages
    - The content is already in the HTML when the page loads. Everything you 
    need is there, so extracting it is relatively straightforward.

- Dynamic Web Pages
    - The content is generated by JavaScript or updated after the page loads. 
    Fetching the HTML alone may not give you what you want. You may need to 
    interact with the page to see the full content.


#### Legal and Ethical Considerations
Even though web scraping can be extremely powerful, it comes with important 
responsibilities and best practices that should not be ignored.

- Respect robots.txt: Before scraping a site, check its robots.txt file to 
    understand which pages or sections the website owner allows you to access, 
    and make sure to follow these guidelines.

- Avoid overloading servers: Sending too many requests too quickly can strain a 
    website’s server. Introduce pauses between requests, use random delays, and 
    keep your scraping activity at a reasonable pace to avoid causing problems 
    for the website.

- Do not collect sensitive information: Avoid scraping personal, confidential, 
    or protected information unless you have explicit permission to do so, as 
        this could violate privacy laws or ethical standards.

- Check copyright and licensing: Just because you can access the data does not 
    mean you have the right to use or redistribute it freely. Always review the 
    site’s terms of use and any applicable copyright rules.

- Be transparent: When using scraped data in reports, analyses, or projects, 
    clearly cite your sources and acknowledge where the information came from. 
    Transparency helps maintain credibility and respect for the original content 
    creators.

### Basics of Web Requests

Before you can extract any data from a website, you need to first fetch the 
content of the page. In Python, this is typically done using the requests 
library, which allows you to send HTTP requests and receive responses. 
Understanding how these requests work is key to effective web scraping.

If you haven't done so already, you will need install requests to your terminal.

```bash
pip install requests
```

#### HTML Basics

To scrape web pages effectively, it is important to understand the basic 
structure of HTML, which is the language used to create web pages. Web pages are 
made up of elements enclosed in tags. Some common tags include:

- `<body>`: this tag identifies the main body of the website, which contains the 
    content that is visible to users.

- `<table>`: This tag identifies a table element, which is used to organize data 
    in rows and columns.
- `<tbody>`: This tag identifies the body of the table, which contains all the 
    rows of the table.
- `<tr>`: This tag identifies a table row, which contains individual cells (`<td>`) 
    with data.

#### Response Status Codes

Every HTTP request returns a status code that indicates whether it was successful.

- `200` means the request was successful, and the server returned the requested 
    content.
- `404` means the page was not found, which usually indicates the URL is incorrect.
- `403` means access is forbidden, which often happens when a server blocks 
    automated requests.
- `500` indicates a server error on the website.

Checking these codes allows you to handle errors gracefully and avoid scraping 
pages that are unavailable or blocked.

#### HTTP Methods

The two most common HTTP methods used in web scraping are GET and POST.

- `GET` requests are used to retrieve data from a web server. This is the method 
you will use most often because you are usually trying to download the content 
of a page.
- `POST` requests are used to send data to a server, often when submitting forms 
or interacting with a website. Some websites require POST requests to access 
certain content or search results.

#### Using Headers and User-Agent Strings

Web servers sometimes treat automated requests differently from requests made by 
real browsers. By adding headers, you can make your requests look more like they 
are coming from a normal user.

A common header to include is the User-Agent, which identifies the browser and 
device making the request. Here is an example of what a header can look like:

```{python}
import requests

url = 'https://uconn.edu'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
}

response = requests.get(url, headers=headers)
print(response.status_code)  # prints the HTTP status code
print(response.text[:500])   # prints the first 500 characters of the HTML
```

### An Introduction to BeautifulSoup

BeautifulSoup is one of the most popular Python libraries for web scraping. It 
helps you collect and organize information from web pages written in HTML or XML. 
When you look at a web page’s source code, it often looks like a confusing block 
of text. BeautifulSoup turns that chaos into a clean, organized structure that 
you can easily search and navigate. It is a reliable tool for collecting data 
from static websites such as news articles, course lists, or research archives.

#### Installing and Importing BeautifulSoup

To use BeautifulSoup, you first need to install it.

```bash
pip install beautifulsoup4
```

Once installed, import it into your Python script:

```{python}
from bs4 import BeautifulSoup
```

BeautifulSoup works well alongside the requests library, creating a simple and 
efficient way to download and process web pages.

#### Loading HTML into BeautifulSoup

Once you have downloaded the HTML content of a web page, the next step is to 
load it into BeautifulSoup.

```{python}
url = "https://decisiondrivers.com/nyc/zip-code/"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
}

response = requests.get(url, headers=headers)
print("Status code:", response.status_code)

soup = BeautifulSoup(response.text, "html.parser")
```

The `requests.get()` function fetches the page, and `BeautifulSoup()` parses it 
into a structure that can be easily searched. The `"html.parser"` argument tells 
BeautifulSoup how to interpret the HTML. The resulting `soup` object becomes your 
workspace for exploring and extracting information. You can now search for tags 
like `<table>`, `<tr>`, or `<td>` to extract the data you need.

Since the status code prints 200, that means the request worked successfully and 
the page’s content is ready to scrape.

#### Navigating and Searching the Parse Tree

BeautifulSoup organizes the HTML into a tree like structure that you can move 
through. Each tag, paragraph, or link becomes something you can directly access.

The most common ways to search are `find()`, `find_all()`, and `select()`.

- `find()` – finds the first instance of a tag
- `find_all()` – finds all instances of a tag
- `select()` – finds elements using CSS selectors

```{python}
# Find the first heading on the page
heading = soup.find('h1')

# Find all links on the page
links = soup.find_all('a')

# Use a CSS selector to find specific items
nav_links = soup.select('nav a')
```

#### Extracting Data

Before writing any scraping code, it’s helpful to understand where the data 
actually sits within the webpage. You can right-click anywhere on the site and 
choose Inspect to open your browser’s developer tools. This reveals the HTML 
structure of the page, where information is organized using tags such as `<table>`, 
`<tr>`, and `<td>`. These tags define how the data is arranged, helping you 
identify exactly which elements you’ll need to target with BeautifulSoup.

For example, on the [Decision Drivers NYC ZIP Codes]
(https://decisiondrivers.com/nyc/zip-code/) page, the ZIP code information is 
not stored inside a table but listed under each borough heading. Every borough 
name appears as a heading tag, such as `<h2>`, followed by a list of ZIP codes 
written in plain text  or inside `<ul>` and `<li>` tags. Instead of looping 
through table rows, you can search for each heading, record the borough name, 
and then capture the ZIP codes that appear beneath it. This structure works well 
with `BeautifulSoup` because it allows you to identify the borough from the 
heading and pair it with its corresponding ZIP codes by reading the text that 
follows.  

```{python}
import pandas as pd

boroughs = []
zips = []

# Split the entire text into lines, then look for boroughs and ZIPs
lines = soup.get_text("\n", strip=True).splitlines()
current_borough = None

for line in lines:
    text = line.strip()
    # If the line is a borough name, update the current borough
    if text.upper() in ["MANHATTAN", "BROOKLYN", "QUEENS", "BRONX", 
    "STATEN ISLAND"]:
        current_borough = text.title()
        continue

    # If the line is a 5-digit number, it’s a ZIP code
    if current_borough and text.isdigit() and len(text) == 5:
        boroughs.append(current_borough)
        zips.append(text)

# Create a clean DataFrame
df = pd.DataFrame({"Borough": boroughs, "ZIP": zips})
df = df.drop_duplicates().sort_values(["Borough", "ZIP"]).reset_index(drop=True)

# Display the first few rows
print(df.head(10))
print("\nTotal ZIP codes found:", len(df))
```

#### Extended Example

This is an example that will be useful for our midterm assignment. Here, we 
scrape the addresses of all the precincts in NYC. 

```{python}
url = "https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
}

response = requests.get(url, headers=headers)
print("Status code:", response.status_code)

soup = BeautifulSoup(response.text, "html.parser")

# Step 1: Locate the table containing the precinct information
table = soup.find("table", class_="rt")

# Step 2: If the table is found, extract all precinct rows
if table:
    rows = table.find_all("tr")
else:
    print("No table found on this page.")
    rows = []

# Step 3: Prepare a list to store precinct data as dictionaries
precinct_data = []

# Step 4: Loop through each row and collect precinct name and address
for row in rows:
    precinct_cell = row.find("td", attrs={"data-label": "Precinct"})
    address_cell = row.find("td", attrs={"data-label": "Address"})

    if precinct_cell and address_cell:
        precinct_name = precinct_cell.text.strip()
        address = address_cell.text.strip()
        precinct_data.append({
            "Precinct": precinct_name,
            "Address": address
        })

precincts_df = pd.DataFrame(precinct_data)

# Step 6: Display a quick summary
print(f"Extracted {len(precincts_df)} precincts successfully.")
print(precincts_df.head())
```


### An Introduction to Selenium

While BeautifulSoup works best for static websites, not all web pages are 
created equal.  Many modern sites use JavaScript to load content after the 
initial HTML has already been sent to the browser. This means that if you use 
`requests` and `BeautifulSoup`, you might get an empty page or partial data. 
This is where you use Selenium.

Selenium is a powerful browser automation tool that lets you control an actual 
web browser, like Chrome or Firefox, through Python. It can click buttons, fill 
out forms, scroll through pages, and load JavaScript rendered content before 
scraping it. You can then pass the fully loaded page to BeautifulSoup for 
parsing.

#### Installing and Setting Up Selenium

To use Selenium, you first need to install it.

```bash
pip install selenium
```

Selenium also requires a WebDriver, which acts as a bridge between Python and 
your browser. If you are using Google Chrome, download ChromeDriver and make 
sure it is installed correctly. You can verify this by typing the following 
command in your terminal:

```bash
chromedriver --version
```

If you are on macOS and see the error “chromedriver cannot be opened because the 
developer cannot be verified,” you can fix it by running this command:

```bash
xattr -d com.apple.quarantine $(which chromedriver)
```

After this, your WebDriver should work normally.

Now we can work on the code itself. This is how we set up Selenium:

```{python}
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import os
```

Before launching Chrome, it’s common to set a few options that control how it 
behaves. For scraping, “headless” mode is useful because it runs Chrome without 
opening a window. It is faster and less distracting.

```{python}
# Create Chrome options to control browser behavior
options = Options()

# Run Chrome invisibly (no window)
options.add_argument("--headless")

# Disable GPU acceleration for smoother headless operation
options.add_argument("--disable-gpu")

# Helps avoid permission issues in some environments
options.add_argument("--no-sandbox")
```

Instead of typing out the full path manually, this function looks through your 
system’s PATH to find where chromedriver is installed. 

```{python}
def find_chromedriver():

    # Loop through all directories listed in the system PATH
    for path in os.environ["PATH"].split(os.pathsep):

        # Build the potential path to chromedriver
        driver_path = os.path.join(path, "chromedriver")

        # Check if it exists and is executable
        if os.path.isfile(driver_path) and os.access(driver_path, os.X_OK):

            return driver_path  # Return the path if found

    return None  # Return None if not found

# Run the function to locate chromedriver
chromedriver_path = find_chromedriver()

# If found, print its path
if chromedriver_path:
    print("Found ChromeDriver at:", chromedriver_path)

# If not found, raise an error with instructions
else:
    raise FileNotFoundError("ChromeDriver not found.")

```

Once you have the driver path and your browser settings, you can start Chrome.

```{python}
# Import the Service class and connect the driver
service = Service(chromedriver_path)

# Start the Chrome WebDriver with the defined options
driver = webdriver.Chrome(service=service, options=options)
```

At this point, Selenium is ready. You can visit a webpage by calling:

```{python}
driver.get("https://example.com")
```



#### Using Selenium to Load Dynamic Pages

Here is a simple example that shows how Selenium can open a website, wait 
for it to load, and then pass the content to BeautifulSoup.

```{python}
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd

def scrape_schedule(url, league):  # league = "NFL" or "MLB"
    league = league.upper()
    # Identify the correct team link pattern based on the league
    team_href = "/nfl/team/_/name" if league == "NFL" else "/mlb/team/_/name"

    # Configure and launch Chrome
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)

    # Load the ESPN schedule page
    driver.get(url)
    wait = WebDriverWait(driver, 25)

    # Wait until key elements are visible (date headers and team links)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 
    "div.Table__Title")))
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 
    f"a.AnchorLink[href*='{team_href}']")))

    rows_out = []

    # Extract data by date section
    # Each schedule day starts with a <div class="Table__Title"> element
    for title in driver.find_elements(By.CSS_SELECTOR, "div.Table__Title"):
        date_text = title.text.strip()
        if not date_text:
            continue  # Skip if the date header is empty

        # The table directly after the title contains the matchups
        try:
            table = title.find_element(By.XPATH, "following::table[1]")
        except:
            continue  # Skip if structure differs

        # Extract teams for each game
        for row in table.find_elements(By.XPATH, 
        ".//tr[contains(@class,'Table__TR')]"):
            teams = [
                a.text.strip()
                for a in row.find_elements(By.CSS_SELECTOR, 
                f"td a.AnchorLink[href*='{team_href}']")
                if a.text.strip()
            ]
            if len(teams) >= 2:
                # Format as "Away @ Home"
                rows_out.append({"Date": date_text, "Matchup": f"{teams[0]} @ {
                    teams[1]}"})

    # --- Step 5: Clean up and close browser ---
    driver.quit()

    # Build a clean DataFrame
    df = pd.DataFrame(rows_out, columns=["Date", "Matchup"])
    df["League"] = league
    return df


# --- Step 6: Run for both leagues ---
nfl_df = scrape_schedule("https://www.espn.com/nfl/schedule", "NFL")
mlb_df = scrape_schedule("https://www.espn.com/mlb/schedule", "MLB")

print("===== NFL =====")
print(nfl_df)
print("\n===== MLB =====")
print(mlb_df)
```

Here is how the code works:

1. Setting Up Selenium
- The first part initializes the Chrome browser in headless mode so it runs 
quietly in the background.

2. Waiting for the Page to Load
- Selenium doesn’t automatically know when a website’s content is ready. Using 
`WebDriverWait` and `expected_conditions`, the script pauses until:
    - A date header (`div.Table__Title`) appears
    - A team link with a specific pattern (`/nfl/team/_/name` or 
    `/mlb/team/_/name`) exists

3. Looping Through Each Day’s Games
- Each date on the ESPN page appears as a header `(Table__Title)`, followed by a 
table of games. The script finds each table using the XPath rule 
`"following::table[1]"`, meaning the first table that comes after the header.

4. Extracting Teams from Each Row
- Inside each table, every row `(tr)` represents one game. The team names are 
stored in `<a>` tags with URLs containing the league’s team name pattern. 
Selenium grabs these links and combines them into `"Away @ Home"` format.

5. Creating a DataFrame
- Once all data is collected, it’s turned into a Pandas DataFrame with columns:
    - `Date`: the game date from the header
    - `Matchup`: formatted team pairing
    - `League`: either NFL or MLB

6. Closing the Browser
- Always close the browser using `driver.quit()` to free up system resources.

### Further Reading 
1. [Python Web Scraping Tutorials](
    https://realpython.com/tutorials/web-scraping/) 
2. [Python Web Scraping: Full Tutorial With Examples](
    https://www.scrapingbee.com/blog/web-scraping-101-with-python/) 
3. [Ultimate Guide to Web Scraping with Python](
    https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/) 
4. [BeautifulSoup Documentation](
    https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
5. [Webscraping With Selenium](
    https://medium.com/@datajournal/web-scraping-with-selenium-955fbaae3421) 

