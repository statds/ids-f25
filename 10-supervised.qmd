# Supervised Learning

## Decision Trees

Decision trees are widely used supervised learning models that predict 
the value of a target variable by iteratively splitting the dataset 
based on decision rules derived from input features. The model 
functions as a piecewise constant approximation of the target 
function, producing clear, interpretable rules that are easily 
visualized and analyzed [@breiman1984classification]. Decision trees 
are fundamental in both classification and regression tasks, serving 
as the building blocks for more advanced ensemble models such as 
Random Forests and Gradient Boosting Machines.


### Recursive Partition Algorithm

The core mechanism of a decision tree algorithm is the identification 
of optimal splits that partition the data into subsets that are 
increasingly homogeneous with respect to the target variable. At any 
node $m$, the data subset is denoted as $Q_m$ with a sample size of 
$n_m$. The objective is to find a candidate split $\theta$, defined 
as a threshold for a given feature, that minimizes an impurity or 
loss measure $H$.

When a split is made at node $m$, the data is divided into two 
subsets: $Q_{m,l}$ (left node) with sample size $n_{m,l}$, and 
$Q_{m,r}$ (right node) with sample size $n_{m,r}$. The split 
quality, measured by $G(Q_m, \theta)$, is given by:

$$
G(Q_m, \theta) = \frac{n_{m,l}}{n_m} H(Q_{m,l}(\theta)) + 
\frac{n_{m,r}}{n_m} H(Q_{m,r}(\theta)).
$$

The algorithm aims to identify the split that minimizes the impurity:

$$
\theta^* = \arg\min_{\theta} G(Q_m, \theta).
$$

This process is applied recursively at each child node until a 
stopping condition is met.

+ **Stopping Criteria:** 
  The algorithm stops when the maximum tree depth is reached or when
  the node sample size falls below a preset threshold.
+ **Pruning:**
  Reduce the complexity of the final tree by removing
  branches that add little predictive value. This reduces overfitting
  and improves generalization.

### Search Space for Possible Splits

At each node, the search space for possible splits 
comprises all features in the dataset and potential thresholds derived 
from the feature values. For a given feature, the algorithm 
considers each of its unique value in the current node as a possible 
split point. The potential thresholds are typically set as midpoints 
between consecutive unique values, ensuring effective partition.

Formally, let the feature set be $\{X_1, X_2, \ldots, X_p\}$, where 
$p$ is the total number of features, and let the unique values of 
feature $X_j$ at node $m$ be denoted by 
$\{v_{m,j,1}, v_{m,j,2}, \ldots, v_{m,j,k_{mj}}\}$. The search space
at node $m$ includes:


- Feature candidates: $\{X_1, X_2, \ldots, X_p\}$.
- Threshold candidates for $X_j$: 
  $$
  \left\{ \frac{v_{m,j,i} + v_{m,j,i+1}}{2} \mid 1 \leq i < k_{mj} \right\}.
  $$

While the complexity of this search can be 
substantial, particularly for high-dimensional data or features with 
numerous unique values, efficient algorithms use sorting and single-pass 
scanning techniques to mitigate the computational cost.


### Metrics

#### Classification
In classification, the split quality metric measures how pure the
resulting nodes are after a split. A pure node contains observations
that predominantly belong to a single class. 

+ **Gini Index**: The Gini index measures node impurity by
  the probability that two observations randomly drawn from the node
  belong to different classes. A perfect split (all instances belong
  to one class) has a Gini index of 0. At node $m$, the Gini index is
  $$
  H(Q_m) = \sum_{k=1}^{K} p_{mk} (1 - p_{mk}) 
  = 1 - \sum_{k=1}^n p_{mk}^2,
  $$
  where $p_{mk}$ is the proportion of samples of class $k$ at node $m$;
  and $K$ is the total number of classes
  The Gini index is often preferred for its speed and simplicity, and
  it’s used by default in many implementations of decision trees,
  including `sklearn`.

  The Gini index originates from the Gini coefficient, introduced by
  Corrado Gini in 1912 to quantify inequality in income
  distributions. In that context, the Gini coefficient measures how
  unevenly a quantity (such as wealth) is distributed across a
  population. Decision tree algorithms adapt this concept of
  inequality to measure the impurity of a node: instead of wealth, the
  distribution concerns class membership. A perfectly pure node, where
  all observations belong to the same class, represents complete
  equality and yields a Gini index of zero. As class proportions
  become more mixed, inequality in class membership increases, leading
  to higher impurity values. Thus, the Gini index used in decision
  trees can be viewed as a statistical measure of diversity or
  heterogeneity derived from Gini’s original work on inequality.
  
+ **Entropy (Information Gain):** Derived from information theory,
  entropy quantifies the disorder of the data at a node. Lower entropy
  means higher purity. At node $m$, it is defined as
  $$
  H(Q_m) = - \sum_{k=1}^{K} p_{mk} \log p_{mk}.
  $$
  Entropy is commonly used in decision tree algorithms like ID3 and
  C4.5. The choice between Gini and entropy often depends on specific
  use cases, but both perform similarly in practice.
  
+ **Misclassification Error:** Misclassification error focuses on
  the most frequent class in the node. It measures the proportion of
  samples that do not belong to the majority class. Although less
  sensitive than Gini and entropy, it can be useful for classification
  when simplicity is preferred. At node $m$, it is defined as
  $$
  H(Q_m) = 1 - \max_k p_{mk},
  $$
  where $\max_k p_{mk}$ is the largest proportion of samples belonging
  to any class $k$.

#### Regression Criteria

In regression, the goal is to minimize the spread or variance of
the target variable within each node.

+ **Mean Squared Error (MSE):** MSE is the average squared
  difference between observed and predicted values (mean
  of the target in the node). The smaller the MSE, the better the
  fit. At node $m$, it is 
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} (y_i - \bar{y}_m)^2,
  $$
  where
    - $y_i$ is the actual value for sample $i$;
    - $\bar{y}_m$ is the mean value of the target at node $m$;
    - $n_m$ is the number of samples at node $m$.

  MSE works well when the target is continuous and normally distributed.

+ **Half Poisson Deviance:** Used for count target, 
  the Poisson deviance measures the variance in the
  number of occurrences of an event. At node $m$, it is
  $$
  H(Q_m) = \sum_{i=1}^{n_m} \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right),
  $$
  where $\hat{y}_i$ is the predicted count. This criterion is
  especially useful when the target variable represents discrete
  counts, such as predicting the number of occurrences of an event.

+ **Mean Absolute Error (MAE):** MAE aims to minimize the absolute
  differences between actual and predicted values.
  While it is more robust to outliers than MSE, it is slower
  computationally due to the lack of a closed-form solution for
  minimization. At node $m$, it is
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} |y_i - \bar{y}_m|.
  $$
  MAE is useful when you want to minimize large deviations and can be
  more robust in cases where outliers are present in the data.


### Ames Housing Example

The Ames Housing data are used to illustrate a regression tree model 
for predicting log house price. 

As before, we retrive the data from `OpenML`.

```{python}
import openml
import pandas as pd
import numpy as np

# Load Ames Housing dataset (OpenML ID 42165)
dataset = openml.datasets.get_dataset(42165)
df, *_ = dataset.get_data()
df["LogPrice"] = np.log(df["SalePrice"])
```

A decision tree partitions the feature 
space into regions where the average log price is relatively constant.


```{python}
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
import plotnine as gg
import pandas as pd

numeric_features = [
    "OverallQual", "GrLivArea", "GarageCars",
    "TotalBsmtSF", "YearBuilt", "FullBath"
]
categorical_features = ["KitchenQual"]

preprocessor = ColumnTransformer([
    ("num", "passthrough", numeric_features),
    ("cat", OneHotEncoder(drop="first"), categorical_features)
])

X = df[numeric_features + categorical_features]
y = df["LogPrice"]

depths = range(2, 11)
cv_scores = [
    cross_val_score(
        Pipeline([
            ("pre", preprocessor),
            ("model", DecisionTreeRegressor(max_depth=d, random_state=0))
        ]),
        X, y, cv=5, scoring="r2"
    ).mean()
    for d in depths
]

list(zip(depths, cv_scores))
```

Cross-validation identifies an appropriate tree depth that balances fit
and generalization. A too-deep tree overfits, while a shallow tree 
misses structure.

```{python}
dt = Pipeline([
    ("pre", preprocessor),
    ("model", DecisionTreeRegressor(max_depth=4, random_state=0))
])

dt.fit(X, y)
y_pred = dt.predict(X)

df_pred = pd.DataFrame({"Observed": y, "Predicted": y_pred})

(gg.ggplot(df_pred, gg.aes(x="Observed", y="Predicted")) +
 gg.geom_point(alpha=0.5) +
 gg.geom_abline(slope=1, intercept=0, linetype="dashed") +
 gg.labs(title="Decision Tree Regression on Ames Housing",
         x="Observed Log Price", y="Predicted Log Price"))
```

The plot shows predicted versus observed log prices. A well-fitted 
model has points close to the diagonal. The decision tree naturally 
captures nonlinear effects and interactions, though its predictions are 
piecewise constant, producing visible step patterns.


<!-- {{< include _random_forest.qmd >}} -->


## Gradient-Boosted Models

Gradient boosting is a powerful ensemble technique in machine learning
that combines multiple weak learners into a strong predictive
model. Unlike bagging methods, which train models independently,
gradient boosting fits models sequentially, with each new model
correcting errors made by the previous ensemble
[@friedman2001greedy]. While decision trees are commonly used as weak
learners, gradient boosting can be generalized to other base
models. This iterative method optimizes a specified loss function by
repeatedly adding models designed to reduce residual errors.


### Introduction

Gradient boosting builds on the general concept of boosting, aiming to
construct a strong predictor from an ensemble of sequentially trained
weak learners. The weak learners are often shallow decision trees
(stumps), linear models, or generalized additive models
[@hastie2009elements]. Each iteration adds a new learner focusing
primarily on the data points poorly predicted by the existing
ensemble, thereby progressively enhancing predictive accuracy.


Gradient boosting's effectiveness stems from:

- Error Correction: Each iteration specifically targets previous
  errors, refining predictive accuracy.
- Weighted Learning: Iteratively focuses more heavily on
  difficult-to-predict data points.
- Flexibility: Capable of handling diverse loss functions and various
  types of predictive tasks.


The effectiveness of gradient-boosted models has made them popular
across diverse tasks, including classification, regression, and
ranking. Gradient boosting forms the foundation for algorithms such as
XGBoost [@chen2016xgboost], LightGBM [@ke2017lightgbm], and CatBoost
[@prokhorenkova2018catboost], known for their high performance and
scalability.


### Gradient Boosting Process

Gradient boosting builds an ensemble by iteratively minimizing the
residual errors from previous models. This iterative approach
optimizes a loss function, $L(y, F(x))$, where $y$ represents the
observed target variable and $F(x)$ the model's prediction for a
given feature vector $x$. 

Key concepts:

- Loss Function: Guides model optimization, such as squared error for
  regression or logistic loss for classification.
- Learning Rate: Controls incremental updates, balancing training
  speed and generalization.
- Regularization: Reduces overfitting through tree depth limitation,
  subsampling, and L1/L2 penalties.

#### Model Iteration

The gradient boosting algorithm proceeds as follows:

1. Initialization:
   Define a base model $F_0(x)$, typically the mean of the target
   variable for regression or the log-odds for classification.

2. Iterative Boosting:
   At each iteration $m$:
   - Compute pseudo-residuals representing the negative gradient of
     the loss function at the current predictions. For each
     observation $i$:
   $$
   r_i^{(m)} = -\left.\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right|_{F(x)=F_{m-1}(x)},
   $$
   where $x_i$ and $y_i$ denote the feature vector and observed value
   for the $i$-th observation, respectively. The residuals represent the
   direction of steepest descent in function space, so fitting a learner
   to them approximates a gradient descent step minimizing $L(y, F(x))$.

   - Fit a new weak learner $h_m(x)$ to these residuals.

   - Update the model:
   $$
   F_m(x) = F_{m-1}(x) + \eta \, h_m(x),
   $$
   where $\eta$ is a small positive learning rate (e.g., 0.01–0.1),
   controlling incremental improvement and reducing overfitting.
   
   In some implementations, the update step includes an additional
   multiplier determined by a one-dimensional line search that minimizes
   the loss function at each iteration. Specifically, the optimal step
   length is defined as
   $$
   \gamma_m = \arg\min_\gamma \sum_{i=1}^n 
    L\bigl(y_i,\, F_{m-1}(x_i) + \gamma\, h_m(x_i)\bigr),
   $$
   leading to an updated model of the form
   $$
    F_m(x) = F_{m-1}(x) + \eta\, \gamma_m\, h_m(x),
   $$
   where $\eta$ remains a shrinkage factor controlling the overall
   rate of learning, while $\gamma_m$ adjusts the step size adaptively
   at each iteration.

3. Final Model: After $M$ iterations, the ensemble model is:
  $$
  F_M(x) = F_0(x) + \sum_{m=1}^M \eta \, h_m(x).
  $$


Stochastic gradient boosting is a variant that enhances gradient
boosting by introducing randomness through subsampling at each
iteration, selecting a random fraction of data points (typically
50\%--80\%) to fit the model \citep{friedman2002stochastic}. This
randomness helps reduce correlation among trees, improve model
robustness, and lower the risk of overfitting.

### Boosted Trees with Ames Housing

Boosted trees apply the gradient boosting framework to decision trees.
They build an ensemble of shallow trees, each trained to correct the
residual errors of the preceding ones. By sequentially emphasizing
observations that are difficult to predict, the model progressively
improves its overall predictive accuracy.
We now apply gradient boosting using the same preprocessed features.
Boosting combines many shallow trees, each correcting the residual
errors of its predecessors, to improve predictive accuracy.

```{python}
from sklearn.ensemble import GradientBoostingRegressor

# Define a range of tree counts for tuning
n_estimators_list = [50, 100, 200, 400]

cv_scores_gb = [
    cross_val_score(
        Pipeline([
            ("pre", preprocessor),
            ("model", GradientBoostingRegressor(
                n_estimators=n,
                learning_rate=0.05,
                max_depth=3,
                random_state=0))
        ]),
        X, y, cv=5, scoring="r2"
    ).mean()
    for n in n_estimators_list
]

list(zip(n_estimators_list, cv_scores_gb))
```

Cross-validation shows how increasing the number of boosting rounds
initially improves performance but eventually risks overfitting when
too many trees are added.

```{python}
gb = Pipeline([
    ("pre", preprocessor),
    ("model", GradientBoostingRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=3,
        random_state=0))
])

gb.fit(X, y)
y_pred_gb = gb.predict(X)

df_pred_gb = pd.DataFrame({"Observed": y, "Predicted": y_pred_gb})

(gg.ggplot(df_pred_gb, gg.aes(x="Observed", y="Predicted")) +
 gg.geom_point(alpha=0.5) +
 gg.geom_abline(slope=1, intercept=0, linetype="dashed") +
 gg.labs(title="Gradient-Boosted Regression on Ames Housing",
         x="Observed Log Price", y="Predicted Log Price"))
```

The boosted model produces predictions that are generally closer to the
45-degree line than a single tree, reflecting improved accuracy and
smoother response across the feature space.


Gradient-boosted trees introduce several parameters that govern model
complexity, learning stability, and overfitting control:

+ `n_estimators`: the number of trees (boosting rounds). More trees
   can reduce bias but increase computation and risk of overfitting.
   learning_rate — the shrinkage parameter $\eta$ controlling the
   contribution of each new tree. Smaller values (e.g., 0.05 or 0.01)
   require more trees but often yield better generalization.
+ `max_depth`: the maximum depth of each individual tree, limiting
   the model’s ability to overfit local noise. Shallow trees (depth
   2–4) are typical weak learners. 
+ `subsample`: the fraction of data used in each iteration. Values
   below 1.0 introduce randomness (stochastic boosting), improving
   robustness and reducing correlation among trees.
+ `min_samples_split` and `min_samples_leaf`: minimum numbers of
  observations required for splitting or forming leaves. These control
  tree granularity and help regularize the model. 
  
In practice, moderate learning rates with a sufficiently large number
of estimators and shallow trees often perform best, balancing bias,
variance, and computational cost.

### XGBoost: Extreme Gradient Boosting

XGBoost is a scalable and 
efficient implementation of gradient-boosted decision trees 
[@chen2016xgboost]. It has become one of the most widely used machine 
learning methods for structured data due to its high predictive 
performance, regularization capabilities, and speed. XGBoost builds 
an ensemble of decision trees in a stage-wise fashion, minimizing a 
regularized objective that balances training loss and model complexity.


The core idea of XGBoost is to fit each new tree to the *gradient* of 
the loss function with respect to the model’s predictions. Unlike 
traditional boosting algorithms like AdaBoost, which use only 
first-order gradients, XGBoost optionally uses second-order derivatives 
(Hessians), enabling better convergence and stability 
[@friedman2001greedy].


XGBoost is widely used in data science competitions and real-world 
applications. It supports regularization (L1 and L2), handles missing 
values internally, and is designed for distributed computing.


XGBoost builds upon the same foundational idea as gradient boosted 
machines—sequentially adding trees to improve the predictive model—
but introduces a number of enhancements:

| Aspect               | Traditional GBM                            | XGBoost                        |
|----------------------|---------------------------------------------|--------------------------------|
| Implementation       | Basic gradient boosting                    | Optimized, regularized boosting |
| Regularization       | Shrinkage only                             | L1 and L2 regularization        |
| Loss Optimization    | First-order gradients                      | First- and second-order         |
| Missing Data         | Requires manual imputation                 | Handled automatically           |
| Tree Construction    | Depth-wise                                 | Level-wise (faster)             |
| Parallelization      | Limited                                    | Built-in                        |
| Sparsity Handling    | No                                          | Yes                             |
| Objective Functions  | Few options                                | Custom supported                |
| Cross-validation     | External via `GridSearchCV`                | Built-in `xgb.cv`               |

XGBoost is therefore more suitable for large-scale problems and provides 
better generalization performance in many practical tasks.

## Support Vector Machine

This section was prepared by Emma Wishneski, a senior pursuing a dual
degree in Applied Data Analysis and Political Science, with a minor 
in Geographic Information Sciences.  

This section will explain support vector machine, a powerful supervised
machine learning algorithm used for both classification and regression tasks.

### Introduction

**Support Vector Machine (SVM)** is a helpful supervised machine learning
technique used to assist mainly with classification. It essentially works
by finding the optimal hyperplane that will separate data into different
classes the best.  

The goal for this is to maximize the margin between the data points of the
different classes, in order to make classifications best suited to the data. 

Note:Supervised learning uses labeled data to predict outcomes, while 
unsupervised learning uses unlabeled data. 

### Historical Background
- Initially began with Vladimir Vapnik’s “Generalized Portrait Method” (1960s)
which laid the groundwork for what is now SVMs. 

- In the 1990s, key advancements were made, including the “kernel trick” and
the “soft margin" which expanded to algorithm to work with nonlinear data, 
and made it less affected by extreme data points. 

- In the early 2000’s, an *unsupervised* version called “Support Vector 
Clustering”  was developed

- Currently, deep learning has surpassed SVMs in some applications, 
but they remain a powerful and widely used tool still. 

### Key Concepts of SVM

**Hyperplane:** The decision boundary that separates different classes
in the feature space.  
    
**Margin:** The distance between the hyperplane and the nearest data point
from either class. A larger margin indicates better separation.  
  
**Support Vectors:** Data points that lie closest to the hyperplane and
influence its position. 

The central purpose of a SVM is to find the *best* boundary to separate
the data into classes. 

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Creating a simple dataset
X = np.array([[1, 2], [2, 3], [3, 3], [6, 6], [6, 7], [7, 8]])
y = [0, 0, 0, 1, 1, 1]

# Fitting the SVM model
clf = svm.SVC(kernel='linear', C=1)
clf.fit(X, y)

# Plotting the decision boundary
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(0, 10)
yy = a * xx - (clf.intercept_[0]) / w[1]

plt.plot(xx, yy, 'k-')
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title('Linear SVM Example')
plt.show()
```


Clear example using linear data
```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# ---------- Data ----------
X = np.array([[1, 2], [2, 3], [3, 3], [6, 6], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1, 1])

# ---------- Helper: nice SVM plot ----------
def plot_svm_2d(clf, X, y, title="SVM decision boundary", padding=1.0, steps=400,
                plot_support=True, ax=None):
    """
    Shades decision regions and draws hyperplane (0) with margins (-1, +1).
    Works for linear or nonlinear kernels (uses decision_function).
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(6, 5))

    # Limits with padding
    x_min, x_max = X[:, 0].min() - padding, X[:, 0].max() + padding
    y_min, y_max = X[:, 1].min() - padding, X[:, 1].max() + padding

    # Meshgrid for background shading
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, steps),
        np.linspace(y_min, y_max, steps)
    )

    # Decision function on grid
    # (SVC has decision_function for both linear and RBF kernels)
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    # 1) Shade regions by predicted class
    preds = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    ax.contourf(xx, yy, preds, alpha=0.20)

    # 2) Draw margins (-1 and +1) and the decision boundary (0)
    cs = ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=2, colors='k')

    # 3) Scatter original points
    sc = ax.scatter(X[:, 0], X[:, 1], c=y, s=60, edgecolor='k', label='Data Points')

    # 4) Highlight support vectors if available
    if plot_support and hasattr(clf, "support_vectors_"):
        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
                   s=120, facecolors='none', edgecolors='k', linewidths=1.5, label='support vectors')

    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)
    ax.set_xlabel("x1")
    ax.set_ylabel("x2")
    ax.set_title(title)
    ax.legend(loc="best")
    return ax


# ---------- Example A: Linear SVM with margins ----------
linear_clf = make_pipeline(
    StandardScaler(),
    SVC(kernel='linear', C=1.0)  # Can increase C for a “tighter” margin; decrease for wider margin
)
linear_clf.fit(X, y)

plt.figure()
plot_svm_2d(linear_clf, X, y, title="Linear SVM (C=1.0)")
plt.show()
```

Shaded regions show the boundary.  

```{python}
#with circles
rbf_clf = make_pipeline(
    StandardScaler(),
    SVC(kernel='rbf', C=10.0, gamma=0.5) 
)
rbf_clf.fit(X, y)

plt.figure()
plot_svm_2d(rbf_clf, X, y, title="SVM With Circles to show support vectors")

plt.show()
```


### Parameter Tuning
Parameter tuning for SVM involves finding the optimal combination of
hyperparameters like C, kernel, and gamma to improve model performance.
Parameter tuning often uses techniques like grid search, random search,
or Bayesian optimization. You can test different values for these parameters
by creating a parameter grid and using a search algorithm with 
cross-validation to evaluate performance and select the best combination.  

Parameter tuning helps achieve the right balance between bias and variance 
in the model.

### Implementation and Workflow

For a more complex example, we will be using the cleaned NYC crash data 
feather, (courtesy of Wilson Tang), in order to show an example of an SVM.

```{python}
# --- Imports ---
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
```


```{python} 
df = pd.read_feather("data/nyc_crashes_cleaned.feather")
df.head()
```

```{python} 
# defining a new column, injury
## equals 1 if anyone was injured or killed, and 0 otherwise
df["injury"] = np.where(
    (df["number_of_persons_injured"].fillna(0) + df["number_of_persons_killed"].fillna(0)) > 0, 1, 0
)
print(df["injury"].value_counts())

# extracting time features
df["crash_hour"] = df["crash_datetime"].dt.hour
df["crash_dayofweek"] = df["crash_datetime"].dt.dayofweek  # 0 = Monday, 6 = Sunday
```

```{python} 
# Selecting features for modeling
features = [
    "borough",
    "zip_code",
    "vehicle_type_code_1",
    "contributing_factor_vehicle_1",
    "crash_hour",
    "crash_dayofweek"
]

X = df[features]
y = df["injury"]
```

--- 

```{python}
# scatter for lat and long
sns.scatterplot(
    data=df.sample(1000, random_state=42),
    x="longitude", y="latitude",
    hue=((df["number_of_persons_injured"] + df["number_of_persons_killed"]) > 0).astype(int),
    palette={0:"lightgray", 1:"red"}, alpha=0.6
)
plt.title("Spatial Distribution of Injury vs Non-Injury Crashes")
plt.show()

```

### Handling Non-Linear Data (Kernel Trick)
- When data aren’t linearly separable, we can use kernels to let SVMs operate 
in a higher-dimensional space where a linear boundary can exist.  
      - *The Kernel Trick* is essentially a function that allows us to shortcut
      point transformation  
      - They calculate the similarity between two points in the higher 
      dimensional space, without explicitly transforming the data into that 
      space
      - Radial Basis Function (RBF) Kernel is most commonly used. 
      - K(x,y)=e − (γ∣∣x−y∣∣ 2)  where γ is a parameter that controls the 
      influence of each training example.




```{python}
# --- SVM decision plot over latitude/longitude (DataFrame-safe) ---
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# 1) Use only rows with coords + injury defined
df_svm = df.dropna(subset=["longitude", "latitude", "injury"]).copy()

# Keep as DataFrame so column-name selectors in any pipeline step won't break
X = df_svm[["longitude", "latitude"]]   # <-- no .to_numpy()
y = df_svm["injury"].astype(int)

# 2) Fit SVM with scaling
svm_clf = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("svc", SVC(kernel="rbf", C=10.0, gamma=0.5))
])
svm_clf.fit(X, y)

# 3) Build a grid over the map extent (also as a DataFrame with same col names)
GRID_STEPS = 350
pad_lon = 0.01 * (X["longitude"].max() - X["longitude"].min())
pad_lat = 0.01 * (X["latitude"].max() - X["latitude"].min())
x_min, x_max = X["longitude"].min() - pad_lon, X["longitude"].max() + pad_lon
y_min, y_max = X["latitude"].min() - pad_lat, X["latitude"].max() + pad_lat

xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, GRID_STEPS),
    np.linspace(y_min, y_max, GRID_STEPS)
)
grid = np.c_[xx.ravel(), yy.ravel()]
grid_df = pd.DataFrame(grid, columns=["longitude", "latitude"])  # <-- keep names

# decision_function gives signed distance to the margin
Z = svm_clf.decision_function(grid_df).reshape(xx.shape)

# 4) Plot decision regions + boundary + points
plt.figure(figsize=(8,7))

plt.contourf(xx, yy, Z, levels=20, alpha=0.35)
margins = plt.contour(xx, yy, Z, levels=[-1, 0, 1],
                      linestyles=["--", "-", "--"], linewidths=[1, 2, 1], colors="k")
plt.clabel(margins, fmt={-1: "-1", 0: "0", 1: "+1"}, inline=True, fontsize=8)

overlay = df_svm.sample(min(3000, len(df_svm)), random_state=42)
plt.scatter(overlay["longitude"], overlay["latitude"],
            c=overlay["injury"], s=10, alpha=0.5, cmap="coolwarm", label="Crashes")

# Circle support vectors in original coord scale
scaler = svm_clf.named_steps["scaler"]
svc = svm_clf.named_steps["svc"]
sv_scaled = svc.support_vectors_
# Inverse-transform needs a 2D array; wrap in DataFrame with same columns for safety
sv = scaler.inverse_transform(sv_scaled)
plt.scatter(sv[:, 0], sv[:, 1], s=80, facecolors="none", edgecolors="black",
            linewidths=1.2, label="Support Vectors")

plt.title("SVM Decision Regions — Injury vs No Injury (features: lon, lat)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend(loc="upper right")
plt.tight_layout()
plt.show()

```


```{python} 
# train/testing
categorical_features = ["borough", "zip_code", "vehicle_type_code_1", "contributing_factor_vehicle_1"]
numeric_features = ["crash_hour", "crash_dayofweek"]

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features),
        ("num", StandardScaler(), numeric_features)
    ]
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
```


This plot uses Support Vector Machines to classify crash locations as either 
injury crashes or non-injury crashes, using only longitude and latitude as 
features. Each point is a crash, and the background colors represent the SVM’s 
predicted decision regions

Because I used a nonlinear RBF kernel, the SVM is able to draw flexible, curved 
decision boundaries. These boundaries adapt to how crash patterns cluster 
geographically instead of forcing a straight line. The shaded background shows 
how the model believes the city is divided into zones where injuries are more 
or less likely

The contour lines represent the margins of the SVM classifier. The solid line 
is the main decision boundary separating the two classes. The dashed lines show
the +1 and −1 margins on either side. These margins are where the support 
vectors lie.

The points shown with black outlines are the support vectors — these are the 
crashes that sit closest to the decision boundary. They are the most important 
observations in the dataset, because they determine where the SVM draws its 
separating surface. If we removed or changed them, the boundary would shift.

You can see that the regions are shaped around the natural clustering of
crashes in places like Manhattan, Brooklyn, and parts of Queens. The model 
finds pockets where injury crashes are more concentrated, even though there’s 
no simple linear separation between injury and non-injury crashes. In practice,
these results show that spatial coordinates alone do not strongly separate 
injury vs non-injury crashes.

This example highlights why SVMs with nonlinear kernels are powerful: when the 
true pattern is complex and doesn’t separate cleanly with a straight line, 
SVM learns a curved boundary that fits the underlying structure. This is 
exactly the type of problem where linear models fail but SVM shines. This 
flexibility helps capture complex geographic patterns that a linear model 
would miss.

### Explanation
- SVMs work by finding the hyperplane that best separates classes in the 
feature space.
During training, the model identifies support vectors—the data points closest 
to the decision boundary that define the position and orientation of this 
hyperplane.  
- The model’s objective is to maximize the margin between these support vectors,
which helps improve generalization to unseen data. Maximizing the margin reduces
model variance and helps avoid overfitting because the classifier must separate 
the classes with the widest possible buffer. 
- Once trained, the SVM uses this optimized boundary to classify new 
observations based on which side of the hyperplane they fall on, effectively 
scoring them relative to a threshold.
- When applied to NYC crash data, SVM struggled to find a highly distinct 
boundary because injuries occur at all hours of the day. This illustrates an 
important takeaway: SVM is most effective when the features produce naturally 
separable clusters.

### Real World Applications
- Financial Forecasting
    - Uses classification to predict stock price & movement
  
- Spam Detection
    - Classifies documents into different categories 
  
- Bioinformatics
    - Classification of genes
    - Medical Diagnoses

- Quality Control
    - Can be used in manufacturing to classify products as standard (or 
    substandard) based on quality control metrics  


### Limitations
- Poor scalability
    - Training time grows roughly quadratically with the number of samples, 
    making them less efficient for very big data.
  
- Sensitivity to outliers
    - Outliers can heavily influence the position of the hyperplane since SVMs 
    try to maximize the margin (even one extreme point can shrink or shift the 
    margin significantly).
  
- Poor handling of imbalanced datasets. 
    - When one class is much larger than the other, the SVM tends to favor the 
    majority class.


### Conclusion

Support Vector Machines remain a powerful and versatile method for both 
classification and regression tasks. Their strength lies especially in finding 
optimal decision boundaries that generalize data well, particularly for 
moderate-sized and structured datasets. Through kernel functions, SVMs can model
 complex non-linear relationships without extensive feature engineering, while 
 parameter tuning balances bias and variance to prevent overfitting.  

While modern deep learning methods often outperform them on large unstructured 
data, SVMs still excel when interpretability and computational efficiency are 
key.

### Further Readings

- **ML Journey,** "Support Vector Machine Examples"  
    ( https://mljourney.com/support-vector-machine-examples/ )

- **IMB,** "What are Support Vector Machines"  
    ( https://www.ibm.com/think/topics/support-vector-machine )
- **National Library of Medicine,** "Evolution of Support Vector Machine and 
Regression Modeling in Chemoinformatics and Drug Discovery"  
    ( https://pmc.ncbi.nlm.nih.gov/articles/PMC9325859/ )
- **Medium,** "The Forgotten Soviet Origins of the Support Vector Machine: How 
a 1960s Soviet Algorithm Became a Pillar of Modern Machine Learning"  
    ( https://valeman.medium.com/the-forgotten-soviet-origins-of-the-support-vector-machine-how-a-1960s-soviet-algorithm-became-a-54d3a8b728b7 )
