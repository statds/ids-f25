# Supervised Learning

## Introduction

Machine Learning (ML) is a branch of artificial intelligence that
enables systems to learn from data and improve their performance over
time without being explicitly programmed. At its core, machine
learning algorithms aim to identify patterns in data and use those
patterns to make decisions or predictions.


Machine learning can be categorized into three main types: 
supervised learning, unsupervised learning, and reinforcement
learning. Each type differs in the data it uses and the learning tasks
it performs, addressing addresses different tasks and
problems. Supervised learning aims to predict outcomes based on
labeled data, unsupervised learning focuses on discovering hidden
patterns within the data, and reinforcement learning centers around
learning optimal actions through interaction with an environment.


Let's define some notations to introduce them:

- $X$: A set of feature vectors representing the input data. Each
  element $X_i$ corresponds to a set of features or attributes that
  describe an instance of data.
  
- $Y$: A set of labels or rewards associated with outcomes. In
  supervised learning, $Y$ is used to evaluate the correctness of the
  model’s predictions. In reinforcement learning, $Y$ represents the
  rewards that guide the learning process.
  
- $A$: A set of possible actions in a given context. In reinforcement
  learning, actions $A$ represent choices that can be made in response
  to a given situation, with the goal of maximizing a reward.

### Supervised Learning
Supervised learning is the most widely used type of machine
learning. In supervised learning, we have both feature vectors $X$ and
their corresponding labels $Y$. The objective is to train a model that
can predict $Y$ based on $X$. This model is trained on labeled
examples, where the correct outcome is known, and it adjusts its
internal parameters to minimize the error in its predictions, which
occurs as part of the cross-validation process.


Key tasks in supervised learning include:

- Classification: Assigning data points to predefined categories or classes.
- Regression: Predicting a continuous value based on input data.

In supervised learning, the data consists of both feature vectors $X$
and labels $Y$, namely, $(X, Y)$.


### Unsupervised Learning
Unsupervised learning involves learning patterns from data without any
associated labels or outcomes. The objective is to explore and
identify hidden structures in the feature vectors $X$. Since there are
no ground-truth labels $Y$ to guide the learning process, the
algorithm must discover patterns on its own. This is particularly useful
when subject matter experts are unsure of common properties within a
data set.



Common tasks in unsupervised learning include:

- Clustering: Grouping similar data points together based on certain
  features.
  
- Dimension Reduction: Simplifying the input data by reducing the
  number of features while preserving essential patterns.


In unsupervised learning, the data consists solely of feature vectors
$X$.



### Reinforcement Learning
Reinforcement learning involves learning how to make a sequence of
decisions to maximize a cumulative reward. Unlike supervised learning,
where the model learns from a static dataset of labeled examples,
reinforcement learning involves an agent that interacts with an
environment by taking actions $A$, receiving feedback in the form of
rewards $Y$, and learning over time which actions lead to the highest
cumulative reward.

The process in reinforcement learning involves:

- States: The context or environment the agent is in, represented by
  feature vectors $X$.

- Actions: The set of possible choices the agent can make in response
  to the current state, denoted as $A$.
  
- Rewards: Feedback the agent receives after taking an action, which
  guides the learning process.
  

In reinforcement learning, the data consists of feature vectors $X$,
actions $A$, and rewards $Y$, namely, $(X, A, Y)$.


## Decision Trees

Decision trees are widely used supervised learning models that predict 
the value of a target variable by iteratively splitting the dataset 
based on decision rules derived from input features. The model 
functions as a piecewise constant approximation of the target 
function, producing clear, interpretable rules that are easily 
visualized and analyzed [@breiman1984classification]. Decision trees 
are fundamental in both classification and regression tasks, serving 
as the building blocks for more advanced ensemble models such as 
Random Forests and Gradient Boosting Machines.


### Recursive Partition Algorithm

The core mechanism of a decision tree algorithm is the identification 
of optimal splits that partition the data into subsets that are 
increasingly homogeneous with respect to the target variable. At any 
node $m$, the data subset is denoted as $Q_m$ with a sample size of 
$n_m$. The objective is to find a candidate split $\theta$, defined 
as a threshold for a given feature, that minimizes an impurity or 
loss measure $H$.

When a split is made at node $m$, the data is divided into two 
subsets: $Q_{m,l}$ (left node) with sample size $n_{m,l}$, and 
$Q_{m,r}$ (right node) with sample size $n_{m,r}$. The split 
quality, measured by $G(Q_m, \theta)$, is given by:

$$
G(Q_m, \theta) = \frac{n_{m,l}}{n_m} H(Q_{m,l}(\theta)) + 
\frac{n_{m,r}}{n_m} H(Q_{m,r}(\theta)).
$$

The algorithm aims to identify the split that minimizes the impurity:

$$
\theta^* = \arg\min_{\theta} G(Q_m, \theta).
$$

This process is applied recursively at each child node until a 
stopping condition is met.

+ **Stopping Criteria:** 
  The algorithm stops when the maximum tree depth is reached or when
  the node sample size falls below a preset threshold.
+ **Pruning:**
  Reduce the complexity of the final tree by removing
  branches that add little predictive value. This reduces overfitting
  and improves generalization.

### Search Space for Possible Splits

At each node, the search space for possible splits 
comprises all features in the dataset and potential thresholds derived 
from the feature values. For a given feature, the algorithm 
considers each of its unique value in the current node as a possible 
split point. The potential thresholds are typically set as midpoints 
between consecutive unique values, ensuring effective partition.

Formally, let the feature set be $\{X_1, X_2, \ldots, X_p\}$, where 
$p$ is the total number of features, and let the unique values of 
feature $X_j$ at node $m$ be denoted by 
$\{v_{m,j,1}, v_{m,j,2}, \ldots, v_{m,j,k_{mj}}\}$. The search space
at node $m$ includes:


- Feature candidates: $\{X_1, X_2, \ldots, X_p\}$.
- Threshold candidates for $X_j$: 
  $$
  \left\{ \frac{v_{m,j,i} + v_{m,j,i+1}}{2} \mid 1 \leq i < k_{mj} \right\}.
  $$

While the complexity of this search can be 
substantial, particularly for high-dimensional data or features with 
numerous unique values, efficient algorithms use sorting and single-pass 
scanning techniques to mitigate the computational cost.


### Metrics

#### Classification
In classification, the split quality metric measures how pure the
resulting nodes are after a split. A pure node contains observations
that predominantly belong to a single class. 

+ **Gini Index**: The Gini index measures node impurity by
  the probability that two observations randomly drawn from the node
  belong to different classes. A perfect split (all instances belong
  to one class) has a Gini index of 0. At node $m$, the Gini index is
  $$
  H(Q_m) = \sum_{k=1}^{K} p_{mk} (1 - p_{mk}) 
  = 1 - \sum_{k=1}^n p_{mk}^2,
  $$
  where $p_{mk}$ is the proportion of samples of class $k$ at node $m$;
  and $K$ is the total number of classes
  The Gini index is often preferred for its speed and simplicity, and
  it’s used by default in many implementations of decision trees,
  including `sklearn`.

  The Gini index originates from the Gini coefficient, introduced by
  Corrado Gini in 1912 to quantify inequality in income
  distributions. In that context, the Gini coefficient measures how
  unevenly a quantity (such as wealth) is distributed across a
  population. Decision tree algorithms adapt this concept of
  inequality to measure the impurity of a node: instead of wealth, the
  distribution concerns class membership. A perfectly pure node, where
  all observations belong to the same class, represents complete
  equality and yields a Gini index of zero. As class proportions
  become more mixed, inequality in class membership increases, leading
  to higher impurity values. Thus, the Gini index used in decision
  trees can be viewed as a statistical measure of diversity or
  heterogeneity derived from Gini’s original work on inequality.
  
+ **Entropy (Information Gain):** Derived from information theory,
  entropy quantifies the disorder of the data at a node. Lower entropy
  means higher purity. At node $m$, it is defined as
  $$
  H(Q_m) = - \sum_{k=1}^{K} p_{mk} \log p_{mk}.
  $$
  Entropy is commonly used in decision tree algorithms like ID3 and
  C4.5. The choice between Gini and entropy often depends on specific
  use cases, but both perform similarly in practice.
  
+ **Misclassification Error:** Misclassification error focuses on
  the most frequent class in the node. It measures the proportion of
  samples that do not belong to the majority class. Although less
  sensitive than Gini and entropy, it can be useful for classification
  when simplicity is preferred. At node $m$, it is defined as
  $$
  H(Q_m) = 1 - \max_k p_{mk},
  $$
  where $\max_k p_{mk}$ is the largest proportion of samples belonging
  to any class $k$.

#### Regression Criteria

In regression, the goal is to minimize the spread or variance of
the target variable within each node.

+ **Mean Squared Error (MSE):** MSE is the average squared
  difference between observed and predicted values (mean
  of the target in the node). The smaller the MSE, the better the
  fit. At node $m$, it is 
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} (y_i - \bar{y}_m)^2,
  $$
  where
    - $y_i$ is the actual value for sample $i$;
    - $\bar{y}_m$ is the mean value of the target at node $m$;
    - $n_m$ is the number of samples at node $m$.

  MSE works well when the target is continuous and normally distributed.

+ **Half Poisson Deviance:** Used for count target, 
  the Poisson deviance measures the variance in the
  number of occurrences of an event. At node $m$, it is
  $$
  H(Q_m) = \sum_{i=1}^{n_m} \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right),
  $$
  where $\hat{y}_i$ is the predicted count. This criterion is
  especially useful when the target variable represents discrete
  counts, such as predicting the number of occurrences of an event.

+ **Mean Absolute Error (MAE):** MAE aims to minimize the absolute
  differences between actual and predicted values.
  While it is more robust to outliers than MSE, it is slower
  computationally due to the lack of a closed-form solution for
  minimization. At node $m$, it is
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} |y_i - \bar{y}_m|.
  $$
  MAE is useful when you want to minimize large deviations and can be
  more robust in cases where outliers are present in the data.


### Ames Housing Example

The Ames Housing data are used to illustrate a regression tree model 
for predicting log house price. 

As before, we retrive the data from `OpenML`.

```{python}
import openml
import pandas as pd
import numpy as np

# Load Ames Housing dataset (OpenML ID 42165)
dataset = openml.datasets.get_dataset(42165)
df, *_ = dataset.get_data()
df["LogPrice"] = np.log(df["SalePrice"])
```

A decision tree partitions the feature 
space into regions where the average log price is relatively constant.


```{python}
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
import plotnine as gg
import pandas as pd

numeric_features = [
    "OverallQual", "GrLivArea", "GarageCars",
    "TotalBsmtSF", "YearBuilt", "FullBath"
]
categorical_features = ["KitchenQual"]

preprocessor = ColumnTransformer([
    ("num", "passthrough", numeric_features),
    ("cat", OneHotEncoder(drop="first"), categorical_features)
])

X = df[numeric_features + categorical_features]
y = df["LogPrice"]

depths = range(2, 11)
cv_scores = [
    cross_val_score(
        Pipeline([
            ("pre", preprocessor),
            ("model", DecisionTreeRegressor(max_depth=d, random_state=0))
        ]),
        X, y, cv=5, scoring="r2"
    ).mean()
    for d in depths
]

list(zip(depths, cv_scores))
```

Cross-validation identifies an appropriate tree depth that balances fit
and generalization. A too-deep tree overfits, while a shallow tree 
misses structure.

```{python}
dt = Pipeline([
    ("pre", preprocessor),
    ("model", DecisionTreeRegressor(max_depth=4, random_state=0))
])

dt.fit(X, y)
y_pred = dt.predict(X)

df_pred = pd.DataFrame({"Observed": y, "Predicted": y_pred})

(gg.ggplot(df_pred, gg.aes(x="Observed", y="Predicted")) +
 gg.geom_point(alpha=0.5) +
 gg.geom_abline(slope=1, intercept=0, linetype="dashed") +
 gg.labs(title="Decision Tree Regression on Ames Housing",
         x="Observed Log Price", y="Predicted Log Price"))
```

The plot shows predicted versus observed log prices. A well-fitted 
model has points close to the diagonal. The decision tree naturally 
captures nonlinear effects and interactions, though its predictions are 
piecewise constant, producing visible step patterns.


<!-- {{< include _random_forest.qmd >}} -->


## Gradient-Boosted Models

Gradient boosting is a powerful ensemble technique in machine learning
that combines multiple weak learners into a strong predictive
model. Unlike bagging methods, which train models independently,
gradient boosting fits models sequentially, with each new model
correcting errors made by the previous ensemble
[@friedman2001greedy]. While decision trees are commonly used as weak
learners, gradient boosting can be generalized to other base
models. This iterative method optimizes a specified loss function by
repeatedly adding models designed to reduce residual errors.


### Introduction

Gradient boosting builds on the general concept of boosting, aiming to
construct a strong predictor from an ensemble of sequentially trained
weak learners. The weak learners are often shallow decision trees
(stumps), linear models, or generalized additive models
[@hastie2009elements]. Each iteration adds a new learner focusing
primarily on the data points poorly predicted by the existing
ensemble, thereby progressively enhancing predictive accuracy.


Gradient boosting's effectiveness stems from:

- Error Correction: Each iteration specifically targets previous
  errors, refining predictive accuracy.
- Weighted Learning: Iteratively focuses more heavily on
  difficult-to-predict data points.
- Flexibility: Capable of handling diverse loss functions and various
  types of predictive tasks.


The effectiveness of gradient-boosted models has made them popular
across diverse tasks, including classification, regression, and
ranking. Gradient boosting forms the foundation for algorithms such as
XGBoost [@chen2016xgboost], LightGBM [@ke2017lightgbm], and CatBoost
[@prokhorenkova2018catboost], known for their high performance and
scalability.


### Gradient Boosting Process

Gradient boosting builds an ensemble by iteratively minimizing the
residual errors from previous models. This iterative approach
optimizes a loss function, $L(y, F(x))$, where $y$ represents the
observed target variable and $F(x)$ the model's prediction for a
given feature vector $x$. 

Key concepts:

- Loss Function: Guides model optimization, such as squared error for
  regression or logistic loss for classification.
- Learning Rate: Controls incremental updates, balancing training
  speed and generalization.
- Regularization: Reduces overfitting through tree depth limitation,
  subsampling, and L1/L2 penalties.

#### Model Iteration

The gradient boosting algorithm proceeds as follows:

1. Initialization:
   Define a base model $F_0(x)$, typically the mean of the target
   variable for regression or the log-odds for classification.

2. Iterative Boosting:
   At each iteration $m$:
   - Compute pseudo-residuals representing the negative gradient of
     the loss function at the current predictions. For each
     observation $i$:
   $$
   r_i^{(m)} = -\left.\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right|_{F(x)=F_{m-1}(x)},
   $$
   where $x_i$ and $y_i$ denote the feature vector and observed value
   for the $i$-th observation, respectively. The residuals represent the
   direction of steepest descent in function space, so fitting a learner
   to them approximates a gradient descent step minimizing $L(y, F(x))$.

   - Fit a new weak learner $h_m(x)$ to these residuals.

   - Update the model:
   $$
   F_m(x) = F_{m-1}(x) + \eta \, h_m(x),
   $$
   where $\eta$ is a small positive learning rate (e.g., 0.01–0.1),
   controlling incremental improvement and reducing overfitting.
   
   In some implementations, the update step includes an additional
   multiplier determined by a one-dimensional line search that minimizes
   the loss function at each iteration. Specifically, the optimal step
   length is defined as
   $$
   \gamma_m = \arg\min_\gamma \sum_{i=1}^n 
    L\bigl(y_i,\, F_{m-1}(x_i) + \gamma\, h_m(x_i)\bigr),
   $$
   leading to an updated model of the form
   $$
    F_m(x) = F_{m-1}(x) + \eta\, \gamma_m\, h_m(x),
   $$
   where $\eta$ remains a shrinkage factor controlling the overall
   rate of learning, while $\gamma_m$ adjusts the step size adaptively
   at each iteration.

3. Final Model: After $M$ iterations, the ensemble model is:
  $$
  F_M(x) = F_0(x) + \sum_{m=1}^M \eta \, h_m(x).
  $$


Stochastic gradient boosting is a variant that enhances gradient
boosting by introducing randomness through subsampling at each
iteration, selecting a random fraction of data points (typically
50\%--80\%) to fit the model \citep{friedman2002stochastic}. This
randomness helps reduce correlation among trees, improve model
robustness, and lower the risk of overfitting.

### Boosted Trees with Ames Housing

Boosted trees apply the gradient boosting framework to decision trees.
They build an ensemble of shallow trees, each trained to correct the
residual errors of the preceding ones. By sequentially emphasizing
observations that are difficult to predict, the model progressively
improves its overall predictive accuracy.
We now apply gradient boosting using the same preprocessed features.
Boosting combines many shallow trees, each correcting the residual
errors of its predecessors, to improve predictive accuracy.

```{python}
from sklearn.ensemble import GradientBoostingRegressor

# Define a range of tree counts for tuning
n_estimators_list = [50, 100, 200, 400]

cv_scores_gb = [
    cross_val_score(
        Pipeline([
            ("pre", preprocessor),
            ("model", GradientBoostingRegressor(
                n_estimators=n,
                learning_rate=0.05,
                max_depth=3,
                random_state=0))
        ]),
        X, y, cv=5, scoring="r2"
    ).mean()
    for n in n_estimators_list
]

list(zip(n_estimators_list, cv_scores_gb))
```

Cross-validation shows how increasing the number of boosting rounds
initially improves performance but eventually risks overfitting when
too many trees are added.

```{python}
gb = Pipeline([
    ("pre", preprocessor),
    ("model", GradientBoostingRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=3,
        random_state=0))
])

gb.fit(X, y)
y_pred_gb = gb.predict(X)

df_pred_gb = pd.DataFrame({"Observed": y, "Predicted": y_pred_gb})

(gg.ggplot(df_pred_gb, gg.aes(x="Observed", y="Predicted")) +
 gg.geom_point(alpha=0.5) +
 gg.geom_abline(slope=1, intercept=0, linetype="dashed") +
 gg.labs(title="Gradient-Boosted Regression on Ames Housing",
         x="Observed Log Price", y="Predicted Log Price"))
```

The boosted model produces predictions that are generally closer to the
45-degree line than a single tree, reflecting improved accuracy and
smoother response across the feature space.


Gradient-boosted trees introduce several parameters that govern model
complexity, learning stability, and overfitting control:

+ `n_estimators`: the number of trees (boosting rounds). More trees
   can reduce bias but increase computation and risk of overfitting.
   learning_rate — the shrinkage parameter $\eta$ controlling the
   contribution of each new tree. Smaller values (e.g., 0.05 or 0.01)
   require more trees but often yield better generalization.
+ `max_depth`: the maximum depth of each individual tree, limiting
   the model’s ability to overfit local noise. Shallow trees (depth
   2–4) are typical weak learners. 
+ `subsample`: the fraction of data used in each iteration. Values
   below 1.0 introduce randomness (stochastic boosting), improving
   robustness and reducing correlation among trees.
+ `min_samples_split` and `min_samples_leaf`: minimum numbers of
  observations required for splitting or forming leaves. These control
  tree granularity and help regularize the model. 
  
In practice, moderate learning rates with a sufficiently large number
of estimators and shallow trees often perform best, balancing bias,
variance, and computational cost.

### XGBoost: Extreme Gradient Boosting

XGBoost is a scalable and 
efficient implementation of gradient-boosted decision trees 
[@chen2016xgboost]. It has become one of the most widely used machine 
learning methods for structured data due to its high predictive 
performance, regularization capabilities, and speed. XGBoost builds 
an ensemble of decision trees in a stage-wise fashion, minimizing a 
regularized objective that balances training loss and model complexity.


The core idea of XGBoost is to fit each new tree to the *gradient* of 
the loss function with respect to the model’s predictions. Unlike 
traditional boosting algorithms like AdaBoost, which use only 
first-order gradients, XGBoost optionally uses second-order derivatives 
(Hessians), enabling better convergence and stability 
[@friedman2001greedy].


XGBoost is widely used in data science competitions and real-world 
applications. It supports regularization (L1 and L2), handles missing 
values internally, and is designed for distributed computing.


XGBoost builds upon the same foundational idea as gradient boosted 
machines—sequentially adding trees to improve the predictive model—
but introduces a number of enhancements:

| Aspect               | Traditional GBM                            | XGBoost                        |
|----------------------|---------------------------------------------|--------------------------------|
| Implementation       | Basic gradient boosting                    | Optimized, regularized boosting |
| Regularization       | Shrinkage only                             | L1 and L2 regularization        |
| Loss Optimization    | First-order gradients                      | First- and second-order         |
| Missing Data         | Requires manual imputation                 | Handled automatically           |
| Tree Construction    | Depth-wise                                 | Level-wise (faster)             |
| Parallelization      | Limited                                    | Built-in                        |
| Sparsity Handling    | No                                          | Yes                             |
| Objective Functions  | Few options                                | Custom supported                |
| Cross-validation     | External via `GridSearchCV`                | Built-in `xgb.cv`               |

XGBoost is therefore more suitable for large-scale problems and provides 
better generalization performance in many practical tasks.


<!-- This section is for nueral network. -->
{{< include _nnet.qmd >}}


<!-- This section is for nueral network with time series. -->
{{< include _nntemp.qmd >}}

## Naive Bayes

Naive Bayes classifier is a supervised learning algorithm based on **Bayes' 
Theorem** for solving classification problems [@domingos1997optimal]. This 
method determines which category a sample belongs to by calculating the 
posterior probability of each category, and assumes that each feature is 
independent of each other under the conditions of the given category.

This "naive" assumption greatly simplifies the model computation and makes 
naive Bayes a classical algorithm that is both fast and stable.

In statistical learning and machine learning, naive Bayes is widely used for 
tasks such as text classification, medical diagnosis, credit risk assessment, 
and anomaly detection. Despite its relatively strict assumptions, it still 
performs well in high dimensional data scenarios due to its robustness, low 
computational cost, and good interpretability [@rish2001empirical].

### Theoretical basis

#### Bayes theorem

The theoretical basis of Naive Bayes classifier is **Bayes' Theorem**. It 
describes how to update our level of trust in an event after new observation 
information is available.

##### Basic form

The basic form of Bayes theorem is:

$$
P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}.
$$

Among them:

-   $P(A)$: **Prior Probability** of event $A$, representing the initial belief 
about event $A$ in the absence of any additional information.
-   $P(B|A)$: **Likelihood**, denotes the probability of observing $B$ given 
the occurrence of $A$.
-   $P(A|B)$: **Posterior Probability**, which is the updated probability of 
event $A$ occurring after the observation of $B$.
-   $P(B)$: **Marginal Probability** of event $B$, which represents the overall 
probability of observing $B$ under all possible scenarios.

##### Application to classification problems

In the classification task, we let:

-   $Y$ is the category variable (e.g., "Is it a serious accident", "is it 
spam");
-   $X = (x_1, x_2, \ldots, x_n)$ is the observed eigenvector.

Bayes theorem can be rewritten as follows.

$$
P(Y|X) = \frac{P(X|Y)\,P(Y)}{P(X)}.
$$

Here:

-   $P(Y)$: prior probability, reflecting the distribution of each category in 
the population sample;
-   $P(X|Y)$: likelihood function, which represents the probability of 
occurrence of feature $X$ under the category $Y$;
-   $P(X)$: the normalization term, which is used to ensure that the posterior 
probabilities of all categories sum to one;
-   $P(Y|X)$: posterior probability, which represents the likelihood that the 
sample after a given feature $X$ belongs to the class $Y$.

Because $P(X)$ is the same for all categories, it can be ignored when comparing 
which category has a greater probability. The prediction criteria of the model 
are therefore simplified as follows.

$$
\hat{Y} = \arg\max_Y P(Y|X) = \arg\max_Y P(X|Y)\,P(Y).
$$

In other words: Instead of calculating all probabilities, we can simply compare 
the "prior × likelihood" of each category which is greater.

##### Intuitive explanation

Bayes' theorem can be understood as a "belief update formula" : When we obtain 
new observational information $X$, our belief about a certain category $Y$ 
changes.

| stage | concept | meaning |
|------------------------|------------------------|------------------------|
| Prior $P(Y)$ | Our original knowledge of the category $Y$ | For example: 30% <br> of serious accidents |
| Likelihood $P(X|Y)$ | probability of feature $X$ under category $Y$ | For example: a higher proportion of <br> serious accidents in nighttime accidents |
| A posterior $P(Y|X)$ | update probability after combining observations | For example, if it is at night, <br> the probability of a serious accident rises to about 56% |


In other words, Bayes' theorem lets us **update beliefs** with data. It is this 
updating mechanism that naive Bayes uses for classification.

#### Conditional independence assumption

Naive Bayes' "naive" comes from a key assumption: Given the category $Y$, the 
individual features $x_i$ are **conditionally independent** among each other 
[@domingos1997optimal]. Namely:

$$
P(X|Y) = \prod_{i=1}^{n} P(x_i|Y)
$$

This assumption greatly simplifies the calculation of the joint distribution. 
Originally we needed to estimate $P(x_1,x_2,... x_n|Y)$ such a high-dimensional 
joint probability, But with the independence assumption, one only needs to 
estimate the conditional probability $P(x_i|Y)$ of each individual feature.

This is not always true in real data, but in most scenarios it is still 
satisfactory, This is especially the case when the correlation between features 
is weak or the sample size is large enough.

#### Parameter estimates with likelihood functions

The core of model training is the estimation of two types of parameters:

I.  **Prior probability** $P(Y)$

Represents the proportion of categories occurring in the population:

$$
    \hat{P}(Y_k) = \frac{N_k}{N},
$$

Where $N_k$ is the number of samples for category $k$ and $N$ is the total 
number of samples.

II. **Conditional probability** $P(x_i|Y)$

-   If the feature is discrete:

$$
    \hat{P}(x_i|Y) = \frac{\text{count}(x_i,Y)}{\text{count}(Y)}.
$$

-   If the feature is continuous, assume that it follows a Gaussian 
distribution under each category:

$$
    P(x_i|Y) = \frac{1}{\sqrt{2\pi\sigma_{Y,i}^2}}
    \exp\! \left(-\frac{(x_i-\mu_{Y,i})^2}{2\sigma_{Y,i}^2}\right)
$$

where $\mu_{Y,i}$ and $\sigma_{Y,i}^2$ are the mean and variance of feature 
$x_i$ among all samples in class $Y$, respectively.

These parameters are usually obtained by Maximum Likelihood Estimation (MLE).

#### Class Prediction and Log Form

The goal of model prediction is to find the class that maximizes the posterior 
probability:

$$
\hat{Y} = \arg\max_Y P(Y) \prod_{i=1}^n P(x_i|Y)
$$

However, in practice, because continuous multiplication can lead to numerical 
underflow, We usually take logarithms and turn multiplication into addition:

$$
\log P(Y|X) = \log P(Y) + \sum_{i=1}^{n} \log P(x_i|Y)
$$

The final output of the model is:

$$
\hat{Y} = \arg\max_Y \big[\log P(Y) + \sum_i \log P(x_i|Y)\big].
$$

In this way, not only accuracy problems can be avoided, but also the 
contribution of each feature to the classification result can be observed more 
intuitively.

#### Theoretical Performance and Advantages and Limitations

I.  **Advantages**

-   The algorithm is simple, the computational complexity is linear, and the 
training and prediction speed is extremely fast;
-   Good performance on high dimensional data (such as bag-of-words model);
-   Good performance on small sample data; The model results are highly 
interpretable; Certain robustness against noisy data and missing data.

II. **Limitations**

Conditional independence assumption is often not true, and performance may 
decrease when feature correlation is strong; Inability to capture feature 
interaction effects; For the values of features that have not appeared before, 
there may be a zero probability problem; Output probability estimates are not 
necessarily reliable (often biased toward 0 or 1).

#### Laplace Smoothing

In naive Bayes, if a feature value never occurs in the training set, its 
corresponding probability is estimated to be zero. As a result, the whole 
product is 0 and the classifier outputs the wrong result. To avoid this 
situation, **Laplace Smoothing** is introduced.

That is to add a constant $\alpha > 0$ (usually 1) to the probability estimate:

$$
\hat{P}(x_i|Y) =
\frac{\text{count}(x_i,Y) + \alpha}
{\sum_{x_i'}(\text{count}(x_i',Y) + \alpha)}.
$$

So even if a value doesn't appear in the training set, the probability is not 
zero, The model is thus more stable and robust.

#### Surprisingly Good Performance

Although the conditional independence assumption is often not strictly true, 
naive Bayes still performs surprisingly well in practice. This is mainly due to 
the following:

-   **Robustness** : Even if there is some correlation between features, the 
classification result is still accurate as long as the dependence between 
different categories is not very different.

-   **Error Compensation** : The bias caused by conditional independence 
assumption often cancerates each other when multiple features are combined.

-   **Implicit Regularization** : Feature independence makes the model 
complexity low and can effectively prevent overfitting.

-   **High-dimensional Advantage** : In scenarios where the number of features 
is much larger than the number of samples, such as text classification, naive 
Bayes is almost always more stable than linear models.

### Types of Naive Bayes

There is not just one form of naive Bayesian algorithm. According to the type 
of feature variable (continuous or discrete), the value characteristics of the 
feature (Boolean, count, frequency, etc.) and the distribution assumption of 
the data, naive Bayes can be divided into the following main types:

#### Gaussian Naive Bayes

When the data feature is **continuous variable**, each feature is assumed to 
follow **Gaussian distribution** under a given category:

$$
P(x_i|Y) = \frac{1}{\sqrt{2\pi\sigma_{Y,i}^2}}
\exp\! \left(-\frac{(x_i - \mu_{Y,i})^2}{2\sigma_{Y,i}^2}\right)
$$

The advantages of Gaussian naive Bayes are:

-   suitable for continuous characteristics (e.g. age, temperature, price, 
etc.).
-   Computationally efficient.
-   Parameter estimation only requires sample mean and variance.
-   Good results can still be achieved for nonlinear separable data.

#### Categorical Naive Bayes

Categorical naive Bayes is used when the feature is **discrete (categorical) 
variable**. Assume that each feature follows a **categorical distribution** 
under the category $Y$:

$$
P(x_i|Y) = \frac{\text{count}(x_i,Y) + \alpha}
{\sum_{x_i'} (\text{count}(x_i',Y) + \alpha)}
$$

Where $\alpha$ is the smoothing coefficient (usually 1) and is used to avoid 
the zero-probability problem. It is often used to analyze categorical 
characteristic data, such as region, color, occupation type, etc.

#### Bernoulli Naive Bayes

Bernoulli naive Bayes is used when the features take only two values, such as 
0/1, True/False. It assumes that each feature follows a **Bernoulli 
Distribution** under the category $Y$:

$$
P(x_i|Y) = P_i^{x_i} (1-P_i)^{(1-x_i)},
$$

Where $P_i = P(x_i=1|Y)$. Often used in "word presence or absence" models for 
text classification (e.g., spam detection). Compared with the polynomial model, 
the Bernoulli model pays more attention to "whether a certain feature is 
included" rather than "the number of occurrences of features".

#### Multinomial Naive Bayes

Polynomial naive Bayes is often used for text analysis or document 
classification tasks. It assumes that features are represented as discrete 
frequencies or counts such as word frequency TF. Conditional probabilities are 
calculated as follows:

$$
P(X|Y) = \frac{(\sum_i x_i)! }{\prod_i x_i! } \prod_i P(x_i|Y)^{x_i},
$$

Where $x_i$ is the number of occurrences of the $i$word in the document. This 
model is particularly suitable for "Bag-of-Words" models, such as news 
classification, comment sentiment analysis, etc.

#### Complement Naive Bayes

Complement NB is an improvement of the polynomial model, It is mainly used for 
**class imbalanced** datasets [@zhang2004complement]. It estimates conditional 
probabilities by considering "samples outside of one category", reducing the 
bias of the model by mainstream categories:

$$
P(x_i|\bar{Y}) = \frac{\text{count}(x_i,\bar{Y}) + \alpha}
{\sum_{x_i'}(\text{count}(x_i',\bar{Y}) + \alpha)}
$$

This method works well when there are large differences in category 
proportions, For example, it is often used in tasks such as "accident severity 
classification" and "fraud detection".

#### Model comparison and application scenario summary

| Model type | Feature type | Common applications | Distributional assumptions | Remarks |
|-------------|---------------|----------------------|-----------------------------|----------|
| **Gaussian NB** | Continuous variables | medical diagnosis, price prediction | Gaussian distribution | more sensitive to outliers |
| **Categorical NB** | Categorical variables | Questionnaire, demographic analysis | Categorical distribution | fit discrete features |
| **Bernoulli NB** | Binary variables | Spam classification, whether the text <br> contains keywords | Bernoulli distribution | Focus on "presence or absence" |
| **Multinomial NB** | Count / frequency | text classification, sentiment analysis | Multinomial distribution | Fit for high dimensional sparse data |
| **Complement NB** | Imbalanced sample | fraud detection, traffic accident prediction | Polynomial complementary distribution | Improved imbalanced problem |


### Naive Bayesian application: NYC Crash Data

In this section, we will use the accident data set provided in the course to 
demonstrate how naive Bayesian models can be used to predict whether an 
accident is a "serious accident".

The goal here is to classify an original traffic accident record into:

-   severe (severe = 1) : Someone is injured or killed
-   No severe (severe = 0) : no injuries or deaths

This is a typical real world task: we want to quickly determine the severity of 
an accident as soon as it occurs or is recorded, so that we can direct 
resources, do risk monitoring, and observe patterns.

#### Modeling objectives

The task we are going to complete is a **binary classification problem** :

-   **Response variable (Y)** : 'severe_flag' Definition rule: If 
'persons_injured \> 0' or 'persons_killed \> 0', it is considered a serious 
accident and marked as 1, otherwise it is 0.

-   **Independent variable (X)** : We will use some features that can be 
extracted/discretized from accident records such as 
`borough`,`zip_code`,`crash_datetime`, 
`vehicle_type_code_1`,`contributing_factor_vehicle_1`... Most of these features 
are categorical factors, which are suitable for conditional probability based 
modeling in naive Bayes.

-   Load packages

```{r, message=FALSE, warning=FALSE}
library(arrow)
library(dplyr)
library(lubridate)
library(e1071) # naiveBayes
library(caret)
library(forcats)
```

-   Read our data

```{r}
# 1.
df <- arrow::read_feather("data/nyc_crashes_cleaned.feather")
```

-   Create the target variable severe_flag (severe accident =1, otherwise =0)

```{r}
df <- df %>%
  mutate(
    number_of_persons_injured = ifelse(is.na(number_of_persons_injured), 0, 
number_of_persons_injured),
    number_of_persons_killed = ifelse(is.na(number_of_persons_killed), 0, 
number_of_persons_killed),
    severe_flag = ifelse(number_of_persons_injured > 0 | 
number_of_persons_killed > 0, 1, 0)
  )
```

-   Extract hours from time and divide periods

```{r}
df <- df %>%
  mutate(
    crash_datetime = ymd_hms(crash_datetime, quiet = TRUE),
    hour = hour(crash_datetime),
    time_category = case_when(
      hour >= 0 & hour < 6 ~ "Late Night",
      hour >= 6 & hour < 12 ~ "Morning",
      hour >= 12 & hour < 18 ~ "Afternoon",
      hour >= 18 & hour < 24 ~ "Evening",
      TRUE ~ "Unknown"
    )
  )
```

-   Select features

```{r}
# borough: What borough is it in
# zip_code/zip_filled: Location code information
# time_category: Approximate time period when the accident occurred
# contributing_factor_vehicle_1: The primary contributing_factor for vehicle 1
# vehicle_type_code_1: Type of vehicle involved

# Note: We do not directly use the latitude and longitude, street names, these 
# are high base text/continuous values, not suitable for direct feeding naive 
# Bayes for classification.

model_df <- df %>%
  select(
    borough,
    zip_filled,
    time_category,
    contributing_factor_vehicle_1,
    vehicle_type_code_1,
    severe_flag
  ) %>%
  # Handling missing rows
  tidyr::drop_na() %>%
  mutate(
    borough = as.factor(borough),
    zip_filled = as.factor(zip_filled),
    time_category = as.factor(time_category),
    contributing_factor_vehicle_1 = as.factor(contributing_factor_vehicle_1),
    vehicle_type_code_1 = as.factor(vehicle_type_code_1),
    severe_flag = as.factor(severe_flag)
  )
```

-   Split data into training/testing sets and Train the Naive Bayes model.

```{r}
set.seed(42)
idx <- createDataPartition(model_df$severe_flag, p = 0.7, list = FALSE)
train_data <- model_df[idx, ]
test_data <- model_df[-idx, ]

nb_model <- naiveBayes(
  severe_flag ~ .,
  data = train_data,
  laplace = 1
)
```

-   Make predictions and evaluate the model.

```{r}
pred_class <- predict(nb_model, newdata = test_data)
pred_prob <- predict(nb_model, newdata = test_data, type = "raw")

confusionMatrix(
  data = pred_class,
  reference = test_data$severe_flag,
  positive = "1" # Treat "1" as a serious accident
)
```

##### Interpretation of our model results

As can be seen from the above table, the overall accuracy of the Naive Bayes 
model is about **60.6%**, which is higher than that of random guess, indicating 
that the model has a certain predictive ability in the overall classification.

However, the **Kappa value was 0.19**, indicating that the agreement between 
the model prediction results and the true label was weak.

In terms of classification ability, the model identified "non-serious 
accidents" better (specificity 0.77). However, the ability to identify "serious 
accidents" was relatively weak (sensitivity 0.43), that is, there was a certain 
proportion of missed detection.

The main reasons for this may include:

-   **Imbalanced Data** : The samples of non-serious accidents are far more 
than those of serious accidents, resulting in the model being biased towards 
the majority class.

-   **Naive Assumption** : Naive Bayes assumes that each feature is independent 
of each other, but this assumption is often not true in real traffic accident 
data.

-   **Limited feature information** : The variables used (such as time period, 
vehicle type, causative factors, etc.) are relatively macroscopic and lack 
continuity or quantitative characteristics, which limits the accuracy of the 
model.

Nevertheless, the naive Bayesian model has the following advantages:

-   fast training and prediction.
-   strong interpretation of parameters
-   Suitable as a baseline model (baseline) for more complex models (e.g. 
Random Forest, XGBoost).

Therefore, the results of this model can be used as a reference basis for 
subsequent improvements, and further improvements in model performance are 
expected after the introduction of more features or balanced samples.