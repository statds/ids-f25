## Hypothesis Testing with 'scipy.stats' 

This section was prepared by Jessica Yoon, an undergraduate senior pursuing a
degree in Biological Data Science 

### What is Hypothesis Testing?

- **Hypothesis testing** is a statistical method used to make decisions or
draw conclusions about a population based on sample data.  
- It helps determine whether results we observe are due to **chance** or 
reflect a **real effect**.  
- Every test starts with two statements:  
  - **Null hypothesis (H₀):** No effect, no difference, or no relationship.  
  - **Alternative hypothesis (H₁):** There *is* an effect, difference, or relationship.  

---

### The Logic Behind It

1. Collect sample data from a population.  
2. Compute a **test statistic** to measure how far our data deviate from H₀.  
3. Use a **probability distribution** (t, F, or chi-square) to find a **p-value**.  
4. The **p-value** tells us how likely our observed data would occur if H₀ were true.  
5. Decision rule:  
   - If *p* < α (usually 0.05): reject H₀ → evidence of an effect.  
   - If *p* ≥ α: fail to reject H₀ → no strong evidence against H₀.

---

### Why Hypothesis Testing is Important

- Provides a **structured, objective framework** for decision-making under uncertainty.  
- Separates **signal from noise** — prevents drawing conclusions based on intuition alone.  
- Commonly used to:  
  - Compare **treatments or interventions** (e.g., drug vs placebo)  
  - Evaluate **policy changes** (e.g., new traffic laws on crash rates)  
  - Detect **patterns or associations** (e.g., weather vs accident frequency)  

---

### Example Context: Housing Data 

- The housing dataset allows us to explore real-world market patterns and relationships 
between property features and price.   
  - Are house prices different across neighborhoods?  
  - Do certain features, like having a driveway, air conditioning, or a larger area, 
lead to higher prices?
  - Is there a correlation between the number of bedrooms and the overall cost?  
- Hypothesis testing provides **quantitative evidence** for these questions — 
helping us determine which factors truly influence housing prices rather than relying 
on assumptions.   

---

### Load, Clean and Preview the Data 

- Let's begin by loading the data 

```{python}
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

housing = pd.read_csv("data/housing.csv")
```

- Then, we want to clean the data 

```{python}
# Step 1: Clean up column names
housing.columns = (
    housing.columns
    .str.strip()                          # remove leading/trailing spaces
    .str.lower()                          # convert to lowercase
    .str.replace(' ', '_')                # replace spaces with underscores
    .str.replace('[^a-z0-9_]', '', regex=True)  # remove special chars
)

# Step 2: Drop columns that are completely empty (all NaN)
housing = housing.dropna(axis=1, how='all')

# Step 3: Drop rows with any missing values
housing = housing.dropna(axis=0, how='any')

# Step 4: Verify the cleaned dataset
housing.info()
housing.head()
```

---

### Basic Summary Statistics 
```{python}
housing.describe()
```

---

### One-Sample T-test 

- A one-sample t-test is a statistical test used to determine whether the mean of 
a single sample is significantly different from a known or hypothesized population mean.
- In the context of housing data, we can test whether the average house price differs 
from a target or benchmark value. For example, $1 million. 
- H₀: μ = 1,000,000 (average house price = 1)
- H₁: μ ≠ 1,000,000 (average house price ≠ 1) 

```{python}
prices = housing["medv"].dropna()

t_stat, p_value = stats.ttest_1samp(prices, popmean=20)
t_stat, p_value
```

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.3f}) → Mean home value differs significantly from $20,000.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.3f}) → No significant difference from $20,000.")
```

- We can visualize this by using a histogram

```{python}
plt.hist(prices, bins=30, color="skyblue", edgecolor="black")
plt.axvline(20, color="red", linestyle="--", label="μ = 20 ($20,000)")
plt.title("Distribution of Median Home Values")
plt.xlabel("Median Value ($1000s)")
plt.ylabel("Frequency")
plt.legend()
plt.show()
```

- Most homes cluster below the hypothesized mean value, shown by the red dashed line at 
μ = 20 (\$20,000).

---

### One Sample T-Test Nonparametric Alternative (Wilcoxon Signed-Rank Test) 

- It’s used when the normality assumption doesn’t hold — meaning your data are skewed, 
have outliers, or are not normally distributed.
- The Wilcoxon Signed-Rank Test compares the median of the sample to a hypothesized value. 
- H₀: median home value = $20,000 
- H₁: median home value ≠ $20,000

```{python}
from scipy.stats import wilcoxon

prices = housing["medv"].dropna()

wilcoxon(prices - 20, alternative="two-sided")
```

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → Median home value differs significantly from $20,000.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant difference from $20,000.")
```

### Two Sample T-Test 

- A two-sample t-test (also called an independent samples t-test) is used to compare 
the means of two independent groups to see if they’re statistically different from 
each other.
- Here, we’ll test whether the average home value differs between houses located 
along the Charles River and those not near the river.
- H₀: μ₁ = μ₂ (the mean home values are the same for houses near and away from the 
Charles River.)
- H₁: μ₁ ≠ μ₂ (the mean home values differ depending on proximity to the Charles River.)

```{python}
from scipy import stats

by_river = housing[housing["chas"] == 1]["medv"].dropna()
not_by_river = housing[housing["chas"] == 0]["medv"].dropna()

t_stat, p_value = stats.ttest_ind(by_river, not_by_river, equal_var=False)
print(f"T-statistic: {t_stat:.3f}, P-value: {p_value:.4f}")
```

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → Mean home values differ significantly based on proximity to the Charles River.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant difference in mean home values based on proximity to the Charles River.")
```

- We can visualize these differences using boxplots 

```{python}
sns.boxplot(x="chas", y="medv", data=housing, palette="pastel")
plt.title("Median Home Values – Near vs. Away from Charles River")
plt.xlabel("Next to Charles River (1 = Yes, 0 = No)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```

### Two Sample T-Test Nonparametric Alternative: Man-Whitney U 

- Used when the assumption of normality or equal variances between the two groups is violated.
- Tests whether the distributions of median home values differ for houses near versus away from the Charles River
- H₀: μ₁ = μ₂ (the distributions of home values are the same for both groups.) 
- H₁: μ₁ ≠ μ₂ (the distributions of home values differ between the two groups.) 

```{python}
from scipy import stats

by_river = housing[housing["chas"] == 1]["medv"].dropna()
not_by_river = housing[housing["chas"] == 0]["medv"].dropna()

stat, p_value = stats.mannwhitneyu(by_river, not_by_river, alternative="two-sided")

print(f"Mann-Whitney U statistic: {stat:.3f}, p-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Median home values differ significantly based on proximity to the Charles River.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in median home values based on proximity to the Charles River.")
```

--- 

### ANOVA 

- ANOVA stands for Analysis of Variance
- It’s a statistical method used to compare the means of three or more groups to 
determine whether at least one of them is significantly different from the others.
- Here, we’ll compare the average home values (medv) across different levels of
highway accessibility (rad).
- H₀: All group means are equal.
- H₁: At least one group mean differs.

```{python}
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

groups = [group["medv"].dropna() for name, group in housing.groupby("rad")]

f_stat, p_value = stats.f_oneway(*groups)

# Print results
print(f"F-statistic: {f_stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Mean home values differ significantly across highway accessibility levels.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in mean home values across highway accessibility levels.")

```

- We can visualize this by using boxplots 

```{python}
sns.boxplot(x="rad", y="medv", data=housing, palette="pastel")
plt.title("Home Values Across Highway Accessibility Levels (RAD)")
plt.xlabel("RAD (Accessibility to Highways)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```

--- 

### ANOVA Nonparametric Alternative: Kruskal Wallis Test 

- The Kruskal–Wallis test is a nonparametric alternative to one-way ANOVA.
- It’s used when the normality assumption is violated or the data contain 
outliers.
- Here, we compare median home values (medv) across different levels of highway 
accessibility (rad).
- H₀: μ₁ = μ₂ = μ₃ = … (the distributions of home values are the same across all 
highway accessibility levels)
- H₁: At least one μᵢ differs (the distributions of home values differ across 
highway accessibility levels) 

```{python}
groups = [group["medv"].dropna() for name, group in housing.groupby("rad")]

h_stat, p_value = stats.kruskal(*groups)

print(f"H-statistic: {h_stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Median home values differ significantly across highway accessibility levels.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in median home values across highway accessibility levels.")
```


### Chi-Square Test of Independence 

- The Chi-Square Test of Independence checks whether two categorical variables 
are related or independent of each other.
- It’s a nonparametric test, meaning it doesn’t assume normality or equal variances.
- Here, we test whether proximity to the Charles River (chas) is related to 
highway accessibility (rad).
- H₀: chas and rad are independent.
- H₁: There is an association between chas and rad.


```{python}
table = pd.crosstab(housing["chas"], housing["rad"])

chi2, p_value, dof, expected = stats.chi2_contingency(table)

print(f"Chi2 Statistic: {chi2:.3f}, P-value: {p_value:.4f}, Degrees of Freedom: {dof}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant association "
          f"between proximity to the Charles River and highway accessibility.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant association "
          f"between proximity to the Charles River and highway accessibility.")
```

- Visual

```{python}
sns.countplot(x="rad", hue="chas", data=housing, palette="pastel")

plt.title("Highway Accessibility by Proximity to the Charles River")
plt.xlabel("RAD (Index of Highway Accessibility)")
plt.ylabel("Count of Homes")
plt.legend(title="Next to Charles River (1 = Yes, 0 = No)")
plt.show()
```

---

### Proportion Test  

- A proportion test is used to compare observed proportions to a 
hypothesized proportion or between two groups.
- Useful for categorical yes/no variables, like homes located next 
to the Charles River (`chas`).
- H₀: p = p₀ (the true proportion equals the hypothesized value)
- H₁: p ≠ p₀ (the true proportion differs from the hypothesized value)

```{python}
from statsmodels.stats.proportion import proportions_ztest

count = (housing["chas"] == 1).sum()
nobs = len(housing)
p0 = 0.10

stat, p_value = proportions_ztest(count, nobs, value=p0, alternative="two-sided")
print(f"Z-statistic: {stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print("Reject H₀ → The proportion of homes near the river differs from 10%.")
else:
    print("Fail to reject H₀ → No significant difference from 10%.")
```

### Correlation Test 

- The Correlation Test measures the strength and direction of the relationship 
between two continuous variables.
- Here, we test whether there is a linear relationship between the number of rooms 
per dwelling (rm) and the median home value (medv).
- H₀: ρ = 0 (no correlation between rooms and home value._ 
- H₁: ρ ≠ 0 (a significant correlation exists between rooms and home value.) 

```{python}
x = housing["rm"]      # average number of rooms per dwelling
y = housing["medv"]    # median home value ($1000s)

corr, p_value = stats.pearsonr(x, y)

print(f"Correlation coefficient (r): {corr:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant linear correlation between rooms and home value.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant correlation between rooms and home value.")
```

- Scatterplot for visualization 

```{python}
sns.scatterplot(x=housing["rm"], y=housing["medv"], alpha=0.4)
plt.title("Rooms vs. Median Home Value")
plt.xlabel("Average Number of Rooms per Dwelling (RM)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```

---

### Correlation Test Nonparametric Alternative: Spearman's ρ

- The Spearman correlation coefficient (ρ) measures the strength and direction 
of a monotonic relationship between two continuous variables.
- Unlike Pearson’s correlation, it doesn’t assume normality or a linear relationship.
- Here, we test whether the number of rooms (rm) and median home value (medv) are 
monotonically related.
- H₀: ρ = 0 (There is no monotonic relationship between the number of rooms and home value.)
- H₁: ρ ≠ 0 (There is a monotonic relationship between the number of rooms and home value.)

```{python}
x = housing["rm"].fillna(0)
y = housing["medv"].fillna(0)

rho, p_value = stats.spearmanr(x, y)

print(f"Spearman’s rho: {rho:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant monotonic relationship between rooms and home value.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant monotonic relationship between rooms and home value.")
```

---

### Summary of Tests 

```{python}
data = {
    "Test": [
        "One-sample t-test",
        "Wilcoxon Signed-Rank Test",
        "Two-sample t-test",
        "Mann–Whitney U Test",
        "ANOVA",
        "Kruskal–Wallis Test",
        "Chi-Square Test",
        "Proportion Test (z-test)",
        "Pearson Correlation",
        "Spearman Correlation"
    ],
    "Purpose": [
        "Compare sample mean to a known value",
        "Nonparametric alternative to one-sample t-test",
        "Compare means between two independent groups",
        "Nonparametric alternative to two-sample t-test",
        "Compare means across 3+ groups",
        "Nonparametric alternative to ANOVA",
        "Test association between categorical variables",
        "Compare observed vs expected proportions",
        "Measure linear relationship between two continuous variables",
        "Measure monotonic relationship between two continuous variables"
    ],
    "Example": [
        "Avg home value vs $20k",
        "Median home value vs $20k",
        "Homes near vs away from river",
        "Homes near vs away from river",
        "Home values across highway access levels",
        "Home values across highway access levels",
        "River proximity vs highway access",
        "Proportion of riverfront homes = 10%",
        "Rooms vs home value",
        "Rooms vs home value"
    ],
    "Interpretation": [
        "Is mean ≠ hypothesized value?",
        "Is median ≠ hypothesized value?",
        "Difference in group means",
        "Difference in group medians",
        "At least one mean differs",
        "At least one median differs",
        "Are the variables independent?",
        "Is the proportion different from expected?",
        "Is there a linear correlation?",
        "Is there a monotonic correlation?"
    ]
}

df = pd.DataFrame(data)
df
```

--- 

### Discussions and Takeaways 

- Check assumptions: normality, independence, equal variance.
- Use nonparametric methods when assumptions fail.
- Statistical significance ≠ practical importance.
- Clear hypotheses + clean data = valid conclusions.

### Further Readings 
- SciPy.stats Documentation: https://docs.scipy.org/doc/scipy/reference/stats.html
- “Practical Statistics for Data Scientists” — Bruce & Gedeck
- “Think Stats” by Allen B. Downey 
