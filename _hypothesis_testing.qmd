## Hypothesis Testing with 'scipy.stats' 

This section was prepared by Jessica Yoon, an undergraduate senior pursuing a
degree in Biological Data Science 

### What is Hypothesis Testing?

- **Hypothesis testing** is a statistical method used to make decisions or
draw conclusions about a population based on sample data.  
- It helps determine whether results we observe are due to **chance** or 
reflect a **real effect**.  
- Every test starts with two statements:  
  - **Null hypothesis (H₀):** No effect, no difference, or no relationship.  
  - **Alternative hypothesis (H₁):** There *is* an effect, difference, or relationship.  

---

### Why Hypothesis Testing is Important

- Provides a **structured, objective framework** for decision-making under uncertainty.  
- Separates **signal from noise** — prevents drawing conclusions based on intuition alone.  
- Commonly used to:  
  - Compare **treatments or interventions** (e.g., drug vs placebo)  
  - Evaluate **policy changes** (e.g., new traffic laws on crash rates)  
  - Detect **patterns or associations** (e.g., weather vs accident frequency)  

---

### The Logic Behind It

1. Collect sample data from a population.  
2. Compute a **test statistic** to measure how far our data deviate from H₀.  
3. Use a **probability distribution** (t, F, or chi-square) to find a **p-value**.  
4. The **p-value** tells us how likely our observed data would occur if H₀ were true.  
5. Decision rule:  
   - If *p* < α (usually 0.05): reject H₀ → evidence of an effect.  
   - If *p* ≥ α: fail to reject H₀ → no strong evidence against H₀.

---

### Example Context: Housing Data 

- The housing dataset allows us to explore real-world market patterns and relationships 
between property features and price.   
  - Are house prices different across neighborhoods?  
  - Do certain features, like having a driveway, air conditioning, or a larger area, 
lead to higher prices?
  - Is there a correlation between the number of bedrooms and the overall cost?  
- Hypothesis testing provides **quantitative evidence** for these questions — 
helping us determine which factors truly influence housing prices rather than relying 
on assumptions.   

---

### Load, Clean and Preview the Data 

- Let's begin by loading the data 

```{python}
# Step 1: import essential packages for analysis and visualization 
import pandas as pd # used for data manipulation and structuring data in a DataFrame 
import numpy as np # supports numerical operations
from scipy import stats # provides statistical operations 
import matplotlib.pyplot as plt # visualizations
import seaborn as sns # visualizations 

# Step 2: Load dataset from csv 
housing = pd.read_csv("data/housing.csv")
```

- Then, we want to clean the data 
- Missing or null data (NaNs) can cause statistical test to fail or 
return NaN results, especially with scipy.stats
- Cleaning ensures the tests use valid and complete data 

```{python}
# Step 1: Clean up column names
housing.columns = (
    housing.columns
    .str.strip()                          # remove leading/trailing spaces
    .str.lower()                          # convert to lowercase
    .str.replace(' ', '_')                # replace spaces with underscores
    .str.replace('[^a-z0-9_]', '', regex=True)  # remove special chars
)

# Step 2: Drop columns that are completely empty (all NaN)
housing = housing.dropna(axis=1, how='all')

# Step 3: Drop rows with any missing values
housing = housing.dropna(axis=0, how='any')

# Step 4: Verify the cleaned dataset
housing.info()
housing.head()
```

- We can summarize the dataset using '.describe()' to see its spread and variability before running tests

```{python}
housing.describe()
```

---

### One-Sample T-test 

- A one-sample t-test is a statistical test used to determine whether the mean of 
a single sample is significantly different from a known or hypothesized population mean.
- In the context of housing data, we can test whether the average house price differs 
from a target or benchmark value. For example, $1 million. 
- H₀: μ = 1,000,000 (average house price = 1)
- H₁: μ ≠ 1,000,000 (average house price ≠ 1) 

```{python}
prices = housing["medv"].dropna()

t_stat, p_value = stats.ttest_1samp(prices, popmean=20)  
t_stat, p_value
```

- The t_stat tells us how far the sample mean is from the hypothesized value
- The p_value tells us how unusual that different would be if H₀ were true
- the ttest_1samp() compares sample's mean to a known population mean 

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.3f}) → Mean home value differs significantly from $20,000.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.3f}) → No significant difference from $20,000.")
```

- The output (6.19, 1.21e-09) corresponds to the t-statistic and p-value 
- The t-statistic means that the sample is roughly 6 standard errors away from the 
hypothesized population mean of $20,000 
- The p-value is extremely small, far below the 0.05 significance threshold
- We can conclude that the mean home value is significantly different from $20,000 and 
we reject the null hypothesis. The average housing prices are not centered around $20,000 

- We can also visualize this by using a histogram

```{python}
plt.hist(prices, bins=30, color="skyblue", edgecolor="black")
plt.axvline(20, color="red", linestyle="--", label="μ = 20 ($20,000)")
plt.title("Distribution of Median Home Values")
plt.xlabel("Median Value ($1000s)")
plt.ylabel("Frequency")
plt.legend()
plt.show()
```

- Notice how most homes cluster below the hypothesized mean value, shown by the red dashed 
line at μ = 20 (\$20,000).
- Most home fall to the left of this line, showing that the actual mean is significantly 
higher than $20,000, which is consistent with the t-test result 

---

- If the data were not normally distributed, meaning the data was skewed or had outliers, 
we'd use the Wilcoxon Signed-Rank Test instead.
- The Wilcoxon Signed-Rank Test compares the median of the sample to a hypothesized value. 
- H₀: median home value = $20,000 
- H₁: median home value ≠ $20,000

```{python}
from scipy.stats import wilcoxon

prices = housing["medv"].dropna()

wilcoxon(prices - 20, alternative="two-sided")
```

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → Median home value differs significantly from $20,000.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant difference from $20,000.")
```

- wilcoxon() tests whether the median of the sample differs from a hypothesized value 
- alternative="twosided" checks for any difference 
- Wilcoxon test statistic = 48,214.5 and p <0.001
- We would reject the null hypothesis and conclude the median home value is significantly higher 
than $20,000, confiming the t-test results without assuming normality

### Two Sample T-Test 

- A two-sample t-test (also called an independent samples t-test) is used to compare 
the means of two independent groups to see if they’re statistically different from 
each other.
- Here, we’ll test whether the average home value differs between houses located 
along the Charles River and those not near the river.
- H₀: μ₁ = μ₂ (the mean home values are the same for houses near and away from the 
Charles River.)
- H₁: μ₁ ≠ μ₂ (the mean home values differ depending on proximity to the Charles River.)

```{python}
from scipy import stats

by_river = housing[housing["chas"] == 1]["medv"].dropna()
not_by_river = housing[housing["chas"] == 0]["medv"].dropna()

t_stat, p_value = stats.ttest_ind(by_river, not_by_river, equal_var=False)
print(f"T-statistic: {t_stat:.3f}, P-value: {p_value:.4f}")
```

```{python}
alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → Mean home values differ significantly based on proximity to the Charles River.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant difference in mean home values based on proximity to the Charles River.")
```

- chas is a binary variable (1 = near river and 0 = away)
- ttest_ind() compares means of two independent groups 
- equal_var=False: Welch's t-test version, which doesn't assume equal variances
- t = 3.11, p = 0.0036 
- We would reject the null hypothesis so we can conclude that homes near the Charles River 
have significantly different mean values than those farther away 

---

- We can visualize these differences using boxplots 
- The boxes represent interquartile ranges, so if they barely overlap that's a visual 
clue of statisticla difference 

```{python}
sns.boxplot(x="chas", y="medv", data=housing, palette="pastel")
plt.title("Median Home Values – Near vs. Away from Charles River")
plt.xlabel("Next to Charles River (1 = Yes, 0 = No)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```

- This boxplot compares the median home values for houses next to the Charles River 
(chas =1 ) versus those not near the river (chas = 0) 
- The orange box is visibly higher than the blue box, showing that homes near the 
Charles river tend to have high median values 

---

- If the data don't meet the t-test assumptions (normality, equal variance), 
we use the Mann-Whitney U test between two groups 
- In this case, we will test whether the distributions of median home values 
differ for houses near versus away from the Charles River
- H₀: μ₁ = μ₂ (the distributions of home values are the same for both groups.) 
- H₁: μ₁ ≠ μ₂ (the distributions of home values differ between the two groups.) 

```{python}
from scipy import stats

by_river = housing[housing["chas"] == 1]["medv"].dropna()
not_by_river = housing[housing["chas"] == 0]["medv"].dropna()

stat, p_value = stats.mannwhitneyu(by_river, not_by_river, alternative="two-sided")

print(f"Mann-Whitney U statistic: {stat:.3f}, p-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Median home values differ significantly based on proximity to the Charles River.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in median home values based on proximity to the Charles River.")
```

--- 

### ANOVA 

- ANOVA stands for Analysis of Variance
- It’s a statistical method used to compare the means of three or more groups to 
determine whether at least one of them is significantly different from the others.
- Here, we’ll compare the average home values (medv) across different levels of
highway accessibility (rad).
- H₀: All group means are equal.
- H₁: At least one group mean differs.

```{python}
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

groups = [group["medv"].dropna() for name, group in housing.groupby("rad")]

f_stat, p_value = stats.f_oneway(*groups)

print(f"F-statistic: {f_stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Mean home values differ significantly across highway accessibility levels.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in mean home values across highway accessibility levels.")

```

- groupby("rad") splits data into groups based on accessibility index 
- f_oneway() performs the one-way ANOVA test 

---

- To see this visually: 

```{python}
sns.boxplot(x="rad", y="medv", data=housing, palette="pastel")
plt.title("Home Values Across Highway Accessibility Levels (RAD)")
plt.xlabel("RAD (Accessibility to Highways)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```


- The Kruskal–Wallis test is a nonparametric alternative to one-way ANOVA.
- It’s used when the normality assumption is violated or the data contain 
outliers.
- Here, we compare median home values (medv) across different levels of highway 
accessibility (rad).
- H₀: μ₁ = μ₂ = μ₃ = … (the distributions of home values are the same across all 
highway accessibility levels)
- H₁: At least one μᵢ differs (the distributions of home values differ across 
highway accessibility levels) 

```{python}
groups = [group["medv"].dropna() for name, group in housing.groupby("rad")]

h_stat, p_value = stats.kruskal(*groups)

print(f"H-statistic: {h_stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → "
          f"Median home values differ significantly across highway accessibility levels.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → "
          f"No significant difference in median home values across highway accessibility levels.")
```

---

### Chi-Square Test of Independence 

- The Chi-Square Test of Independence checks whether two categorical variables 
are related or independent of each other.
- It’s a nonparametric test, meaning it doesn’t assume normality or equal variances.
- Here, we test whether proximity to the Charles River (chas) is related to 
highway accessibility (rad).
- H₀: chas and rad are independent.
- H₁: There is an association between chas and rad.

```{python}
table = pd.crosstab(housing["chas"], housing["rad"])

chi2, p_value, dof, expected = stats.chi2_contingency(table)

print(f"Chi2 Statistic: {chi2:.3f}, P-value: {p_value:.4f}, Degrees of Freedom: {dof}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant association "
          f"between proximity to the Charles River and highway accessibility.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant association "
          f"between proximity to the Charles River and highway accessibility.")
```

- pd.crosstab() creates a contingency table 
- chi2_contingency() tests independence of two categorical variables 

---

- Visual

```{python}
sns.countplot(x="rad", hue="chas", data=housing, palette="pastel")

plt.title("Highway Accessibility by Proximity to the Charles River")
plt.xlabel("RAD (Index of Highway Accessibility)")
plt.ylabel("Count of Homes")
plt.legend(title="Next to Charles River (1 = Yes, 0 = No)")
plt.show()
```

- Chi-square tests are great for frequency data, think of them as testing whether 
"categories are distributed as expected"

---

### Proportion Test  

- A proportion test is used to compare observed proportions to a 
hypothesized proportion or between two groups.
- Useful for categorical yes/no variables, like homes located next 
to the Charles River (`chas`).
- H₀: p = p₀ (the true proportion equals the hypothesized value)
- H₁: p ≠ p₀ (the true proportion differs from the hypothesized value)

```{python}
from statsmodels.stats.proportion import proportions_ztest

count = (housing["chas"] == 1).sum()
nobs = len(housing)
p0 = 0.10

stat, p_value = proportions_ztest(count, nobs, value=p0, alternative="two-sided")
print(f"Z-statistic: {stat:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print("Reject H₀ → The proportion of homes near the river differs from 10%.")
else:
    print("Fail to reject H₀ → No significant difference from 10%.")
```

- count is the number of homes near the river 
- nobs is the total homes 
- p0 is the hypothesized proportionn 
- proportions_ztest() is the z-test for proportions (like t-test but for percentages) 

---

### Correlation Test 

- The Correlation Test measures the strength and direction of the relationship 
between two continuous variables.
- r close to +1 means a strong correlation 
- r close to 0 means little or no correlation 
- r close to -1 means a strong negative correlation 
- Here, we test whether there is a linear relationship between the number of rooms 
per dwelling (rm) and the median home value (medv).
- H₀: ρ = 0 (no correlation between rooms and home value._ 
- H₁: ρ ≠ 0 (a significant correlation exists between rooms and home value.) 

```{python}
x = housing["rm"]      # average number of rooms per dwelling
y = housing["medv"]    # median home value ($1000s)

corr, p_value = stats.pearsonr(x, y)

print(f"Correlation coefficient (r): {corr:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant linear correlation between rooms and home value.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant correlation between rooms and home value.")
```

- Scatterplot for visualization 

```{python}
sns.scatterplot(x=housing["rm"], y=housing["medv"], alpha=0.4)
plt.title("Rooms vs. Median Home Value")
plt.xlabel("Average Number of Rooms per Dwelling (RM)")
plt.ylabel("Median Home Value ($1000s)")
plt.show()
```

---

- The Spearman correlation coefficient (ρ) measures the strength and direction 
of a monotonic relationship between two continuous variables.
- Unlike Pearson’s correlation, it doesn’t assume normality or a linear relationship.
- Here, we test whether the number of rooms (rm) and median home value (medv) are 
monotonically related.
- H₀: ρ = 0 (There is no monotonic relationship between the number of rooms and home value.)
- H₁: ρ ≠ 0 (There is a monotonic relationship between the number of rooms and home value.)

```{python}
x = housing["rm"].fillna(0)
y = housing["medv"].fillna(0)

rho, p_value = stats.spearmanr(x, y)

print(f"Spearman’s rho: {rho:.3f}, P-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    print(f"Reject H₀ (p = {p_value:.4f}) → There is a significant monotonic relationship between rooms and home value.")
else:
    print(f"Fail to reject H₀ (p = {p_value:.4f}) → No significant monotonic relationship between rooms and home value.")
```

---

### Summary of Tests 

- Each statisical test corresponds to a specific research question type: 

```{python}
data = {
    "Test": [
        "One-sample t-test",
        "Wilcoxon Signed-Rank Test",
        "Two-sample t-test",
        "Mann–Whitney U Test",
        "ANOVA",
        "Kruskal–Wallis Test",
        "Chi-Square Test",
        "Proportion Test (z-test)",
        "Pearson Correlation",
        "Spearman Correlation"
    ],
    "Purpose": [
        "Compare sample mean to a known value",
        "Nonparametric alternative to one-sample t-test",
        "Compare means between two independent groups",
        "Nonparametric alternative to two-sample t-test",
        "Compare means across 3+ groups",
        "Nonparametric alternative to ANOVA",
        "Test association between categorical variables",
        "Compare observed vs expected proportions",
        "Measure linear relationship between two continuous variables",
        "Measure monotonic relationship between two continuous variables"
    ],
    "Example": [
        "Avg home value vs $20k",
        "Median home value vs $20k",
        "Homes near vs away from river",
        "Homes near vs away from river",
        "Home values across highway access levels",
        "Home values across highway access levels",
        "River proximity vs highway access",
        "Proportion of riverfront homes = 10%",
        "Rooms vs home value",
        "Rooms vs home value"
    ],
    "Interpretation": [
        "Is mean ≠ hypothesized value?",
        "Is median ≠ hypothesized value?",
        "Difference in group means",
        "Difference in group medians",
        "At least one mean differs",
        "At least one median differs",
        "Are the variables independent?",
        "Is the proportion different from expected?",
        "Is there a linear correlation?",
        "Is there a monotonic correlation?"
    ]
}

df = pd.DataFrame(data)
df
```

--- 

### Discussions and Takeaways 

- Check assumptions: normality, independence, equal variance.
- Use nonparametric methods when assumptions fail.
- Statistical significance ≠ practical importance.
- Clear hypotheses + clean data = valid conclusions.

### Further Readings 
- SciPy.stats Documentation: https://docs.scipy.org/doc/scipy/reference/stats.html
- “Practical Statistics for Data Scientists” — Bruce & Gedeck
- “Think Stats” by Allen B. Downey 
