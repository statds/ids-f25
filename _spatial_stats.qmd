---
title: "Spatial Statistical Methods"
author: "Quinn Saltus"
format: revealjs
---

<!-- TODO
This document is formatted to render slides. Before PR'ing, follow guidelines in classnotes (remove yaml header, fix heading depth, etc)
-->

### Introduction

Spatially dependent data can cause problems for standard statistical methods. Many common statistical tools assume that individual observations are independent, but if location matters, then close data points will be more similar than far data points.

Spatial Statistics can:

+ Quantify the spatial dependence among data
+ Improve predictive models' accuracy and statistical rigor

## Dataset {.smaller}

The dataset I'll use for demonstration is the [Turnout by County for the 1980 Election](https://www.openml.org/search?type=data&status=active&id=507).

```{python}
from plotnine import *
import pandas as pd
import geopandas as gp
import numpy as np
from sklearn.datasets import fetch_openml

data = fetch_openml(name='space_ga', version=1)

df = data.data
df.columns = df.columns.str.lower()

df["pct_voter"] = np.exp(data.target) * 100

# convert to degrees
df["xcoord"] = df["xcoord"] / 10**6
df["ycoord"] = df["ycoord"] / 10**6

df = gp.GeoDataFrame(
    data=df,
    geometry=gp.points_from_xy(df["xcoord"], df["ycoord"]),
    crs="EPSG:4269"
)

(ggplot(df, aes(x="xcoord", y="ycoord", color="pct_voter"))
    + geom_point(size=0.6)
    + scale_color_continuous(cmap_name="Spectral")
    + coord_equal(ratio=1.3)
    + labs(
        title="Turnout Percentage By County for the 1980 Election",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa")
    )
)
```

---

A basic multiple linear regression model will serve as the baseline for non-spatial performance.

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, Y_train, Y_test = train_test_split(
    X := df[["pop", "education", "houses", "income"]],
    Y := df["pct_voter"],
    test_size=0.2, random_state=1234
)

model_basic = LinearRegression().fit(X_train, Y_train)
```

:::: {.columns}
::: {.column}

```{python}
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_basic.predict(X_test),
            "True Value" : Y_test,
            }), 
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + theme_light()
    + theme(figure_size=(4, 3))
)
```

:::
::: {.column}

```{python}
print(f"Coefficients: {model_basic.coef_}")
print(f"Test R-squared: {model_basic.score(X_test, Y_test):.4f}")
```

:::
::::

## Libraries

The libraries I use are The Python Spatial Analysis Library and PyKrige. We haven't used these in class before, so you'll probably need to install them with Pip.

``` {.shell}
pip install pykrige
pip install pysal
```

I'll also use `Scipy`'s `KDTree` class, which can efficiently find nearby points. This is important for speed with large datasets.

```{python}
#| echo: true

import libpysal
from scipy.spatial import KDTree
from pysal.explore import esda
from pysal.model import spreg
import pykrige as pk
```

## Spatial Weights {.smaller}

Standard regression models errors as completely random:

$$
Y = \beta X + \epsilon
$$

The spatial error model (SEM) formulates residuals as a weighted mean of nearby errors plus a random error term:

$$
Y = \beta X + u, u = \lambda W u + \epsilon 
$$

With PySAL, we need to precompute the weights matrix $W$ to give to analysis functions. I'll use inverse square distance, but other methods like K-Nearest-Neighbors (`libpysal.weights.KNN`) and [Gabriel Graphs](https://en.wikipedia.org/wiki/Gabriel_graph) (`libpysal.weights.Gabriel`) are available.

```{python}
#| echo: true

dist_kdtree = KDTree(df[["xcoord", "ycoord"]])
weights = libpysal.weights.DistanceBand(
    dist_kdtree,
    threshold=5, # limit search distance
    binary=False, # use numeric weights
    alpha=-2 # use inverse square distance
)
```

## Moran's I {.smaller}

:::: {.columns}
::: {.column width="70%"}

Moran's I is a common measure of spatial autocorrelation, ranging from -1 (perfect negative autocorrelation) to 1 (perfect positive autocorrelation).

```{python}
#| echo: true

moran = esda.Moran(y = df["pct_voter"], w=weights)
moran.plot_scatter()
```

:::
::: {.column width="30%"}

Some example I-values using weights based on adjacency.

<p><a href="https://commons.wikimedia.org/wiki/File:Moran%27s_I_example.png#/media/File:Moran's_I_example.png"><img src="https://upload.wikimedia.org/wikipedia/commons/f/f0/Moran%27s_I_example.png" alt="Moran's I example.png" height="300" width="500"></a></p>

-------------------------------------------------------------

For this data,

```{python}
print(f"Moran's I = {moran.I:.4f}")
print(f"Moran's I p-value = {moran.p_rand:.4f}")
```

:::
::::

## Kriging

Kriging is a nonparametric method of interpolation. Using PyKrige, we can estimate the mean of a parameter across the spatial plane.

```{python}
df_train, df_test = train_test_split(df, test_size=0.2, random_state=1234)
```

```{python}
#| echo: true

krige = pk.OrdinaryKriging(
    x=df_train["xcoord"],
    y=df_train["ycoord"],
    z=df_train["pct_voter"],
    coordinates_type="geographic",
)

# use trained model to estimate test data
df_test["krige_estimate"], _ = krige.execute(
    "points", df_test["xcoord"], df_test["ycoord"]
)

df_test["krige_residual"] = df_test["pct_voter"] - df_test["krige_estimate"]
```

---

:::: {.columns}
::: {.column}

```{python}
(ggplot(df_test, aes(x="xcoord", y="ycoord", color="krige_estimate"))
    + geom_point(size=0.6)
    + scale_color_continuous(cmap_name="Spectral")
    + coord_equal(ratio=1.3)
    + labs(
        title="Kriging-Estimated Turnout",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa"),
        figure_size=(6, 3.5)
    )
)
```

:::
::: {.column}

```{python}
(ggplot(df_test, aes(x="xcoord", y="ycoord", color="krige_residual"))
    + geom_point(size=0.6)
    + scale_color_continuous(
        cmap_name="PuOr",
        limits=(max(abs(df_test["krige_residual"])), -max(abs(df_test["krige_residual"])))
    )
    + coord_equal(ratio=1.3)
    + labs(
        title="Residuals of Kriging Estimates of Turnout",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa"),
        figure_size=(6, 3.5)
    )
)
```

:::
::::

Kriging seems to be capable of controlling spatial effects.

```{python}
(ggplot(df_test, aes(x="krige_residual", fill="krige_residual"))
    + geom_histogram(
        bins=20,
        fill="#99bbff",
        color="#000000"
    )
    + labs(
        title="Residuals of Kriging (No Regression)",
        x=f"Error (Percentage Points)"
    )
    + theme_light()
    + theme(
        figure_size=(6, 2.5)
    )
)
```

## SEM Regression With Kriging {.smaller}

```{python}
df_train = df_train.reset_index()
df_test = df_test.reset_index()

p_train = df_train[["pop", "education", "houses", "income"]].to_numpy()
x_train = df_train[["xcoord", "ycoord"]].to_numpy()
y_train = df_train["pct_voter"].to_numpy()

p_test = df_test[["pop", "education", "houses", "income"]].to_numpy()
x_test = df_test[["xcoord", "ycoord"]].to_numpy()
y_test = df_test["pct_voter"].to_numpy()
```

```{python}
#| echo: true

from pykrige.rk import RegressionKriging

model_rk = RegressionKriging(regression_model=LinearRegression())

# Note that predictors, points, and targets must be np arrays, not DataFrame
model_rk.fit(p_train, x_train, y_train)
```

:::: {.columns}
::: {.column}

```{python}
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_rk.predict(p_test, x_test),
            "True Value" : Y_test,
            }),
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + theme_light()
    + theme(figure_size=(4, 3))
)
```

```{python}
print(
    "R-squared (Regression):",
    f"{model_rk.regression_model.score(p_test, y_test):.4f}"
)
print(
    "R-squared (Regression + Kriging):",
    f"{model_rk.score(p_test, x_test, y_test):.4f}"
)
```

:::
::: {.column}

```{python}
df_test["basic_pred"] = model_basic.predict(p_test) - df_test["pct_voter"]
df_test["krige_pred"] = model_rk.predict(p_test, x_test) - df_test["pct_voter"]

(ggplot(df_test, aes(x="basic_pred", y="krige_pred"))
    + geom_abline(slope=1, intercept=0, color="#888888", linetype="dashed")
    + geom_abline(slope=0, intercept=0, color="#888888", linetype="dashed")
    + geom_point(color="#00000066")
    + geom_smooth(method="lm", se=False, color="#0099ff", linetype="dashed")
    + coord_equal()
    + labs(
        title="Effect of Kriging on Residuals",
        x="Regression Residuals",
        y="Regression + Kriging Residuals"
    )
    + theme_light()
    + theme(figure_size=(4, 4))
)
```

:::
::::

## Conclusion

## Further Readings

[The PyKrige Documentation](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/)

<!-- TODO cite data -->
<!-- TODO cite Moran image -->