---
title: "Spatial Statistical Methods"
author: "Quinn Saltus"
format: revealjs
bibliography: references.bib
---

<!-- TODO
Before PR'ing, follow guidelines in classnotes (newlines for non-wrapping editors, remove yaml header, fix heading depth, etc)
rendering for presentation -> rendering for class notes
-->

### Introduction

Spatially dependent data can cause problems for standard statistical methods. 

If location matters, then close data points will be more similar than far data points, so you can't assume independence.

Spatial Statistics can:

+ Quantify the spatial dependence among data
+ Improve predictive models' accuracy and statistical rigor

## Dataset {.smaller}

The dataset I'll use for demonstration is the [Turnout by County for the 1980 Election](https://www.openml.org/search?type=data&status=active&id=507) from @Pace1997.

```{python}
from plotnine import *
import pandas as pd
import geopandas as gp
import numpy as np
from sklearn.datasets import fetch_openml

data = fetch_openml(name='space_ga', version=1)

df = data.data
df.columns = df.columns.str.lower()

df["pct_voter"] = np.exp(data.target) * 100

# convert to degrees
df["xcoord"] = df["xcoord"] / 10**6
df["ycoord"] = df["ycoord"] / 10**6

df = gp.GeoDataFrame(
    data=df,
    geometry=gp.points_from_xy(df["xcoord"], df["ycoord"]),
    crs="EPSG:4269"
)

(ggplot(df, aes(x="xcoord", y="ycoord", color="pct_voter"))
    + geom_point(size=0.6)
    + scale_color_continuous(cmap_name="Spectral")
    + coord_equal(ratio=1.3)
    + labs(
        title="Turnout Percentage By County for the 1980 Election",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa")
    )
)
```

## Libraries

The libraries I use are The Python Spatial Analysis Library and PyKrige. We haven't used these in class before, so you'll probably need to install them with Pip.

``` {.shell}
pip install pykrige
pip install pysal
```

I'll also use `Scipy`'s `KDTree` class, which can efficiently find nearby points. This is important for speed with large datasets.

```{python}
#| echo: true

import libpysal
from scipy.spatial import KDTree
from pysal.explore import esda
import pykrige as pk
```

## Spatial Weights {.smaller}

Standard regression has error terms that are completely random:

$$
Y = \beta X + \epsilon
$$

The spatial error model (SEM) formulates residuals as a weighted mean of nearby errors plus a random error term:

$$
Y = \beta X + u, u = \lambda W u + \epsilon 
$$

With PySAL, we need to precompute the weights matrix $W$ to give to analysis functions. I'll use inverse square distance, but other methods like K-Nearest-Neighbors (`libpysal.weights.KNN`) and [Gabriel Graphs](https://en.wikipedia.org/wiki/Gabriel_graph) (`libpysal.weights.Gabriel`) are available.

```{python}
#| echo: true

dist_kdtree = KDTree(df[["xcoord", "ycoord"]])
weights = libpysal.weights.DistanceBand(
    dist_kdtree,
    threshold=2.5, # limit search distance
    binary=False, # use numeric weights
    alpha=-2 # use inverse square distance
)
```

## Moran's I {.smaller}

:::: {.columns}
::: {.column width="70%"}

Moran's I is a common exploratory measure of spatial autocorrelation based on each point's similarity to its neighbors.

```{python}
#| echo: true

moran = esda.Moran(y = df["pct_voter"], w=weights)
moran.plot_scatter()
```

:::
::: {.column width="30%"}

Some example I-values using adjacency weights [@MoranExample].

<p><a href="https://commons.wikimedia.org/wiki/File:Moran%27s_I_example.png#/media/File:Moran's_I_example.png"><img src="https://upload.wikimedia.org/wikipedia/commons/f/f0/Moran%27s_I_example.png" alt="Moran's I example.png" height="300" width="500"></a></p>

-------------------------------------------------------------

For this data,

```{python}
print(f"Moran's I = {moran.I:.4f}")
print(f"Moran's I p-value = {moran.p_rand:.4f}")
```

:::
::::

## Kriging

Kriging is a nonparametric method of interpolation. Using PyKrige, we can estimate the mean of a parameter across the spatial plane.

```{python}
from sklearn.model_selection import train_test_split
df_train, df_test = train_test_split(df, test_size=0.2, random_state=1234)
```

```{python}
#| echo: true

krige = pk.OrdinaryKriging(
    x=df_train["xcoord"],
    y=df_train["ycoord"],
    z=df_train["pct_voter"],
    coordinates_type="geographic",
)

# use trained model to estimate test data
df_test["krige_estimate"], _ = krige.execute(
    "points", df_test["xcoord"], df_test["ycoord"]
)

df_test["krige_residual"] = df_test["pct_voter"] - df_test["krige_estimate"]
```

---

:::: {.columns}
::: {.column}

```{python}
(ggplot(df_test, aes(x="xcoord", y="ycoord", color="krige_estimate"))
    + geom_point(size=0.6)
    + scale_color_continuous(cmap_name="Spectral")
    + coord_equal(ratio=1.3)
    + labs(
        title="Kriging-Estimated Turnout",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa"),
        figure_size=(6, 3.5)
    )
)
```

:::
::: {.column}

```{python}
(ggplot(df_test, aes(x="xcoord", y="ycoord", color="krige_residual"))
    + geom_point(size=0.6)
    + scale_color_continuous(
        cmap_name="PuOr",
        limits=(max(abs(df_test["krige_residual"])), -max(abs(df_test["krige_residual"])))
    )
    + coord_equal(ratio=1.3)
    + labs(
        title="Residuals of Kriging Estimates of Turnout",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa"),
        figure_size=(6, 3.5)
    )
)
```

:::
::::

Kriging seems to be capable of controlling spatial effects.

```{python}
(ggplot(df_test, aes(x="krige_residual", fill="krige_residual"))
    + geom_histogram(
        bins=20,
        fill="#99bbff",
        color="#000000"
    )
    + labs(
        title="Residuals of Kriging (No Regression)",
        x=f"Error (Percentage Points)"
    )
    + theme_light()
    + theme(
        figure_size=(6, 2.5)
    )
)
```

## Modeling {.smaller}

A basic multiple linear regression model will serve as the baseline for non-spatial performance.

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, Y_train, Y_test = train_test_split(
    X := df[["pop", "education", "houses", "income"]],
    Y := df["pct_voter"],
    test_size=0.2, random_state=1234
)

model_basic = LinearRegression().fit(X_train, Y_train)
```

:::: {.columns}
::: {.column}

```{python}
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_basic.predict(X_test),
            "True Value" : Y_test,
            }), 
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + theme_light()
    + theme(figure_size=(4, 3))
)
```

:::
::: {.column}

```{python}
print(f"Test R-squared: {model_basic.score(X_test, Y_test):.4f}")
```

:::
::::

## SEM Regression With Kriging {.smaller}

<!-- TODO
drop fig 2, duplicate old regression scatter plot   
-->

```{python}
df_train = df_train.reset_index()
df_test = df_test.reset_index()

p_train = df_train[["pop", "education", "houses", "income"]].to_numpy()
x_train = df_train[["xcoord", "ycoord"]].to_numpy()
y_train = df_train["pct_voter"].to_numpy()

p_test = df_test[["pop", "education", "houses", "income"]].to_numpy()
x_test = df_test[["xcoord", "ycoord"]].to_numpy()
y_test = df_test["pct_voter"].to_numpy()
```

```{python}
#| echo: true

from pykrige.rk import RegressionKriging

model_rk = RegressionKriging(regression_model=LinearRegression())

# Note that predictors, points, and targets must be np arrays, not DataFrame
model_rk.fit(p_train, x_train, y_train)
```

:::: {.columns}
::: {.column}

```{python}
print(
    "R-squared (Regression + Kriging):",
    f"{model_rk.score(p_test, x_test, y_test):.4f}"
)
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_rk.predict(p_test, x_test),
            "True Value" : Y_test,
            }),
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + labs(title="Regression + Kriging")
    + theme_light()
    + theme(figure_size=(4, 3))
)
```

:::
::: {.column}

```{python}
print(
    "R-squared (Regression):",
    f"{model_rk.regression_model.score(p_test, y_test):.4f}"
)
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_basic.predict(X_test),
            "True Value" : Y_test,
            }), 
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + labs(title="Regression Only")
    + theme_light()
    + theme(figure_size=(4, 3))
)
```

:::
::::

## Conclusions

- PySAL offers strong tools for spatial data exploration and visualization. 
- PyKrige lets you drop in spatial predictors or post-processing.
- Spatial analysis is flexible, but you have to validate the method.

## Further Readings

[ESRI's explanation of spatial autoregression](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-spatial-autoregression-works.htm)

[The PySAL documentation](https://pysal.org/)

[The PyKrige documentation](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/)

If you're particularly interested in spatial problems and tools, I recommend GEOG 2500 (Introduction to Geographic Information Systems).

### Citations