---
title: "Spatial Statistical Methods"
author: "Quinn Saltus"
format:
  revealjs:
    smaller: true
    scrollable: true
---

<!-- TODO when converting from presentation to section, fix heading levels -->

### Intro

Spatially dependent data can cause problems for standard statistical methods. Many common statistical tools assume that individual observations are independent, but if location matters, then close data points will be more similar than far data points, violating the assumption. 

Techniques like bootstrap and cross-validation mean that we don't necessarily need to make assumptions of independence in order to make accurate models. But that doesn't mean we should ignore the `latitude` and `longitude` columns of our data. Spatial Statistics can:

+ Quantify the spatial dependence among data
+ Improve predictive models' accuracy and statistical rigor

---

### Dataset

The dataset I'll use for demonstration is the [Turnout by County for the 1980 Election](https://www.openml.org/search?type=data&status=active&id=507).

```{python}
from plotnine import *
import pandas as pd
import geopandas as gp
import numpy as np
from sklearn.datasets import fetch_openml

data = fetch_openml(name='space_ga', version=1)

df = data.data
df.columns = df.columns.str.lower()

df["pct_voter"] = np.exp(data.target) * 100

# convert to degrees
df["xcoord"] = df["xcoord"] / 10**6
df["ycoord"] = df["ycoord"] / 10**6

df = gp.GeoDataFrame(
    data=df,
    geometry=gp.points_from_xy(df["xcoord"], df["ycoord"]),
    crs="EPSG:4269"
)

(ggplot(df, aes(x="xcoord", y="ycoord", color="pct_voter"))
    + geom_point(size=0.6)
    + scale_color_continuous(cmap_name="Spectral")
    + coord_equal(ratio=1.3)
    + labs(
        title="Turnout Percentage By County for the 1980 Election",
        x="Longitude",
        y="Latitude"
    )
    + theme_light()
    + theme(
        panel_background=element_rect(fill="#aaaaaa")
    )
)
```

---

A basic multiple linear regression model will serve as the baseline for non-spatial performance.

:::: {.columns}
::: {.column width="60%"}

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X_train, X_test, Y_train, Y_test = train_test_split(
    df[["pop", "education", "houses", "income"]],
    df["pct_voter"], test_size=0.3, random_state=42
)
model_basic = LinearRegression().fit(X_train, Y_train)
print(f"Coefficients: {model_basic.coef_}")
print(f"R-squared: {model_basic.score(X_test, Y_test):.4f}")
```

:::
::: {.column width="40%"}

```{python}
print(f"Coefficients: {model_basic.coef_}")
print(f"R-squared: {model_basic.score(X_test, Y_test):.4f}")
```

:::
::::

```{python}
(ggplot(
        pd.DataFrame({
            "Predicted Value" : model_basic.predict(X_test),
            "True Value" : Y_test,
            }), 
        aes(x="Predicted Value", y="True Value")
    )
    + geom_point(color="#00000088")
    + geom_abline(slope=1, intercept=0, color="red", linetype="dashed")
    + theme_light()
    + theme(figure_size=(5, 4))
)
```

---

### Spatial Autocorrelation

```{python}
import libpysal
from scipy.spatial import KDTree
from pysal.explore import esda

dist_kdtree = KDTree(df[["xcoord", "ycoord"]])

weights = libpysal.weights.DistanceBand(
    dist_kdtree,
    threshold=5, # limit search to 5 degrees
    binary=False, 
    alpha=-2 # use inverse square distance
)
```

```{python}
moran = esda.Moran(y = df["pct_voter"], w=weights)

moran.plot_scatter()

print(f"Moran's I: {moran.I:.4f}")
print(f"Moran's I p-value (under randomization): {moran.p_rand:.4f}")
```

## Conclusion

## Further Readings

[The PyKrige Documentation](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/)

<!-- TODO cite data -->