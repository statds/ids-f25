## Synthetic Minority Over-sampling Technique (SMOTE)

This section was written by Hannah Levine, a junior majoring in 
Applied Data Analysis.

+ Pay attention to the sectioning levels.
+ Cite references with their bib key.
+ In examples, maximize usage of data set that the class is familiar with.
+ Could use datasets in Python packages or downloadable on the fly.
+ Test your section by `quarto render <filename.qmd>`.


### Introduction

In classification problems, imbalanced data refers to skewed class
distribution. Classification problems use numerical variables to 
produce a categorical outcome, like diagnosing a disease based on clinical
data or detecting fraudulent transactions. In a simple example, you have
2 classes for a medical diagnosis, "healthy" and "not healthy". For rare
diseases, there can be a large difference in the amount of individuals
who are "healthy" and who are "not healthy", with the "healthy"
far outnumbering the "not healthy". The "not healthy" class is
known as the minority class.

This is important because machine learning models can become biased towards 
the majority class, and be innacurate in predicting for the minority class. 
This is especially apparent in problems with multiple classes. Adressing this 
class imbalance helps to improve the accuracy of the model. An easy solution 
is to just oversample the minority class, creating duplicates. However, this 
doesn't really add any new information to the model. Another solution is
undersampling of the majority class, but then important information
might be left out. A better technique is Synthetic Minority Over-Sampling
Technique, or SMOTE.

### SMOTE

While random oversampling creates exact replicas of minority observations,
SMOTE creates synthetic samples based on the existing observations in
the minority class. 
    1. SMOTE identifies minority class(es) in the dataset
    2. For each observation in the minority class, SMOTE identifies
    the k nearest neighbors, with k being specified prior (usually k=5).
    3. A line is drawn between the minority observation and one of its
    k nearest neighbors. 
    4. The difference is found, multiplied by a random number between 0 and 1, then added to the initial observation to generate the synthetic
    sample.
    5. This process is repeated until the desired ratio is
    reached, usually when the minority class equals the size of the
    majority class.
The resulting dataset is more balanced, but there's less of a chance of
overlapping data points because it's not duplicates.

This can also be demonstrated by the following algorithm:
$$
x_{new} = x_{i} + \lambda(x_{neighbor} - x_{i})
$$

where $x_{i}$ is the selected minority observation, $x_{new}$ is the
generated synthetic sample, $\lambda$ is a random number between 0 and
1 sampled from a uniform distribution, and $x_{neighbor}$ is the randomly
chosen neighbor out of the identified k neearest neighbors.

***maybe include visual diagram showing points and synthetic samples being created??***

### Example: Created Synthetic Dataset

We can generate a random dataset using the `make_classification`
function from `sklearn.datasets`.

```{python}
from sklearn.datasets import make_classification
from collections import Counter
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# create imbalanced dataset
X, y = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    weights=[0.99, 0.01],
    random_state=1234
)

print(f"Original class distribution: {Counter(y)}")
```

```{python}
# apply SMOTE
smote = SMOTE(random_state=1234)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("After SMOTE class distribution:", Counter(y_resampled))
```

For the application of SMOTE, I only specify a random state to ensure
reproducibility. Other parameters that can be used are
`sampling_strategy` and `k_neighbors`. `sampling_strategy` can be a float, 
string, or dict. Float is only usable for problems with 2 classes. For
string, the possible choices are:
    - "minority": resample only the minority class
    - "not minority": resample all but the minority class
    - "not majority": resample all but the majority class
    - "all": resample all classes
    - "auto": same as "not majority", is default 
`k_neighbors` is an int that defaults to 5.




```{python}
# visualize before and after
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.scatter(X[y==0][:, 0], X[y==0][:, 1], c="#DEA8FF", label="Majority", alpha=0.5)
ax1.scatter(X[y==1][:, 0], X[y==1][:, 1], c="#A3007D", label="Minority", alpha=0.5)
ax1.set_title("Original class distribution")
ax1.legend()

ax2.scatter(X_resampled[y_resampled==0][:, 0], X_resampled[y_resampled==0][:, 1], c="#DEA8FF",
            label="Majority", alpha=0.5)
ax2.scatter(X_resampled[y_resampled==1][:, 0], X_resampled[y_resampled==1][:, 1], c="#A3007D",
            label="Minority (synthetic)", alpha=0.5)
ax2.set_title("After SMOTE class distribution")
ax2.legend()

plt.show()
```

Looking at the scatter plots, you can see the difference in class size
before and after using SMOTE. 
You can also see the synthetic minority points being created between 
existing minority observations.


### Variants

Boredrline, adasyn, tomek,

when to use plain: moderate imbalance,

###

### Conclusion

conclude, also mention advantages and disadvantages

### Further Readings

Put links to further materials.
