## Synthetic Minority Over-sampling Technique (SMOTE)

This section was written by Hannah Levine, a junior majoring in 
Applied Data Analysis.

+ Pay attention to the sectioning levels.
+ Cite references with their bib key.
+ In examples, maximize usage of data set that the class is familiar with.
+ Could use datasets in Python packages or downloadable on the fly.
+ Test your section by `quarto render <filename.qmd>`.


### Introduction

In classification problems, imbalanced data refers to skewed class
distribution. Classification problems use numerical variables to 
produce a categorical outcome, like diagnosing a disease based on clinical
data or detecting fraudulent transactions. In a simple example, you have
2 classes for a medical diagnosis, "healthy" and "not healthy". For rare
diseases, there can be a large difference in the amount of individuals
who are "healthy" and who are "not healthy", with the "healthy"
far outnumbering the "not healthy". The "not healthy" class is
known as the minority class.

This is important because machine learning models can become biased towards 
the majority class, and be innacurate in predicting for the minority class. 
This is especially apparent in problems with multiple classes. Adressing this 
class imbalance helps to improve the accuracy of the model. An easy solution 
is to just oversample the minority class, creating duplicates. However, this 
doesn't really add any new information to the model. Another solution is
undersampling of the majority class, but then important information
might be left out. A better technique is Synthetic Minority Over-Sampling
Technique, or SMOTE.

### SMOTE

While random oversampling creates exact replicas of minority observations,
SMOTE creates synthetic samples based on the existing observations in
the minority class. 
    1. SMOTE identifies minority class(es) in the dataset
    2. For each observation in the minority class, SMOTE identifies
    the k nearest neighbors, with k being specified prior (usually k=5).
    3. A line is drawn between the minority observation and one of its
    k nearest neighbors. 
    4. The difference is found, multiplied by a random number between 0 and 1, then added to the initial observation to generate the synthetic
    sample.
    5. This process is repeated until the desired ratio is
    reached, usually when the minority class equals the size of the
    majority class.
The resulting dataset is more balanced, but there's less of a chance of
overlapping data points because it's not duplicates.

This can also be demonstrated by the following algorithm:
$$
x_{new} = x_{i} + \lambda(x_{neighbor} - x_{i})
$$

where $x_{i}$ is the selected minority observation, $x_{new}$ is the
generated synthetic sample, $\lambda$ is a random number between 0 and
1 sampled from a uniform distribution, and $x_{neighbor}$ is the randomly
chosen neighbor out of the identified k neearest neighbors.

***maybe include visual diagram showing points and synthetic samples being created??***

### Example: Created Synthetic Dataset

```{python}
from sklearn.datasets import make_classification
from collections import Counter
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

X, y = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=3,
    n_redundant=1,
    n_classes=2,
    weights=[0.99, 0.01],
    random_state=1234
)

print(f"Class distribution: {Counter(y)}")
```

### What is SMOTE?

Put materials on topic 2 here.

### Variants

Put matreials on topic 3 here.

###

### Conclusion

conclude, also mention advantages and disadvantages

### Further Readings

Put links to further materials.
