[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book is free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-f25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Spring 2025, Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of useful data challenges:\n\nASA Data Challenge Expo: big data in 2025\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2026.\nA good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAgostino, Michael Angelo\n\nFurther understanding of github and improve my workflow.\nBuild a project in order to gain real world data science experience.\n\nBlake, Roger Cooley\n\nGain experience using git and github\nComplete a project that transforms real-world data into valuable insights\nMaster control over my computer and its file system\n\nCao, Enshuo\nChen, Irene\n\nBecome proficient in using Git and Python\nDeepen machine learning and data science skills\n\nChen, Jingang Calvin\n\ngain exposure and hands on experience to machine learning models\nbecome proficient in Git and Github\napply statistical and data analysis skills to a data science project\n\nFazzina, Sophia Carmen\n\nGet comfortable using Git and GitHub\nWork on a data science project from start to finish\nGet a good grade in this class\n\nHaerter, Alejandro Erik\n\nUnderstand proper workflow principles via GIt\nFind an intersection between data science and economics\nLearn efficient use of AI for trivial tasks\n\nLawrence, Claire Elise\nLevine, Hannah Maya\n\nBecome more comfortable using Git/GitHub\nImprove Python/style skills\nGain experience with real world projects\n\nLucey, Sonia Niamh\n\nLearn practical applications of Data Science and Economics\nLearn how to use Git/GitHub\nImprove Python/coding skills\n\nMayer-Costa, Jaden Paulo\n\nGain hands on experience working with real-world data\nBecome proficient in using Git and Github.\nContinue adding to python skills and using code editing software.\n\nMilun, Lewis Aaron\n\nBecome proficient in Git\nLearn more about the processes involved in Data Science\n\nMontalvo, Victor Samuel\nPatel, Sahil Sanjay\n\nI want learn more about time series forecasting\nI want to be more comfortable using git and GitHub\nI want to learn about the applications of Data Science in Finance\n\nPatel, Tulsi Manankumar\nPerkins, Jack Thomas\n\nBe able to incorporate git into my own workflow.\nUse python to boost efficiency in data problems.\nBecome more comfortable with python for data science.\n\nSaltus, Quinn Lloyd Turner\n\nGain proficiency in data visualization with Python\nBuild experience using version control to expand the scope of my projects\nLearn when and how to apply libraries (such as numpy & pytorch) to improve my code’s performance\nBecome familiar with machine learning tools and techniques\n\nSaxena, Aanya\nSchlessel, Jacob E\n\nLearn about different classification algorithms\nPractice using python for analyzing data\nBecome comfortable with Git and Github\n\nSgro, Owen Bentley\n\nI want to become better acquainted with command line interface and using the terminal.\nI want more experience with creating my own projects and finding my own way of creating things.\nI want to further my experience with python (and any other languages we may use) and be able to apply that to other classes.\n\nTang, Wilson Chen\n\nI want to be very comfortable with using GitHub and GitBash\nI want to learn how to have a clear style\nI want to use programming tools in a professional way\n\nTran, Justin\n\nTo become proficient in the knowledge of Git.\nTo adopt command line knowledge into the workforce.\nFostering good practices with commits.\n\nWhite, Abigail Lynn\n\nBecome more comfortable working with GitHub.\nLearn and memorize more commands used in the Terminal.\nAdvance my statistical skills through a data science project.\n\nWishneski, Emma Irene\n\nGain a better understanding of data science, and real life applications\nGet comfortable using github, python, and other applications\nBecome familiar with useful techniques for machine learning\n\nYoon, Jessica Nari\n\nBecome comfortable with Git and version control.\n\nApply skills from this classes to other courses.\nUnderstand my computer better.\n\nZhang, Mark Justin\n\nGet comfortable with command line and Git\nLearn to make my own data science projects\nlearn theory and application of ML algorithms\n\n\n\n\nStudents in 5255\n\nAnzalone, Matthew James\n\nBuild professional workflow habits for a data-science career\n\nIncrease my Python knowledge enough that I could eventually create usable packages\n\nImprove my data-driven thinking outside of the bounds of economics\n\nGomez-Haibach, Konrad\nPlotnikov, Alexander\n\nGain working knowledge of Git and project management.\nLearn AI and Machine Learning algorithms.\nApply skillset by working on different projects.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(2819 + 4075 + 6227 + 5139 + 4768 + 109)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Chen,Jingang Calvin',\n 'Montalvo,Victor Samuel',\n 'Cao,Enshuo',\n 'Chen,Irene',\n 'Haerter,Alejandro Erik',\n 'Saxena,Aanya',\n 'Gomez-Haibach,Konrad',\n 'Sgro,Owen Bentley',\n 'Lucey,Sonia Niamh',\n 'Yoon,Jessica Nari',\n 'Patel,Sahil Sanjay',\n 'Plotnikov,Alexander',\n 'Milun,Lewis Aaron',\n 'Patel,Tulsi Manankumar',\n 'Perkins,Jack Thomas',\n 'Fazzina,Sophia Carmen',\n 'Tang,Wilson Chen',\n 'Wishneski,Emma Irene',\n 'Lawrence,Claire Elise',\n 'White,Abigail Lynn',\n 'Mayer-Costa,Jaden Paulo',\n 'Anzalone,Matthew James',\n 'Zhang,Mark Justin',\n 'Saltus,Quinn Lloyd Turner',\n 'Tran,Justin',\n 'Levine,Hannah Maya',\n 'Blake,Roger Cooley',\n 'Schlessel,Jacob E',\n 'Agostino,Michael Angelo']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proportions and rates\nDatabase operation with Structured query language (SQL)\nGrammar of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map with gmplot\nAnimation\nSupport vector machine\nRandom forest\nGradient boosting machine\nNaive Bayes\nNeural networks basics\nMLP/ANN/CNN/RNN/LSTM\nUniform manifold approximation and projection\nAutomatic differentiation\nDeep learning\nTensorFlow\nAutoencoders\nK-means clustering\nPrincipal component analysis\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\nMaking presentations with Quarto\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/18\nChen, Jingang Calvin\nSyntax of Markdown\n\n\n09/23\nMontalvo, Victor Samuel\n\n\n\n09/25\nCao, Enshuo\nNaive Bayes\n\n\n09/30\nLucey, Sonia Niamh\nGrammar of Graphics with Plotnine\n\n\n10/02\nHaerter, Alejandro Erik\nGeospatial Data with GeoPandas\n\n\n10/02\nSaxena, Aanya\n\n\n\n10/07\nGomez-Haibach, Konrad\n\n\n\n10/07\nSgro, Owen Bentley\nData Manipulation with Pandas\n\n\n10/09\nChen, Irene\nPresentations with Quarto\n\n\n10/09\nYoon, Jessica Nari\nHypothesis Testing with Scipy.stats\n\n\n10/14\nPatel, Sahil Sanjay\nWeb Scraping\n\n\n10/14\nPlotnikov, Alexander\nRandom Forests\n\n\n10/16\nMilun, Lewis Aaron\nDatabase operation with Structured query language (SQL)\n\n\n10/16\nPatel, Tulsi Manankumar\n\n\n\n10/23\nPerkins, Jack Thomas\n\n\n\n10/23\nFazzina, Sophia Carmen\nK-means clustering\n\n\n10/28\nTang, Wilson Chen\nHow to call R in Python\n\n\n10/28\nWishneski, Emma Irene\nSupport Vector Machine\n\n\n10/30\nLawrence, Claire Elise\n\n\n\n11/04\nWhite, Abigail Lynn\nEffective data science communication\n\n\n11/04\nMayer-Costa, Jaden Paulo\nPersonal Webpage on GitHub\n\n\n11/06\nAnzalone, Matthew James\nDeep Learning\n\n\n11/06\nZhang, Mark Justin\nUniform manifold approximation and projection\n\n\n11/11\nSaltus, Quinn Lloyd Turner\nSpatial Statistical Methods\n\n\n11/11\nTran, Justin\nAnimations\n\n\n11/11\nLevine, Hannah Maya\nSynthetic Minority Over-sampling Technique (SMOTE)\n\n\n11/13\nBlake, Roger Cooley\nVariable importance metrics\n\n\n11/13\nSchlessel, Jacob E\nFilling missing data with multiple imputation\n\n\n11/13\nAgostino, Michael Angelo\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is available under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nChen, Jingang Calvin; Montalvo, Victor Samuel; Cao, Enshuo; Lucey, Sonia Niamh; Haerter, Alejandro Erik\n\n\n11/20\nSaxena, Aanya; Sgro, Owen Bentley; ; Chen, Irene; Yoon, Jessica Nari; Patel, Sahil Sanjay\n\n\n12/02\nMilun, Lewis Aaron; Patel, Tulsi Manankumar; Perkins, Jack Thomas; Fazzina, Sophia Carmen; Tang, Wilson Chen\n\n\n12/04\nWishneski, Emma Irene; Lawrence, Claire Elise; White, Abigail Lynn; Mayer-Costa, Jaden Paulo; Zhang, Mark Justin\n\n\n12/13\nSaltus, Quinn Lloyd Turner; Tran, Justin; Levine, Hannah Maya; Blake, Roger Cooley; Schlessel, Jacob E; Agostino, Michael Angelo\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nSynchronize your local repo of the classnotes with my classnotes repo.\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file _mysection.qmd as an example, which is includeed in index.qmd. See also how _ethics.qmd is included into intro.qmd for example.\nHere is a checklist to help smooth the process.\n\nGet approval for your topic at least one week in advance. Otherwise you loose points.\nNo plagiarism. Under no circumstances should you copy someone else’s notes and use it for your contribution.\nNo yaml header. The whole souce tree is controlled by _quarto.yml.\nThe top heading level of your contribuion is section (##). See existing sections for examples.\nKeep line width under 80 characters.\nInclude a subsection (###) on further readings.\nAvoide dependence on external files (e.g., data, images, etc.). Using example datasets that are already in the data folder or that come with Python packages.\nNo usage of copyrighted images.\nWhen citing article/book references, use BibTeX (learn how from our sources).\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance if you want feedbacks.\n\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?  \nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nIf you need a reference letter, how could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-is-data-science",
    "href": "01-intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denote computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#expectations-from-this-course",
    "href": "01-intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#computing-environment",
    "href": "01-intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have that users previously faced before WSL was introduced.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump-start, here is a tutorial: Ubuntu Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding style against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McKinney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-ethics",
    "href": "01-intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) (American Statistical Association (ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#effective-data-science-communication",
    "href": "01-intro.html#effective-data-science-communication",
    "title": "1  Introduction",
    "section": "1.5 Effective Data Science Communication",
    "text": "1.5 Effective Data Science Communication\nThis section is by Abby White, a senior majoring in Statistical Data Science with a concentration in Advanced Statistics.\n\n1.5.1 Introduction\nData science communication is about more than presenting results; it is about helping others understand and act on them. A clear visualization, a well-phrased sentence, or a reproducible workflow can determine whether your analysis makes an impact or gets lost in translation.\nIn this presentation, I will discuss:\n1. Why communication matters in data science.\n2. Key principles for clarity and transparency.\n3. How visualization and storytelling improve understanding.\n4. The role of reproducibility and ethics in reporting results.\n5. How to connect insights to action.\n\n\n1.5.2 Why Communication Matters\nEven the best analysis doesn’t mean much if people cannot understand it. Clarity, honesty, and reproducibility are what make results useful.\n\n1.5.2.1 Example: Framing a Finding Clearly\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the cleaned NYC crash data\ncrash_df = pd.read_feather(\"../ids-f25/data/nyc_crashes_cleaned.feather\")\n\n# Count crashes by borough\nborough_counts = crash_df[\"borough\"].value_counts().reset_index()\nborough_counts.columns = [\"borough\", \"crash_count\"]\n\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\", y=\"crash_count\",\n    order=borough_counts.sort_values(\"crash_count\", ascending=False)[\"borough\"],\n)\nplt.title(\"NYC Motor Vehicle Collisions by Borough (Labor Day Week, 2025)\")\nplt.xlabel(\"Borough\")\nplt.ylabel(\"Crash Count\")\n\n# Add labels above bars\nfor i, val in enumerate(borough_counts[\"crash_count\"]):\n    plt.text(i, val + (0.01 * borough_counts[\"crash_count\"].max()),\n             f\"{val:,}\", ha=\"center\", fontsize=9)\n\n# Add 5% headroom above the tallest bar\nplt.ylim(0, borough_counts[\"crash_count\"].max() * 1.05)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart shows total crashes by borough. Without labels, the pattern is visible but vague. Adding numbers and sorting by count makes the trend clear. Brooklyn and Queens lead due to population and road density.\nVisuals that highlight why something happens, not just that it happens, communicate far more effectively.\n\nPoor phrasing:\n“Brooklyn has the highest number of crashes.”\n\n\nBetter phrasing:\n“During Labor Day week 2025, Brooklyn recorded the most motor vehicle collisions, followed by Queens and Manhattan. This pattern likely reflects each borough’s larger population, higher traffic volume, and denser road networks.”\n\nThe first sentence is technically correct but empty of insight. The second version connects data to why it matters. It is specific, comparative, and interpretable.\nGood communication translates data into meaning. When presenting findings, always lead with a clear takeaway before showing details.\n\n\n\n1.5.3 Clarity and Context\nClarity comes from balancing accuracy with accessibility. The goal is to make complex analysis understandable without oversimplifying it.\nGuidelines for clear communication:\n- Lead with the main takeaway before showing details.\n- Define all technical terms (e.g., “injury severity index”).\n- Provide relative comparisons, not just counts.\n- Add short interpretations under visuals.\n\n1.5.3.1 Example: Turning Output into Insight\n\n# Calculate the average number of people injured per crash by borough\ninjury_summary = (\n    crash_df.groupby(\"borough\")[\"number_of_persons_injured\"]\n    .mean()\n    .sort_values()\n)\ninjury_summary\n\nborough\nSTATEN ISLAND    0.562500\nMANHATTAN        0.620087\nQUEENS           0.634146\nBROOKLYN         0.664557\nBRONX            0.800000\nName: number_of_persons_injured, dtype: float64\n\n\n\nTechnical phrasing:\n“Mean injuries per crash differ by borough.”\n\n\nClear phrasing:\n“On average, crashes in the Bronx result in the highest injury rates (about 0.8 people injured per crash), while Staten Island has the lowest (around 0.56). Differences are modest but suggest slightly greater crash severity in more densely populated areas.”\n\nThe first phrasing is correct but vague. It does not tell the audience how or by how much boroughs differ. The clearer version adds real numbers, ranks, and a hint of interpretation (density and traffic volume). Good phrasing gives context so the audience immediately grasps what the data shows.\nTo make this pattern easier to see visually:\n\nsns.barplot(\n    x=injury_summary.values,\n    y=injury_summary.index,\n    color=\"steelblue\"  # same color for all bars\n)\nplt.title(\"Average Injuries per Crash by Borough\")\nplt.xlabel(\"Average Number of Injured People\")\nplt.ylabel(\"Borough\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart shows that the Bronx leads with the highest average injuries per crash, while Staten Island has the lowest. Clear labeling, ordered categories, and a clean layout make the trend easy to interpret without visually exaggerating small differences.\n\n\n\n1.5.4 Visual Storytelling\nVisuals are often the clearest way to communicate a pattern. Strong visual design helps people notice relationships quickly and remember them longer.\nA good graphic should make the takeaway obvious in a few seconds. Titles, captions, and color choices all guide the audience toward what matters most.\n\n1.5.4.1 Example: Highlighting a Trend\n\n# Make sure datetime is parsed\ncrash_df[\"crash_datetime\"] = pd.to_datetime(\n    crash_df[\"crash_datetime\"], errors=\"coerce\"\n)\n\n# Group by day \ndaily = (\n    crash_df.groupby(crash_df[\"crash_datetime\"].dt.date)\n    .size()\n    .reset_index(name=\"count\")\n    .rename(columns={\"crash_datetime\": \"date\"})\n)\n\n# Convert date column back to datetime64[ns] for plotting\ndaily[\"date\"] = pd.to_datetime(daily[\"date\"])\n\nsns.lineplot(\n    data=daily.sort_values(\"date\"),\n    x=\"date\",\n    y=\"count\",\n    marker=\"o\"\n)\n\nplt.title(\"Daily NYC Crashes (Labor Day Week, 2025)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Crash Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis line chart shows how crash frequency changed across Labor Day week 2025. The dataset covers a short time window, so we see day-to-day variation rather than long-term trends. Clear labeling and a simple layout make the pattern easy to interpret without overstating the limited time frame.\nA clear title, readable x-axis, and minimal clutter make the trend easy to interpret.\n\nA short caption might read:\n“Daily crash counts fluctuate slightly over the observed period, reflecting normal day-to-day variation in traffic volume.”\n\nThat single sentence turns a small snapshot of data into an understandable story without overstating what the time range can show. Once we understand how visuals convey meaning, the next step is choosing the chart type that best fits our data.\n\n\n\n1.5.5 Choosing the Right Chart Type\nSelecting the right chart is just as important as making it look clean. The wrong type can hide a pattern or even mislead the audience, while the right one highlights exactly what matters.\nWhen deciding how to visualize data, start with the question you want to answer. Every chart should answer one specific question rather than trying to show everything at once. Different chart types serve different purposes:\nCompare Categories:\n- Chart: Bar or column chart\n- Example: Average injuries per crash by borough shows differences clearly without exaggeration.\nShow Change Over Time:\n- Chart: Line chart\n- Example: Daily or hourly crash counts reveal rush-hour spikes or weekend dips.\nDisplay Proportions:\n- Chart: Pie or stacked bar chart\n- Example: Percent of crashes involving pedestrians vs. motorists shows relative risk.\nReveal Relationships:\n- Chart: Scatter plot\n- Example: Plotting vehicle speed vs. injury severity could show how risk increases with speed.\nShow Distributions:\n- Chart: Histogram or box plot\n- Example: A histogram of crash times shows when collisions are most common across a day.\nThese choices matter because each chart highlights a specific relationship between variables. For instance, the line chart of hourly injuries works better than a bar chart because it emphasizes flow and continuity across time. Conversely, comparing borough averages suits a bar chart since categories are discrete.\nWhen in doubt, simplicity and intent guide good design. Avoid flashy visuals or 3D effects that distract from the message. Instead, use consistent colors, clear labels, and honest axes to help the audience see what you want them to see.\n\n\n1.5.6 Reproducibility and Transparency\nReproducibility builds trust in your analysis. Transparent workflows including data sources, software versions, and assumptions, allow others to verify and extend your work. In data science, reproducibility isn’t just about rerunning code; it’s about clear communication of process.\nGood practices for transparency:\n- Combine code and writing in a single Quarto or Jupyter file.\n- Record where and when data was retrieved.\n- Comment on all major data-cleaning steps.\n- Use readable variable names and consistent file structure.\n\n1.5.6.1 Example: Adding Reproducible Metadata\n\n# Data source: NYC Open Data (accessed October 2025)\n# File: ids-f25/data/nyc_crashes_cleaned.feather\n# Environment: Python 3.11 | pandas 2.2 | seaborn 0.13\n\nprint(crash_df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1381 entries, 0 to 1380\nData columns (total 30 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   borough                        1367 non-null   object        \n 1   zip_code                       1365 non-null   object        \n 2   latitude                       1345 non-null   float32       \n 3   longitude                      1345 non-null   float32       \n 4   on_street_name                 953 non-null    object        \n 5   cross_street_name              839 non-null    object        \n 6   off_street_name                428 non-null    object        \n 7   number_of_persons_injured      1381 non-null   int64         \n 8   number_of_persons_killed       1381 non-null   int64         \n 9   number_of_pedestrians_injured  1381 non-null   int64         \n 10  number_of_pedestrians_killed   1381 non-null   int64         \n 11  number_of_cyclist_injured      1381 non-null   int64         \n 12  number_of_cyclist_killed       1381 non-null   int64         \n 13  number_of_motorist_injured     1381 non-null   int64         \n 14  number_of_motorist_killed      1381 non-null   int64         \n 15  contributing_factor_vehicle_1  1372 non-null   object        \n 16  contributing_factor_vehicle_2  1059 non-null   object        \n 17  contributing_factor_vehicle_3  118 non-null    object        \n 18  contributing_factor_vehicle_4  33 non-null     object        \n 19  contributing_factor_vehicle_5  12 non-null     object        \n 20  collision_id                   1381 non-null   int64         \n 21  vehicle_type_code_1            1364 non-null   object        \n 22  vehicle_type_code_2            945 non-null    object        \n 23  vehicle_type_code_3            112 non-null    object        \n 24  vehicle_type_code_4            30 non-null     object        \n 25  vehicle_type_code_5            12 non-null     object        \n 26  was_fillable                   1381 non-null   bool          \n 27  zip_code_numeric               1365 non-null   float64       \n 28  zip_filled                     1381 non-null   bool          \n 29  crash_datetime                 1381 non-null   datetime64[ns]\ndtypes: bool(2), datetime64[ns](1), float32(2), float64(1), int64(9), object(15)\nmemory usage: 304.8+ KB\nNone\n\n\nIncluding the info() output and environment details helps document your data’s structure. Anyone revisiting your project can quickly see how the dataset was formatted and what columns were used. This kind of metadata makes collaboration and replication straightforward.\n\n\n\n1.5.7 Ethical and Responsible Communication\nEthical communication means presenting your data truthfully and clearly. When data visuals or summaries are misleading, even unintentionally, they can distort public understanding. Effective communicators focus on honesty and clarity.\nCommon pitfalls:\n- Cropping or compressing axes to exaggerate trends.\n- Omitting missing data or uncertainty.\n- Implying causation when showing correlation.\nBetter habits:\n- Start axes at zero when showing counts.\n- Provide notes or error ranges when possible.\n- Write captions that clarify context and limitations.\n\n1.5.7.1 Example 1: Cropped Axis\n\n# Example 1: Cropped Axis\nymin = borough_counts[\"crash_count\"].min()\nymax = borough_counts[\"crash_count\"].max()\nsecond_lowest = sorted(borough_counts[\"crash_count\"])[1]\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 7))\n\n# Misleading chart (top)\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\",\n    y=\"crash_count\",\n    ax=axes[0],\n    color=\"steelblue\"\n)\naxes[0].set_ylim(second_lowest * 0.9, ymax * 1.02)\naxes[0].set_title(\"Misleading Chart: Axis Cropped Too High\")\naxes[0].set_xlabel(\"\")\naxes[0].set_ylabel(\"Crash Count\")\naxes[0].tick_params(axis=\"x\", rotation=45)\n\n# Honest chart (bottom)\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\",\n    y=\"crash_count\",\n    ax=axes[1],\n    color=\"steelblue\"\n)\naxes[1].set_ylim(0, ymax * 1.02)\naxes[1].set_title(\"Honest Chart: Axis Starts at Zero\")\naxes[1].set_xlabel(\"Borough\")\naxes[1].set_ylabel(\"Crash Count\")\naxes[1].tick_params(axis=\"x\", rotation=45)\n\nplt.tight_layout(pad=2)\nplt.show()\n\n\n\n\n\n\n\n\nWhen the y-axis is cropped so it starts just below the smaller bars, Brooklyn’s crash count looks dramatically higher than the other boroughs. The honest version, which starts the y-axis at zero, shows that the differences are real but not as extreme. This demonstrates how axis limits can exaggerate scale, even when the underlying data is unchanged.\n\n\n1.5.7.2 Example 2: Distorted Aspect Ratio\n\n# Example 2: Distorted Aspect Ratio\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\nsns.lineplot(x=[1, 2, 3, 4, 5], y=[100, 200, 300, 400, 500],\n             ax=axes[0], marker=\"o\")\n\naxes[0].set_box_aspect(1.8)  # misleading proportions\naxes[0].set_title(\"Misleading: Distorted Aspect Ratio\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Value\")\n\nsns.lineplot(x=[1, 2, 3, 4, 5], y=[100, 200, 300, 400, 500],\n             ax=axes[1], marker=\"o\")\naxes[1].set_box_aspect(1)  # normal proportions\naxes[1].set_title(\"Honest: Equal Scaling\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Value\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nChanging the aspect ratio alters the apparent steepness of trends. The left plot exaggerates change by compressing the x-axis, while the right plot uses consistent scaling to display the real rate of change. Aspect distortion is subtle but powerful, it can make normal variation seem dramatic.\n\n\n1.5.7.3 Example 3: Misleading Color Emphasis\n\n# Example 3: Misleading Color Emphasis\n\n# Sample data \ndata = pd.DataFrame({\n    \"borough\": [\"Brooklyn\", \"Queens\", \"Manhattan\", \"Bronx\"],\n    \"crash_rate\": [8.1, 7.9, 7.7, 7.5]\n})\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 3))\n\n# Misleading chart: overemphasized colors \nsns.barplot(\n    data=data,\n    x=\"borough\",\n    y=\"crash_rate\",\n    hue=\"borough\",\n    palette=[\"darkred\", \"red\", \"salmon\", \"pink\"],\n    legend=False,\n    ax=axes[0]\n)\naxes[0].set_title(\"Misleading: Color Overemphasis\")\n\n# Honest chart: consistent, neutral color\nsns.barplot(\n    data=data,\n    x=\"borough\",\n    y=\"crash_rate\",\n    color=\"steelblue\",\n    ax=axes[1]\n)\naxes[1].set_title(\"Honest: Neutral Colors\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nColor choices can easily mislead. The chart on the left uses intense red tones to suggest large differences between boroughs, even though the variation is small. The right chart uses a neutral palette to draw attention to data values instead of emotional cues.\nEthical visualization respects the audience’s ability to interpret data fairly. This connects to the broader issue of abusing a plot which involves using design choices to distort perception. Such misuse can involve altering aspect ratios, exaggerating colors, omitting context, or adjusting baselines to amplify change. True integrity in visualization means clarity over drama. We should show data as it is, not as we wish it looked.\n\n\n\n1.5.8 Connecting Insights to Action\nGreat communication doesn’t stop at describing what happened, it explains what should happen next. Visuals paired with short, concrete takeaways make insights actionable.\n\n1.5.8.1 Example: Turning Findings into Decisions\n\n# Filter crashes with any injuries (pedestrians, cyclists, or motorists)\ninjury_df = crash_df[\n    (crash_df[\"number_of_pedestrians_injured\"] &gt; 0)\n    | (crash_df[\"number_of_cyclist_injured\"] &gt; 0)\n    | (crash_df[\"number_of_motorist_injured\"] &gt; 0)\n].copy()\n\n# Extract hour of day\ninjury_df[\"hour\"] = injury_df[\"crash_datetime\"].dt.hour\n\n# Create a combined total injury column\ninjury_df[\"total_injuries\"] = (\n    injury_df[\"number_of_pedestrians_injured\"]\n    + injury_df[\"number_of_cyclist_injured\"]\n    + injury_df[\"number_of_motorist_injured\"]\n)\n\n# Group by borough and hour\ninjuries_by_hour = (\n    injury_df.groupby([\"borough\", \"hour\"])[\"total_injuries\"]\n    .sum()\n    .reset_index()\n)\n\n# Plot total injuries by hour for each borough\nsns.lineplot(\n    data=injuries_by_hour,\n    x=\"hour\",\n    y=\"total_injuries\",\n    hue=\"borough\",\n    marker=\"o\"\n)\nplt.title(\"Hourly Injuries (Pedestrians, Cyclists, and Motorists) by Borough\")\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"Total Injuries\")\nplt.xticks(range(0, 24, 2))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows how total traffic-related injuries (pedestrians, cyclists, and motorists combined) change by hour in each borough. Injuries stay relatively low overnight, then rise during the afternoon and peak in the late day and early evening; most noticeably in Brooklyn and Queens, with a smaller but similar bump in the Bronx and Manhattan. Staten Island stays consistently lower overall. These rush-hour spikes line up with high activity on the roads where there are more cars, more people moving, and more chances for conflict. That suggests a clear intervention window.\n\n“Injuries across Brooklyn and Queens jump in the late afternoon and evening, which lines up with commuting traffic. Targeted enforcement, traffic calming, and signal timing changes during these peak hours could help reduce harm.”\n\nHere, the analysis leads directly to a recommendation. That bridge from insight to action turns analysis into impact. Communicating findings this way helps decision-makers use data effectively.\n\n\n\n1.5.9 Recommendations for Effective Communication and Presentation\nGood data communication does not end with a well-designed chart. It extends to how findings are framed, timed, and delivered. Clear, focused communication helps turn technical results into insights that people can actually use.\n\n1.5.9.1 General Recommendations\n\nLead with purpose by explaining why the data matters before discussing details.\n\nShow the story, not the spreadsheet, visuals should clarify, not overwhelm.\n\nKeep design elements consistent so colors, scales, and fonts feel cohesive.\n\nAnticipate how non-technical audiences might interpret results and add context when needed.\n\nConnect each chart or finding to its real-world relevance or next steps.\n\n\n\n1.5.9.2 Being Time-Aware\n\nKnow your total time and plan the pacing of your talk accordingly.\n\nPrioritize the most important findings, it’s better to explain a few points clearly than to rush through many.\n\nPractice transitions between sections so the flow feels natural and on schedule.\n\nKeep visuals simple so they can be understood quickly without overexplaining.\n\nLeave a few minutes for questions or discussion at the end if possible.\n\nIf you run short on time, skip details that don’t change the main takeaway.\n\n\n\n1.5.9.3 Giving a Strong Presentation\n\nStart with a clear message so your audience knows what to expect.\n\nGuide the audience through each visual: what to notice and why it matters.\n\nKeep slides uncluttered, focusing on one key idea at a time.\n\nSpeak at a steady pace, pausing briefly after key visuals to let points sink in.\n\nAvoid jargon and tailor explanations to your audience’s background.\n\nEnd with a short summary that reinforces the main insight and next step.\n\nDelivering data effectively means combining clarity, timing, and empathy for your audience. When visuals, pacing, and delivery align, data becomes insight rather than just information.\n\n\n\n1.5.10 Conclusion\nEffective data science communication blends clarity, accuracy, reproducibility, and ethics. When practiced together, these elements transform complex analyses into stories people can understand and act on.\nThe NYC crash dataset shows how simple design, transparent documentation, and ethical framing make results meaningful far beyond the numbers.\n\n\n1.5.11 Further Reading\neazyBI Blog. (2025). Data Visualization: How to Pick the Right Chart Type.\nhttps://eazybi.com/blog/data-visualization-how-to-pick-the-right-chart-type\nFranconeri, S. L., Padilla, L. M. K., Shah, P., Zacks, J. M., & Hullman, J. (2021).\nThe science of visual data communication: What works. Psychological Science in\nthe Public Interest, 22(3), 110-161.\nhttps://faculty.sites.iastate.edu/tesfatsi/archive/tesfatsi/ScienceOfVisualDataCommunication.FranconeriEtAl2021.pdf\nOfori, E., et al. (2025). Visual communication of public health data:\nA scoping review. Public Health Reviews.\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC12060258/\nPragmatic Editorial Team. (2025). Communication Skills for Data Science.\nhttps://www.pragmaticinstitute.com/resources/articles/data/communication-skills-for-data-science/\n\n\n\n\nAmerican Statistical Association (ASA). (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-git.html",
    "href": "02-git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#set-up-gitgithub",
    "href": "02-git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#most-frequently-used-git-commands",
    "href": "02-git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#tips-on-using-git",
    "href": "02-git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#pull-request",
    "href": "02-git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "03-quarto.html",
    "href": "03-quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#introduction-to-quarto",
    "href": "03-quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#sec-buildnotes",
    "href": "03-quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#the-data-science-life-cycle",
    "href": "03-quarto.html#the-data-science-life-cycle",
    "title": "3  Reproducible Data Science",
    "section": "3.3 The Data Science Life Cycle",
    "text": "3.3 The Data Science Life Cycle\nThis section summarizes Chapter 2 of Veridical Data Science (Yu & Barter, 2024), which introduces the data science life cycle (DSLC). The DSLC provides a structured way to think about the progression of data science projects. It consists of six stages, each with a distinct purpose:\n\nStage 1: Problem formulation and data collection\nCollaborate with domain experts to refine vague questions into ones that can realistically be answered with data. Identify what data already exists or design new collection protocols. Understanding the collection process is crucial for assessing how data relates to reality.\nStage 2: Data cleaning, preprocessing, and exploratory data analysis\nClean data to make it tidy, unambiguous, and correctly formatted. Preprocess it to meet the requirements of specific algorithms, such as handling missing values or scaling variables. Exploratory data analysis (EDA) summarizes patterns using tables, statistics, and plots, while explanatory data analysis polishes visuals for communication.\nStage 3: Exploring intrinsic data structures (optional)\nTechniques such as dimensionality reduction simplify data into lower-dimensional forms, while clustering identifies natural groupings among observations. Even if not central to the project, these methods often enhance understanding.\nStage 4: Predictive and/or inferential analysis (optional)\nMany projects are cast as prediction tasks, training algorithms like regression or random forests to forecast outcomes. Inference focuses on estimating population parameters and quantifying uncertainty. This book emphasizes prediction while acknowledging inference as important in many domains.\nStage 5: Evaluation of results\nFindings should be evaluated both qualitatively, through critical thinking, and quantitatively, through the PCS framework. PCS stands for predictability, computability, and stability:\n\nPredictability asks whether findings hold up in relevant future data.\n\nComputability asks whether methods are feasible with available computational resources.\n\nStability asks whether conclusions remain consistent under reasonable changes in data, methods, or judgment calls.\nTogether, PCS provides a foundation for assessing the reliability of data-driven results.\n\nStage 6: Communication of results\nResults must be conveyed clearly to intended audiences, whether through reports, presentations, visualizations, or deployable tools. Communication should be tailored so findings can inform real-world decisions.\n\nThe DSLC is not a linear pipeline—analysts often loop back to refine earlier steps. The chapter also cautions against data snooping, where patterns discovered during exploration are mistaken for reliable truths. Applying PCS ensures that results are not only technically sound but also trustworthy and interpretable across the life cycle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#a-primer-of-markdown",
    "href": "03-quarto.html#a-primer-of-markdown",
    "title": "3  Reproducible Data Science",
    "section": "3.4 A Primer of Markdown",
    "text": "3.4 A Primer of Markdown\nThis section was prepared by Jingang Chen, an undergraduate junior pursuing a dual degree in computer science and statistical data science.\n\n3.4.1 Introduction\nThis section will focus on the syntax of Markdown, which is a lightweight markup language that allows to user to write content in plain text format, which can be rendered to various formats like HTML and PDF, and is widely used in open-source documentation.\n\n\n3.4.2 Headers\nIn markdown, creating heading levels for sections and subsections are denoted by # (atx-style) in the beginning of the line. The number of hashtags denote the heading level, with more hashtags indicating smaller heading levels. There are a total of 6 heading levels using the hashtags.\n# Header 1 (Main)\n## Header 2 (Subheading)\n### Header 3 (Subheading)\n#### Header 4\n##### Header 5\n###### Header 6\n\nA space is required after the hashtags to denote that it is a heading\n\nHeadings can also be denoted by using underlined = and - signs, though this will only work for the first and second level headers respectively.\nHeader 1\n===\nHeader 2\n----\nAny amount of = and - signs will work to create those top two headings.\n\n\n3.4.3 Paragraph and Line Break Convention\nTo seperate text into paragraphs, make sure there is at least one blank space between the blocks of texts.\nThis is the first pargraph which can contain multiple lines, and as long there\nis no blank line in this, then this block of text is one paragraph.\n\nThis is an example of a second paragraph. This one is seperated by a blank line\nfrom the paragraph above to denote a different paragraph.\nOutput:\nThis is the first pargraph which can contain multiple lines, and as long there is no blank line in this, then this block of text is one paragraph.\nThis is an example of a second paragraph. This one is seperated by a blank line from the paragraph above to denote a different paragraph.\nTo create a line break within a paragraph, end whatever line you’re on with two spaces, then press enter/return to start a new line. The break tag &lt;br&gt; is also sufficient.\nThis is one line.  \nThis is a line that is seperated from the line above using two spaces. &lt;br&gt;\nThis is a third line seperated using the `&lt;br&gt;` break tag.\nOutput:\nThis is one line.\nThis is a line that is seperated from the line above using two spaces.  This is a third line seperated using the &lt;br&gt; break tag.\n\n&lt;&gt; in Markdown is a HTML tag, which is another way to structure the document\n\n\n\n3.4.4 Horizontal Rules\nHorizontal rules visually separate document sections, which can be done in Markdown by adding 3 or more of one of 3 characters: ***, ---, or ___.\n\nparagraph 1.\n\n******\n\nparagraph 2.\n\n------\n\nparagraph 3.\n\n______\n\nparagraph 4.\nOutput:\nparagraph 1.\n\nparagraph 2.\n\nparagraph 3.\n\nparagraph 4.\n\nMake sure when using the horizontal rulers, each of them is one blank line above and below them. There shouldn’t be any text above or below them adjacently.\n\n\n\n3.4.5 Text Formatting\n\n3.4.5.1 Bolding and Italics\nItalic text, uses single asterisks (*) or underscores (_) around the text, while bold text uses double asterisks or underscores. To make the text both bolded and italicized, put three asterisks or underscores around the text.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\n*Italicized*  _Italicized_\nItalicized\n\n\n**Bolded**  __Bolded__\nBolded\n\n\n***bold & italics***  ___bold & italics___\nbolded & italics\n\n\n\nEmphasis can also be placed within a word as well, but only * can be used, not _.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\ns*uperfragalis*t ex**pialidociou**s\nsuperfragalist expialidocious\n\n\n\n\n\n3.4.5.2 Strikethrough\nStrikethrough uses ~~ around the text to cross it out.\n\n\n\nSyntax\nOutput\n\n\n\n\n~~strikethrough text~~\nstrikethrough text\n\n\n\n\n\n3.4.5.3 Superscript and Subscript\nTo superscript, use ^ around the desired text.\nFor subscript, use ~ around the desired text.\n\n\n\nSyntax\nOutput\n\n\n\n\nsuperscript^2^\nsuperscript2\n\n\nsubscript~2~\nsubscript2\n\n\n\n\n\n3.4.5.4 Underlining and Highlighting\nTo underline, bracket the desired text with [], and then follow that using {.underline}.\nTo highlight, bracket the desired text with [], and follow that using {.mark}. Alternatively you can start the text with &lt;mark&gt; and end it with &lt;/mark&gt;.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\n[underlined text]{.underline}\nunderlined text\n\n\n[highlighted]{.mark}  &lt;mark&gt;higlighted&lt;/mark&gt;\nhighlighted\n\n\n\n\n\n3.4.5.5 Escape Characters\nIf you want to display the Markdown syntax characters, it can be done by putting \\ before and after the text.\n\\# Not a heading\\\n\\**not bolded**\\\n\\[Not a link]\\\nOutput:\n# Not a heading\n**not bolded**\n[Not a link]\n\n\n\n\n3.4.6 Blockquotes\nBlockquotes is a way to highlight quoted content or important information in the document, which is denoted by &gt; in the beginning of the line.\n&gt; This is a block quote\n&gt; \n&gt; second block quote\n&gt;\n&gt; third block quote\nOutput:\n\nThis is a blockquote\nsecond blockquote\nthird blockquote\n\nTo make sure that the blockquotes are seperated, make sure that each quote is seperated by &gt;’s with no text in that line. Putting blockquotes adjacent to each other will result in all the text being in the same paragraph.\n&gt; These blockquotes\n&gt; are not seperated and\n&gt; are all in one line\nOutput:\n\nThese blockquotes are not seperated and are all in one line\n\nBlockquotes can also be nested by using multiple &gt; in one line.\n&gt; This is a blockquote\n&gt;\n&gt; &gt; This is a nested blockquote\n&gt; &gt;\n&gt; &gt; &gt; Third level blockquote\nOutput:\n\nThis is a blockquote\n\nThis is a nested blockquote\n\nThird level blockquote\n\n\n\n\n\n3.4.7 Lists\nLists and nested lists can be structured in Markdown either unordered or ordered. For nested lists, make sure to include 4 spaces to properly indent the nested list (applies to both ordered and unordered).\n\n3.4.7.1 Unordered Lists\nFor unordered lists, *, +, or - can be used to make a list.\n* Item 1\n    * Subitem\n        * Another subitem\n* Item 2\n* Item 3\nand\n+ Item 1\n   + Subitem\n       + Another subitem\n+ Item 2\n+ Item 3\n- Item 1\n   - Subitem\n       - Another subitem\n- Item 2\n- Item 3\nas well a mix of all three:\n* Item 1\n   + Subitem\n       - Another subitem\n* Item 2\n* Item 3\nall yield the same output:\n\nItem 1\n\nSubitem\n\nAnother subitem\n\n\nItem 2\nItem 3\n\n\n\n3.4.7.2 Ordered Lists\nOrdered lists use numbers with periods.\n1. Item 1\n2. Item 2\n    1. Sub item\n3. Item 3\nThe numbers don’t necessarily have to be ordered, and they can be duplicated as well. Whatever number the list starts on will be the one that it will count from no matter the numbers that come after it.\n1. Item 1\n1. Item 2\n    1. Sub Item\n1. Item 3\n1. Item 1\n4. Item 2\n    2. Sub Item\n7. Item 3\nAll three of these lists will yield the same result:\n\nItem 1\nItem 2\n\nSub Item\n\nItem 3\n\nHowever, if the list were to start on 2, it would start counting from 2 no matter the order that follows.\n2. Item 1\n2. Item 2\n5. Item 3\nOutput:\n\nItem 1\nItem 2\nItem 3\n\n\n\n3.4.7.3 Task List\nTo denote the lists as a series of tasks, use - [ ], which is unchecked, and [x], which is checked, at the beginning of the line.\n- [ ] Task 1\n- [x] Task 2\nOutput:\n\nTask 1\nTask 2\n\n\n\n3.4.7.4 Definition lists\nDefintion lists can be created with the following convention:\nterm\n: defintion\n\nterm2\n: definition2\nOutput:\n\nterm\n\ndefintion\n\nterm2\n\ndefinition2\n\n\n\n\n3.4.7.5 Some Additional Notes About Lists\nThere are some additional features that can be done with the lists mentioned.\nA list can continue after a break in between. For ordered lists, the numbering still follows through after an interruption.\n1. Item 1\n\ninterruption text\n\n2. Item 2\nOutput:\n\nItem 1\n\ninterruption text\n\nItem 2\n\nText than isn’t numbered or in bullet points can also be added below list using four spaces for indenting. Code chunks can be added in this case as well.\n1. ordered list\n2. item 2\n    continued after indenting 4 spaces\n    ```python\n    print(\"Hello, World!\")\n    ```\n    A. sub-sub-item 1\nOutput:\n\nordered list\nitem 2\ncontinued after indenting 4 spaces\nprint(\"Hello, World!\")\n\nsub-sub-item 1\n\n\n\n\n\n3.4.8 Code\n\n3.4.8.1 Inline Code\nMarkdown allows for inline code and code blocks.\nTo insert inline code, use the backticks ` around the text\nThis is an example of `inline code` in a text.\nOutput: This is an example of inline code in a text.\nTo display the ` as part of an inline code, surround the character with `` backticks and spacing them apart from `.\n\n\n3.4.8.2 Code Blocks\nTo create code blocks, ``` can be used.\n    ```\n    some code\n    ```\nOutput:\nsome code\nAlternatively, code blocks can be indented by identing four or more spaces prior to the code.\nThis is code using the indentation of four spaces\n    This line has more than four spaces\nA language can also be added to specify the language of the code blocks if ``` is used.\n```python\nprint('some python code')\n```\nOutput:\nprint('some python code')\nTo make the code executable, put {} around the syntax language being used in the code blocks.\n    ```{python}\n    print(\"Some python code\")\n    ```\nOutput:\n\nprint('some python code')\n\nsome python code\n\n\n\n\n\n3.4.9 Formulas and Equations\nMarkdown supports LaTeX-style expressions. Mathematical expressions can either be done inline (enclosing using $) or for display math (enclosed by $$).\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\nInline: $x^2 + y^2 = z^2$\nInline: \\(x^2 + y^2 = z^2\\)\n\n\nDisplay:  $$x^2 + y^2 = z^2$$\nDisplay:  \\[x^2 + y^2 = z^2 \\tag{3.1}\\]\n\n\n\nFor mor information on how to use LaTeX expressions, visit https://www.overleaf.com/learn and look under the “Mathematics” section.\n\n\n3.4.10 Link Embedding\nLinks in Markdown can either be added inline or as a reference.\nTo add an inline link, use [] around the text that leads to the link, followed by actually inputting the link in ().\nthis is an example link that will lead to the \n[Markdown Guide](https://www.markdownguide.org/)\nOutput: : This is an example link that will lead to the Markdown Guide.\nAlternatively, for inline links, the URL can be directly added without linking it to text by just using &lt;&gt; around the URL.\nLink to &lt;https://www.markdownguide.org/&gt;.\nOutput: Link to https://www.markdownguide.org/.\nFor reference-style links, you can having text linked to the URL and a seperate number or text that points to the link with []\nExample of reference-style link leading to the [Markdown Guide][link].\n\n[link]: https://www.markdownguide.org\nOutput:\nExample of reference-style link leading to the Markdown Guide.\n\n\n3.4.11 Images\nTo insert an image into markdown, the convention is to first add !, followed by a caption to the image wrapped in [], and finally the file path or URL to the image enclosed with ().\nIn addition, the image can be embedded with a link by first wrapping the 3 parameters mentioned above in [], followed by the link wrapped in (). The link can either be a local file path stored on your computer or a direct link of the image found online. For reproducibility, this example uses a direct URL pointing to an online image.\n[![UConn Husky Logo](https://lofrev.net/wp-content/photos/2016/06/uconn_huskies_logo.jpg)](https://uconn.edu/)\nOutput:\n\n\n\n\n\n\nFigure 3.1: UConn Husky Logo\n\n\n\n\n\n3.4.12 Tables\nTo create tables, the | character is used to seperate the table into columns, while the - is used to seperate the headers of the table from the rest of the data. After the table, a header can be included two lines below the table starting with :.\n| Col 1 | Col 2 | Col 3 | Col 4 |\n|------|-----|----------|-------|\n|   a  |  b  |    c     |  d    |\n|  e   |  f  |  g       |  h    |\n|   i  |   j |    k     |    l  |\n\n: Sample Table 1\nOutput:\n\n\n\nTable 3.1: Sample Table 1\n\n\n\n\n\nCol 1\nCol 2\nCol 3\nCol 4\n\n\n\n\na\nb\nc\nd\n\n\ne\nf\ng\nh\n\n\ni\nj\nk\nl\n\n\n\n\n\n\nColumns can also be aligned to the left, right or center by add a colong : to the left, right or on both sides of the -’s of the table.\n| Right Col | Left Col | Center | Default |\n|----------:|:---------|:------:|---------|\n|        a  |       b  |    c   |    d    |\n|  e        |       f  |   g    |    h    |\n|   i       |        j |  k     |      l  |\n\n: Sample Table 2\nOutput:\n\nSample Table 2\n\n\nRight Col\nLeft Col\nCenter\nDefault\n\n\n\n\na\nb\nc\nd\n\n\ne\nf\ng\nh\n\n\ni\nj\nk\nl\n\n\n\n\n\n3.4.13 Cross Referencing\nPrior to cross referencing a section, there must be a label attached to the section that is being referenced. This is done by using {} after the section and giving it a label inside of it starting with #sec- followed by anything else after it.\n### Tables {#sec-tables}\nThen, to reference the section, use the @ followed by the label specified to create a direct link to the section. Optionally, you can wrap it around [] and add any additional text to the link.\nRefer back to the Tables section in [section @sec-tables].\nOutput:\nRefer back to the Tables section in section 3.4.12.\nTo refer to figures and images, make sure to input the label after the URL. Make sure in the label to start with #fig- to specify that it is a figure that is being referenced.\n[![UConn Husky Logo](https://lofrev.net/wp-content/photos/2016/06/uconn_huskies_logo.jpg){#fig-sample}](https://uconn.edu/)\nNow it can be referred back with the same convention as referencing a section:\nThis refers @fig-sample in the Images section.\nOutput:\nThis refers Figure 3.1 in the Images section.\nTo cross reference tables, include the label on the line where the header is specified. Make sure to start the label with #tbl- to specify that it’s a table that is being referenced.\n| Col 1 | Col 2 | Col 3 | Col 4 |\n|------|-----|----------|-------|\n|   a  |  b  |    c     |  d    |\n|  e   |  f  |  g       |  h    |\n|   i  |   j |    k     |    l  |\n\n: Sample Table 1 {#tbl-sample1}\nNow to reference the table back:\n@tbl-sample1 refers back to the first table in the Tables section.\nOutput:\nTable 3.1 refers back to the first table in the Tables section.\nFinally, to reference an equation, make sure to include the label after the equation, outside of the $$ starting with #eq-.\n$$x^2 + y^2 = z^2$$ {#eq-sample}\nNow to reference the equation from the Equations section back:\n@eq-sample is the Pythagorean theorem referenced from the Equations section.\nOutput:\nEquation 3.1 is the Pythagorean theorem referenced from the Equations section.\n\n\n3.4.14 Footnotes\nFootnotes, often denoted by superscripts, are placed at the bottom of a page in a document, which helps provide additional information and references related to a specific part of the text. To insert a footnote in a text, it is done in the [], where the first character inside of the brackets is ^, followed by the desired name of the footnote. After that is specified, reference that footnote in a newline and put whatever note that is needed.\nThis is where you can place a footnote,[^1] sometimes multiple can be placed in \none sentence.[^longnote]\n\n[^1]: This is a footnote.\n\n[^longnote]: This is a long footnote, which can have paragraphs.\n   \n    Make sure to use the four spaces to inent so that the subsequent paragraphs \n    belong to the same footnote.\n\n    ```\n    {code can also be inserted in here}\n    ```\n\n    End footnote\n\nSeperate paragraph here to show that this isn't part of the footnote.\nOutput:\nThis is where you can place a footnote,1 sometimes multiple can be placed in one sentence2\nSeperate paragraph here to show that this isn’t part of the footnote.\n\n\n3.4.15 Conclusion\nMarkdown is a versatile markup language that simplifies writing for the web in a way that is readable and convenient. Today, it is widely used to present the work that is being done in a variety of areas, and Markdown provides a clean way to organize the content being presented and structure the documents well.\n\n\n3.4.16 Further Reading\n\nQuarto Markdown Basics\nThe Complete Guide to Markdown\nMarkdown: Synatx by John Gruber",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#presentations-with-quarto",
    "href": "03-quarto.html#presentations-with-quarto",
    "title": "3  Reproducible Data Science",
    "section": "3.5 Presentations with Quarto",
    "text": "3.5 Presentations with Quarto\nThis section is written by Irene Chen, a Senior majoring in Statistical Data Science at the University of Connecticut.\n\n3.5.1 Introduction\nQuarto allows you to create dynamic, reproducible, and visually appealing presentations from a single source file. You can generate interactive slides with code, visuals, and math without having to switch between multiple tools. It works smoothly with Git for collaboration and can update automatically as your data changes. Quarto support multiple programming languages, and allows multiple output formats.\n\n\n3.5.2 Presentation Formatting\nQuarto supports several formats for creating presentations including:\n\nrevealjs - reveal.js (HTML)\nbeamer - Beamer (LaTex/PDF)\npptx - PowerPoint (Microsoft Office)\n\nSlides will render as HTML by default, and it is preferred over beamer and pptx unless you are specifically looking for a LaTeX or Office output.\n\n\n\n\nFeature\n\n\nrevealjs\n\n\nbeamer\n\n\npptx\n\n\n\n\n\n\nOutput Format\n\n\nHTML or PDF\n\n\nPDF\n\n\nPowerPoint\n\n\n\n\n\nPros\n\n\n• Runs in any browser • Shares easily with a link • Allows for CSS styling • Allows interactive elements\n\n\n• Excellent math rendering • Ensures consistent fonts and equations • Produces high-quality slides\n\n\n• Fully editable within pptx • Familiar for non-technical users • Easy template customization\n\n\n\n\n\nCons\n\n\n• Requires a browser to present • Limited offline use\n\n\n• Needs a LaTeX distribution • Rigid layout\n\n\n• Lacks Quarto’s interactivity • Manual re-rendering • Requires MS Office\n\n\n\n\n\n\n3.5.3 Creating a Presentation File\nIn your new .qmd file, include a YAML header at the top to tell Quarto to render slides instead of a document.\n---\ntitle: \"Presentations with Quarto\"\nformat: revealjs \n---\n\n3.5.3.1 YAML Header Example\n---\ntitle: \"Presentations with Quarto\"\nauthor: \"Irene Chen\"\nformat:\n  revealjs: \n    embed-resources: true\n    multiplex: true\n    preview-links: true\n    theme: [simple, custom.css]\n    transition: concave\n    footer: \"STAT 3255/5255, Fall 2025\"\n\n---\n\n\n\n3.5.4 YAML Customizations for revealjs\n\n3.5.4.1 Functional YAML Customizations\n\nembed-resources: true - Creates self-contained file HTML files that bundles images, fonts, and CSS for easy sharing.\nmultiplex: true - Allows live audience synchronization.\npreview-links: true - Opens link previews directly within the slide.\ntoc: true - Generates a table of contents slide automatically.\ncenter: false - Aligns content at the top rather than centering on the slides.\n\n\n\n3.5.4.2 Stylistic YAML Customizations\n\ntheme: [slide theme] - Configures the theme as one of the 12 included with Reveal (or one of your own as a .css).\n\ndefault, white, league, night, beige, simple, serif, solarized, moon, dracula, sky, blood\nNote: default is automatically used, so this header is only used for alternative themes.\n\ntransition: [transition] - Adds transitions in between the slides.\n\nnone, slide, fade, convex, concave, zoom\n\nfooter: \"Footer Note\" - Adds a footer or tagline to bottom of each slide.\nslide-number: true - Displays the slide number at bottom of each slide.\nincrimental: true - Reveals bullet points one by one.\n\n\n\n3.5.4.3 CSS files\nA CSS file (short for Cascading Style Sheets) is a styling file used to control how your content looks (colors, fonts, layout, spacing, etc…). In the same folder as your .qmd, make a file called custom.css (or custom.scss). An example could look like this:\n:root{ \n    --r-background-color: #ffffff;\n    --r-main-color: #ffffff;\n}\n\n.reveal p, .reveal li {\n    font-family: \"Open Sans\", sans-serif;\n    font-size 1.1m;\n}\n\n.reveal code {\n    background: #ffffff;\n    color: #ffffff;\n    padding: 2px 4px;\n    border-radius: 4px;\n}\nAdd the file name to the YAML heading like theme: [custom.css]. You can also make a cascading theme like theme: [simple, custom.css], which starts with the simple theme and applies the custom design rules on top.\nNote: Order matters because style sheets cascade. A sheet with multiple rules for the same element creates a hierarchy of specificity.\n\n\n\n3.5.5 Creating a New Slide\n\nEach level 1 header (#) creates a new title slide section.\nEach level 2 header (##) creates a sub-slide that will appear when you press the down arrow.\nAdding (###) will create a subheading in the slide, not a new slide.\nHorizontal rules (---) are used to create a slide when you don’t want to add a heading or title.\n\n\n\n3.5.6 Code Blocks\nIf you want Quarto to run the Python code and show the output, add curly brackets:\n```{python}\nprint(\"Hello World!\")\n```\n\nprint(\"Hello World!\")\n\nHello World!\n\n\nIf you want Quarto to show the code on the slide as text, but not execute it, remove the curly brackets. Quarto will treat this as plain Markdown code and not recognize it as executable. This is perfect for simple code display to show examples or syntax:\n```python\nprint(\"Hello World!\")\n```\nprint(\"Hello World!\")\nSimilarly, Quarto will show the code on the slide without running it, but Quarto will treat it as a code cell that is part of your document even though the code isn’t evaluated. This is perfect for code display that stays consistently styled with other executable blocks.\n```{.python}\nprint(\"Hello World!\")\n```\nprint(\"Hello World!\")\nIf you want Quarto to display the raw code block, add two lines of markdown. This is perfect for tutorials.\n```` markdown\n```` markdown\n```python\nprint(\"Hello World!\")\n```\n```` markdown\n```python\nprint(\"Hello World!\")\n```\n\n\n3.5.7 Code Highlighting\nTo highlight specific lines of code output, you can use code-line-numbers.\nFor example:\n```{.python code-line-numbers=\"2-3\"}\nimport matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [2, 4, 6, 8]\nplt.plot(x, y)\nplt.show()\n```\nYou can also highlight disparate ranges of lines by using , to separate rather than -.\nFor example,\n```{.python code-line-numbers=\"2, 4\"}\nimport matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [2, 4, 6, 8]\nplt.plot(x, y)\nplt.show()\n```\nFinally, you can highlight different line ranges. For example, here we show all the lines highlighted, then only line 1, and finally only line 4 is highlighted:\n```{.python code-line-numbers=\"|1|4\"}\nimport matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [2, 4, 6, 8]\nplt.plot(x, y)\nplt.show()\n```\n\n\n3.5.8 Code Echo\nBy default, executable code blocks within Quarto presentations do not echo their source code. To override this, add #| echo: true inside the code fence at the top.\nEx: echo: false\n```{.python}\n#| echo: false\nprint(\"Hello World!\")\n```\n\n\nHello World!\n\n\nEx: echo: true\n```{.python}\n#| echo: true\nprint(\"Hello World!\")\n```\n\nprint(\"Hello World!\")\n\nHello World!\n\n\n\n\n3.5.9 Output Location\nBy default, outputs from code blocks are displayed immediately after the code. To change this, you can use #| output-location: [output location]. The location options include:\n\ncolumn : Displays the output in a column next to the code\nslide : Displays the output on the following slide\nfragment : Displays the output as a Fragment (delays displaying the output until you advance the slide)\ncolumn-fragment : Displays the output in a column next to the code as a Fragment\n\nNote: output-location only works for code cells that echo (#| echo: true).\n\n\n3.5.10 Images\n![Quarto Logo](https://quarto.org/docs/get-started/hello/images/quarto.png)\n{width=300px}\n\nThe caption for the figure is enclosed in [].\nThe file path for the saved image file is enclosed in ().\nAny optional stylistic attributes like size, alignment, and class are enclosed in {}.\n\nNote: Remember to use relative references when specifying the file path to ensure seamless reproducibility, not absolute references.\n\n\n3.5.11 Hyperlinks\n[Quarto](https://quarto.org)\n\nThe text/caption for the link is enclosed in [].\nThe URL link is enclosed in ().\n\n\n\n3.5.12 Math\nYou can use \\( ... \\) for inline math, which appears in the line of text. For example,\nThe Pythagorean theorem equation is \\(a^2 + b^2 = c^2 \\).\nThe Pythagorean theorem equation is (a^2 + b^2 = c^2 ).\n\nWith LaTeX, you can use $ ... $ to show in-text math. For example,\nThe Pythagorean theorem equation is $a^2 + b^2 = c^2$.\nThe Pythagorean theorem equation is \\(a^2 + b^2 = c^2\\).\n\nWith LaTeX, you can also use $$ ... $$ to display math, which centers the equation in its own line. For example,\nThe quadratic formula is given by\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\n$$\nThe quadratic formula is given by \\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\n\\]\n\n\n3.5.13 Div Containers\nDiv containers are special blocks that allow you to add attributes or structure to specific parts of your presentation. They start and end with triple colons (:::) and include classes like .incremental or attributes like width=\"50%\" .\n\n\n3.5.14 Incremental Lists\nBulleted lists by default are displayed all at once. You can change this globally by using incremental: true in the YAML header, or explicitly change it by slide using {.incremental}.\nTo make a list incremental:\n::: {.incremental}\n\n- Bullet 1\n- Bullet 2\n- Bullet 3\n\n:::\nSimilarly, to make a list non-incremental:\n::: {.nonincremental}\n\n- Bullet 1\n- Bullet 2\n- Bullet 3\n\n:::\n\n\n3.5.15 Columns\nUse {.columns} to create columns in your slides. Each .column can be given a specific width percentage to control how much space is taken up.\n::::{.columns}\n\n:::{.column width=\"30%\"}\nLeft Column\n:::\n\n:::{.column wideth=\"70%\"}\nRight Column\n:::\n\n::::\n\n\nLeft Column\n\nItem 1\nItem 2\n\n\nRight Column\n\nItem 1\nItem 2\n\n\n\n\n\n3.5.16 Overflow\nSome slides may contain more content than can be displayed in a single slide. You can fix this with {.smaller}, which will use a smaller typeface so that more text fits on the slide. You can also use {.scrollable}, to make any content that ends up off the slide available when you scroll.\nBoth of these overlay classes can also be applied globally in the YAML header:\n---\nformat: \n    revealjs:\n        smaller: true\n        scrollable: true\n---\n\n\n3.5.17 Speaker Notes\nAdd notes using a div with class {.notes} that will only be visible to the presenter, not the audience. Press S during the presentation to open Speaker View (which displays your notes, the current slide, and the next slide). You can also see a timer and slide navigation tools.\n::: {.notes}\nSpeaker Notes 1\n:::\n\nSpeaker Notes 1\n\n\n\n3.5.18 Multiplexing\nAdding multiplex: true in your YAML enables a live audience mode:\n\npresentations.html - Audience slides that you publish.\npresentations-speaker.html - Presenter’s version that is used to control the slides.\n\n\n\n3.5.19 Chalkboard\nAnnotate directly on your slides while presenting by enabling the option under your YAML settings with chalkboard:true. Some shortcuts include:\n\nB : Toggles the chalkboard on and off.\nC : Toggles the notes canvas on and off.\nDEL/ BACKSPACE : Clears the board.\nX : Cycles the colors forward.\nY : Cycles the colors backward.\nD : Downloads the drawings.\n\n\n\n3.5.20 Rendering\nRender the Quarto presentation by entering the following code in your terminal and it will reveal presentation.html which can be opened in a browser.\nquarto render presentation.qmd\nTemporarily override the YAML format to test how it looks in other formats. For example:\nquarto render presentation.qmd --to beamer\n\n\n3.5.21 Additional Reading\n\nQuarto Presentations\nQuarto Revealjs\nQuarto Beamer\nQuarto PowerPoint\n\n\n\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#footnotes",
    "href": "03-quarto.html#footnotes",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "This is a footnote.↩︎\nThis is a long footnote, which can have paragraphs.\nMake sure to use the four spaces to inent so that the subsequent paragraphs belong to the same footnote.\n{code can also be inserted in here}\nEnd footnote↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "04-python.html",
    "href": "04-python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#the-python-world",
    "href": "04-python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#standard-library",
    "href": "04-python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#important-libraries",
    "href": "04-python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ 1.40945498, -0.85583838,  3.45660625,  4.70745299,  3.28986492,\n       -2.25182801,  3.61472452, -3.21260146,  0.72966336, -1.92119533])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.09865453, 0.07729685, 0.09333724, 0.07931684, 0.09468259,\n       0.05668919, 0.09193145, 0.04266684, 0.09483064, 0.0616843 ])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#writing-a-function",
    "href": "04-python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n9.68 μs ± 1.66 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.85 μs ± 59.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n62 μs ± 823 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(n_doors, n_trials):\n    doors = np.arange(1, n_doors + 1)\n    prize = np.random.choice(doors, size=n_trials)\n    player = np.random.choice(doors, size=n_trials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(n_trials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(n_trials)])\n    return {'noswitch': np.sum(prize == player),\n               'switch': np.sum(prize == player2)}\n\nTest it out with 3 doors.\n\nmontyhall(3, 1000)\n\n{'noswitch': np.int64(307), 'switch': np.int64(693)}\n\n\nThen with 4 doors\n\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(259), 'switch': np.int64(350)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.\n\n4.4.1.1 Faster version\nThis one avoid loops.\n\nimport numpy as np\nfrom typing import Dict, Optional\n\ndef montyhall_fast(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Run a Monty Hall simulation with `n_doors` doors and `n_trials` repetitions.\n    The host always opens exactly one empty door that is neither the prize door\n    nor the player's initial choice.\n\n    Parameters\n    ----------\n    n_doors : int\n        Total number of doors in the game (must be &gt;= 3).\n    n_trials : int\n        Number of independent trials to simulate.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    Dict[str, int]\n        A dictionary with counts of wins under two strategies:\n        - 'noswitch': staying with the initial choice\n        - 'switch'  : switching after the host reveals one empty door\n\n    Examples\n    --------\n    &gt;&gt;&gt; results = montyhall_fast(3, 100000, seed=42)\n    &gt;&gt;&gt; results\n    {'noswitch': 33302, 'switch': 66698}\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initial random assignments\n    prize = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n    player = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n\n    host = np.empty(n_trials, dtype=np.int32)\n\n    # Case 1: player == prize → host excludes only 'player'\n    same = prize == player\n    if np.any(same):\n        k = rng.integers(n_doors - 1, size=np.sum(same), dtype=np.int32)\n        p = player[same]\n        host[same] = k + (k &gt;= p)\n\n    # Case 2: player != prize → host excludes 'player' and 'prize'\n    diff = ~same\n    if np.any(diff):\n        a = np.minimum(player[diff], prize[diff])\n        b = np.maximum(player[diff], prize[diff])\n        k = rng.integers(n_doors - 2, size=np.sum(diff), dtype=np.int32)\n        host[diff] = k + (k &gt;= a) + (k &gt;= b)\n\n    # Player switches: exclude 'player' and 'host'\n    a2 = np.minimum(player, host)\n    b2 = np.maximum(player, host)\n    k2 = rng.integers(n_doors - 2, size=n_trials, dtype=np.int32)\n    player2 = k2 + (k2 &gt;= a2) + (k2 &gt;= b2)\n\n    return {\n        \"noswitch\": int((prize == player).sum()),\n        \"switch\": int((prize == player2).sum()),\n    }\n\nAnother faster version uses Numba just-in-time (JIT) compiler Python, which translates a subset of Python and Numpy into fast machine code via LLVM at runtime. A decorator is added to the numeric functions and, after a one-time compile on the first call (“warm-up”), later calls run much faster. One can toggle parallel=True for multi-core speedups.\n\nfrom typing import Dict, Optional\nimport numpy as np\nfrom numba import njit, prange\n\n# --- internal helpers (compiled) ---\n\n@njit\ndef _seed_numba(seed: int) -&gt; None:\n    # Seed the RNG used inside Numba-compiled code\n    np.random.seed(seed)\n\n@njit\ndef _trial_once(n_doors: int) -&gt; (int, int):\n    \"\"\"\n    Run one Monty Hall trial (host opens exactly one empty door).\n    Returns (noswitch_win, switch_win) as 0/1 ints.\n    \"\"\"\n    prize  = np.random.randint(0, n_doors)\n    player = np.random.randint(0, n_doors)\n\n    # host picks an empty door not equal to prize or player\n    if player == prize:\n        # exclude only 'player' (size n_doors-1)\n        k = np.random.randint(0, n_doors - 1)\n        host = k + (1 if k &gt;= player else 0)\n    else:\n        # exclude 'player' and 'prize' (size n_doors-2)\n        a = player if player &lt; prize else prize\n        b = prize if prize &gt; player else player\n        k = np.random.randint(0, n_doors - 2)\n        host = k + (1 if k &gt;= a else 0) + (1 if k &gt;= b else 0)\n\n    # player switches: choose uniformly from doors != player and != host\n    a2 = player if player &lt; host else host\n    b2 = host if host &gt; player else player\n    k2 = np.random.randint(0, n_doors - 2)\n    player2 = k2 + (1 if k2 &gt;= a2 else 0) + (1 if k2 &gt;= b2 else 0)\n\n    noswitch_win = 1 if prize == player  else 0\n    switch_win   = 1 if prize == player2 else 0\n    return noswitch_win, switch_win\n\n@njit(parallel=True)\ndef _run_parallel(n_doors: int, n_trials: int) -&gt; (int, int):\n    ns = np.zeros(n_trials, dtype=np.int64)\n    sw = np.zeros(n_trials, dtype=np.int64)\n    for i in prange(n_trials):\n        a, b = _trial_once(n_doors)\n        ns[i] = a\n        sw[i] = b\n    return int(ns.sum()), int(sw.sum())\n\n@njit\ndef _run_serial(n_doors: int, n_trials: int) -&gt; (int, int):\n    noswitch = 0\n    switch   = 0\n    for _ in range(n_trials):\n        a, b = _trial_once(n_doors)\n        noswitch += a\n        switch   += b\n    return noswitch, switch\n\n# --- public API ---\n\ndef montyhall_numba(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None,\n    parallel: bool = True,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Monty Hall (host opens one empty door) using Numba-compiled loops.\n\n    Parameters\n    ----------\n    n_doors : int\n        Number of doors (&gt;= 3).\n    n_trials : int\n        Number of trials to simulate.\n    seed : int, optional\n        Seed for reproducibility.\n    parallel : bool, default True\n        Use a parallel loop over trials (multi-core).\n\n    Returns\n    -------\n    Dict[str, int]\n        {'noswitch': ..., 'switch': ...}\n    \"\"\"\n    if n_doors &lt; 3:\n        raise ValueError(\"n_doors must be &gt;= 3\")\n\n    if seed is not None:\n        _seed_numba(int(seed))  # seed the compiled RNG\n\n    ns, sw = (_run_parallel(n_doors, n_trials)\n              if parallel else\n              _run_serial(n_doors, n_trials))\n    return {\"noswitch\": ns, \"switch\": sw}\n\nIt wins when n_trials is very large (e.g., 10–100M) where loop overhead is amortized and you give it many threads, or the design avoids massive temporaries (not much in this example).\nLet’s see their time comparison.\n\nimport timeit\n\n# --- Timing function ---\ndef benchmark(n_doors: int = 3, n_trials: int = 1_000_00, seed: int = 42) -&gt; None:\n    print(f\"n_doors={n_doors}, n_trials={n_trials}\")\n    # Ensure deterministic where possible\n    np.random.seed(seed)\n\n    # Time original\n    t_orig = min(timeit.repeat(lambda: montyhall(n_doors, n_trials),\n                               repeat=3, number=1))\n    print(f\"Original (lists): {t_orig:.4f}s\")\n\n    # Time vectorized\n    t_vec = min(timeit.repeat(lambda: montyhall_fast(n_doors, n_trials, seed),\n                              repeat=3, number=1))\n    print(f\"Vectorized NumPy: {t_vec:.4f}s\")\n\n    # Time Numba serial (compile excluded by earlier warm-up)\n    t_numba_ser = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=False), repeat=3, number=1))\n    print(f\"Numba serial:     {t_numba_ser:.4f}s\")\n\n    # Time Numba parallel\n    t_numba_par = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=True), repeat=3, number=1))\n    print(f\"Numba parallel:   {t_numba_par:.4f}s\")\n\n\nbenchmark(n_doors = 4, n_trials = 1000_000)\n\nn_doors=4, n_trials=1000000\nOriginal (lists): 41.1009s\nVectorized NumPy: 0.0717s\nNumba serial:     0.0586s\n\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nNumba parallel:   0.0399s",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#variables-versus-objects",
    "href": "04-python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4855634496\n4855634496\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4554394384\n4639563840\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#number-representation",
    "href": "04-python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#sec-python-venv",
    "href": "04-python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "05-visualization.html",
    "href": "05-visualization.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Grammar of Graphics using Plotnine\nIn this chapter we embrace visualization as a cornerstone of modern data-analysis workflows: turning raw numbers into meaningful visuals that support insight, decision-making and communication. We begin by exploring static and information-rich graphics through plotnine, building on the grammar‐of-graphics approach. Next we extend into spatial data-visualisation using GeoPandas, equipping you to map, project and interpret geospatial patterns. Later sections will introduce further tools and techniques (for example interactive maps or dashboards), but throughout we emphasise the same core questions: Which visual form fits the data and question? How do our design and implementation choices influence what the viewer sees — and what they don’t see? With the tools and principles in hand, you’ll be prepared to insert clear, effective visualisation into your data-science project workflow.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visualization.html#grammar-of-graphics-using-plotnine",
    "href": "05-visualization.html#grammar-of-graphics-using-plotnine",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Introduction\nHello everyone! My name is Sonia Lucey and I am a Statistical Data Science major.\nToday I will be talking about the Grammar of Graphics using Plotnine.\nWe will use NYC Crash Data (Week of Labor Day, 2025), saved in the Feather format for faster loading.\nJust like languages have grammar to structure words and sentences, the Grammar of Graphics gives us a structured framework for building visualizations.\n(Wilkinson, 2012)\nEvery plot can be described using a few key components:\n\nData: what dataset you are using.\n\nAesthetics: axes, color, size, shape.\n\nScales: how values map to axes or colors.\n\nGeometric objects (geoms): points, bars, lines, etc.\n\nStatistics: summaries such as counts, means, distributions.\n\nFacets: break plots into subplots.\n\nCoordinate system: Cartesian, polar, etc.\n\n\n\n5.1.2 Building Plots with Plotnine\nThe Grammar of Graphics (Wilkinson, 2012). Plotnine implements the Grammar of Graphics in Python ((Sarkar, 2018); (Bansal, 2018)). It is inspired by ggplot2 in R and allows consistent, layered visualizations.\nSome examples of what we can create:\n- Bar Charts\n- Scatter Plots\n- Histograms\n- Box Plots\n- Facet Plots\nWe’ll build each of these using the NYC crash dataset.\nTo get started, install and import Plotnine:\n\nfrom plotnine import *\nimport pandas as pd\n\ndf = pd.read_feather(\"data/nyc_crashes_cleaned.feather\")\ndf[\"borough\"] = df[\"borough\"].fillna(\"Unknown\")\n\nWhen writing plots in Plotnine, follow a logical order that mirrors the Grammar of Graphics:\n\nData + Mapping → ggplot(data, aes(...))\n\nGeom → what to draw (geom_bar(), geom_point(), etc.)\n\nScales & Labels → labs(), xlab(), ylab(), ggtitle()\n\nCoordinates → coord_flip(), coord_polar()\n\nFacets → facet_wrap() or facet_grid()\n\nTheme → theme_minimal(), theme_classic()\n\n\n\n5.1.3 Examples of Visualizations\nBar Chart\n\n(ggplot(df, aes(x=\"borough\", fill=\"borough\"))\n + geom_bar()\n + ggtitle(\"Number of Crashes by Borough\")\n + coord_flip()\n + xlab(\"Borough\")\n + ylab(\"Number of Crashes\"))\n\n\n\n\n\n\n\n\nThis shows how many crashes happened in each borough.\nScatter Plot\n\n(ggplot(df, aes(x=\"number_of_persons_injured\", \n                y=\"number_of_persons_killed\"))\n + geom_point(alpha=0.6)\n + labs(title=\"Injuries vs Fatalities per Crash\",\n        x=\"Persons Injured\",\n        y=\"Persons Killed\"))\n\n\n\n\n\n\n\n\nMost crashes cause injuries but not fatalities.\nHistogram\n\ndf[\"hour\"] = df[\"crash_datetime\"].dt.hour\n\n(ggplot(df, aes(x=\"hour\"))\n + geom_histogram(binwidth=1, color=\"black\", fill=\"skyblue\")\n + ggtitle(\"Distribution of Crashes by Hour\")\n + xlab(\"Hour of Day\")\n + ylab(\"Crash Count\"))\n\n\n\n\n\n\n\n\nCrashes are elevated throughout the day, with particularly high counts around midday and late afternoon. Midnight also shows an unexpected spike, which may reflect a default rather than commuting patterns.\nBox Plot\n\n(ggplot(df, aes(x=\"borough\", y=\"number_of_persons_injured\", fill=\"borough\"))\n + geom_boxplot()\n + ggtitle(\"Persons Injured by Borough\")\n + xlab(\"Borough\")\n + ylab(\"Number of Persons Injured\"))\n\n\n\n\n\n\n\n\nThe boxplot compares injury severity between boroughs.\nFacet Wrap\n\ndf[\"contributing_factor_vehicle_1\"] = (\n    df[\"contributing_factor_vehicle_1\"]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n    .replace({\"\": None, \"na\": None, \"nan\": None})\n)\ntop_factors = (df[\"contributing_factor_vehicle_1\"]\n               .value_counts()\n               .head(10)\n               .index)\ndf_top = df[df[\"contributing_factor_vehicle_1\"].isin(top_factors)]\n\n\n(ggplot(df_top, aes(x=\"contributing_factor_vehicle_1\", fill=\"borough\"))\n + geom_bar(show_legend=False)\n + facet_wrap(\"~ borough\")\n + theme(axis_text_x=element_text(rotation=90, hjust=1))\n + ggtitle(\"Top 10 Contributing Factors by Borough\"))\n\n\n\n\n\n\n\n\nFaceting lets us compare contributing factors side by side across boroughs.\nFacet Grid\n\n(ggplot(df, aes(x=\"hour\", fill=\"borough\"))\n + geom_histogram(binwidth=1, alpha=0.6, position=\"identity\")\n + facet_grid(\"borough ~ .\")\n + labs(title=\"Crashes by Hour of Day Across Boroughs\",\n        x=\"Hour of Day\", y=\"Number of Crashes\"))\n\n\n\n\n\n\n\n\nGrid vs. Wrap Faceting allows you to split plots by one or two variables for comparison.\n\nfacet_wrap() arranges plots in a single flexible grid (best for one variable).\nfacet_grid() creates a strict row×column layout (best for two variables).\nKey difference: wrap = flexible (one variable), grid = fixed (two variables).\n\n\n\n5.1.4 Key Takeaway\nThe Grammar of Graphics shifts our mindset: instead of asking “what chart type do I need?”, we ask “what grammar components best represent my data and message?”\nThis makes visualizations flexible, reusable, and less error prone. And once you know the grammar, learning tools like Plotnine or ggplot2 ((Wilkinson, 2012); (Sarkar, 2018); (Bansal, 2018)) becomes much easier. Think of plots not as pictures, but as structured sentences written with this grammar.\n\n\n5.1.5 Further Readings\n\nAeturrell’s Data Visualization Tutorials – Clear, example-driven lessons on Plotnine, ggplot2, and the Grammar of Graphics for data scientists.\n\nReal Python: Data Visualization with Plotnine – A practical guide to building layered graphics using Plotnine and pandas.\n\n[GeeksforGeeks: Plotnine in Python] (https://www.geeksforgeeks.org/plotnine-data-visualization-in-python/) – Overview of Plotnine syntax, functions, and common chart types.\n\nJeroen Janssens – Data Science at the Command Line – Broader perspective on data workflows and reproducible visualization pipelines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visualization.html#spatial-data-with-geopandas",
    "href": "05-visualization.html#spatial-data-with-geopandas",
    "title": "5  Data Visualization",
    "section": "5.2 Spatial Data with Geopandas",
    "text": "5.2 Spatial Data with Geopandas\nThis section is by Alejandro Haerter, a junior majoring in Statistical Data Science and Economics.\n\n5.2.1 Spatial Data and Python\nSpatial data is any information which describes the geographic location and shape of features. We might represent these as:\n\nPoints (address, cities)\nLines (Roads, rivers)\nPolygons (Property parcels, city boundaries, ZIP codes)\n\nSpatial data is everywhere; we see it on maps, it has legal implications; and it often delineates demographic information. In short, a data scientist is certain to encounter spatial data in their career, and should know the tools to work with it.\nTraditional Geographic Information Systems (GIS) tools (e.g., ArcGIS, QGIS) are proprietary, require steep learning curves, and do not implement well with the data science workflow. Luckily, with GeoPandas, we can use Python for spatial data analysis, preserving the data science workflow.\n\n\n5.2.2 Introduction to GeoPandas\nGeoPandas is an open source package which adds support for geographic data to pandas objects, first released in 2014. GeoPandas’ two main data structures are the geopandas.GeoSeries, an extension of the pandas.Series, and the geopandas.GeoDataFrame, an extension of the pandas.DataFrame.\nGeoPandas is capable of geometric operations, transformations, and plotting, all relevant tools for for operating with spatial data.\nGeoPandas requires Folium and Matplotlib as dependencies, both of which for plotting.\n\n5.2.2.1 GeoSeries\ngeopandas.GeoSeries a subclass of pandas.Series which can only store geometries. Potential geometries include Points, Lines, Polygons, etc. Not all geometries need be the same type, but must be some form of Shapely geometric object.\nThe GeoSeries.crs attribute stores the coordinate reference system (CRS) information of the GeoSeries. A CRS relates how map coordinates relate to real locations on Earth by specifying a datum (a model of the Earth’s shape), a coordinate system (e.g., Latitude/Longitude, UTM) and units of measurement (e.g., degrees, meters).\n\n\n5.2.2.2 GeoDataFrame\nA GeoDataFrame is the core data stucture of GeoPandas. It can store one or more geometry columns and perform spatial operations. Essentially, it is a pandas.DataFrame combined with one or more GeoSeries.\nA mock GeoDataFrame might look like:\n    city       population                   geometry\n0  NYC        8800000      POINT (-74.0060 40.7128)\n1  Boston      675000      POINT (-71.0589 42.3601)\n2  Chicago    2700000      POINT (-87.6298 41.8781)\nImportantly, while we can have multiple GeoSeries in a GeoDataFrame, only one GeoSeries at a time is the active geometry column. All geometric operations act on this column; it’s accessed by GeoDataFrame.geometry attribute.\n\n\n\n5.2.3 Basic Operations\nA file containg both data and geometry can be read by geopandas.read_file(). For this example, I use a dataset which contains geometric information for each of NYC’s five boroughs.\n\nimport geopandas as gpd\nfrom geodatasets import get_path\n\npath_to_data = get_path(\"nybb\")  # map of NYC boroughs\ngdf = gpd.read_file(path_to_data)\n\ngdf\n\n\n\n\n\n\n\n\nBoroCode\nBoroName\nShape_Leng\nShape_Area\ngeometry\n\n\n\n\n0\n5\nStaten Island\n330470.010332\n1.623820e+09\nMULTIPOLYGON (((970217.022 145643.332, 970227....\n\n\n1\n4\nQueens\n896344.047763\n3.045213e+09\nMULTIPOLYGON (((1029606.077 156073.814, 102957...\n\n\n2\n3\nBrooklyn\n741080.523166\n1.937479e+09\nMULTIPOLYGON (((1021176.479 151374.797, 102100...\n\n\n3\n1\nManhattan\n359299.096471\n6.364715e+08\nMULTIPOLYGON (((981219.056 188655.316, 980940....\n\n\n4\n2\nBronx\n464392.991824\n1.186925e+09\nMULTIPOLYGON (((1012821.806 229228.265, 101278...\n\n\n\n\n\n\n\n\n5.2.3.1 Inspecting a GeoDataFrame\nGeoPandas syntax is just like pandas. Methods like .head(), .info(), and .shape, .rename, .drop, etc., apply, all work the same. For example, I rename the column geometry to poly, so that I don’t confuse it with the .geometry attribute. (This is an example of good naming practice to avoid conflicts with built-in attributes or methods.)\n\ngdf = gdf.rename(columns={\"geometry\": \"poly\"})\ngdf.head(1)\n\n\n\n\n\n\n\n\nBoroCode\nBoroName\nShape_Leng\nShape_Area\npoly\n\n\n\n\n0\n5\nStaten Island\n330470.010332\n1.623820e+09\nMULTIPOLYGON (((970217.022 145643.332, 970227....\n\n\n\n\n\n\n\nGeoPandas also has its own functions, methods, and attributes which are specific to it. Recall .geometry, which gives us the active geometry column. This GeoDataFrame is still pointing to a column called \"geometry\", even though its renamed and doesn’t exist. This can be fixed with .set_geometry.\n\ngdf = gdf.set_geometry('poly')\nprint(gdf.geometry)\n\n0    MULTIPOLYGON (((970217.022 145643.332, 970227....\n1    MULTIPOLYGON (((1029606.077 156073.814, 102957...\n2    MULTIPOLYGON (((1021176.479 151374.797, 102100...\n3    MULTIPOLYGON (((981219.056 188655.316, 980940....\n4    MULTIPOLYGON (((1012821.806 229228.265, 101278...\nName: poly, dtype: geometry\n\n\nRecall how to access CRS:\n\nprint(gdf.crs)\n\nEPSG:2263\n\n\nEPSG:2263 is a CRS specific for New York City. It uses feet for distance operations.\n\n\n5.2.3.2 Area\nIf I wanted to find the area enclosed by the polygons, I’d use the .area attribute, which gives the area enclosed by each polygon.\n\ngdf = gdf.set_index(\"BoroName\") # for legibility\ngdf[\"area\"] = gdf.area\ngdf[\"area\"]\n\nBoroName\nStaten Island    1.623822e+09\nQueens           3.045214e+09\nBrooklyn         1.937478e+09\nManhattan        6.364712e+08\nBronx            1.186926e+09\nName: area, dtype: float64\n\n\nBecause of EPSG:2263, area is given in square footage. For example, Manhattan is 6.364712e+08 = 636,471,200 ft2 = 22.9mi2.\n\n\n5.2.3.3 Boundaries and Centroids\nRight now, the active geometry column contains polygons. We can access the perimeters and the centroids of these polygons:\n\ngdf[\"boundary\"] = gdf.boundary\ngdf[\"centroid_ft\"] = gdf.centroid\n\ngdf[['boundary','centroid_ft']].head()\n\n\n\n\n\n\n\n\nboundary\ncentroid_ft\n\n\nBoroName\n\n\n\n\n\n\nStaten Island\nMULTILINESTRING ((970217.022 145643.332, 97022...\nPOINT (941639.45 150931.991)\n\n\nQueens\nMULTILINESTRING ((1029606.077 156073.814, 1029...\nPOINT (1034578.078 197116.604)\n\n\nBrooklyn\nMULTILINESTRING ((1021176.479 151374.797, 1021...\nPOINT (998769.115 174169.761)\n\n\nManhattan\nMULTILINESTRING ((981219.056 188655.316, 98094...\nPOINT (993336.965 222451.437)\n\n\nBronx\nMULTILINESTRING ((1012821.806 229228.265, 1012...\nPOINT (1021174.79 249937.98)\n\n\n\n\n\n\n\ngdf now has boundary and centroid columns as additional geometry columns, but the active column wont change unless specified.\n\n\n5.2.3.4 Distance Operation\nIf I wanted to find the distance between the center of Brooklyn and the center of the Bronx, that’s taking the .distance() between two centroids.\n\n# active geometry set to centroid info\ngdf = gdf.set_geometry('centroid_ft')\n\n#finds distance between indeces given\ngdf.geometry['Bronx'].distance(gdf.geometry['Brooklyn'])\n\n79011.6278663779\n\n\nRecall the current EPSG:2263, which is a projection in feet. ~79,000ft \\(\\approx\\) ~15mi.\nWe have all the pandas functionality available here too, for example, .mean():\n\nfrom shapely.geometry import Point\n\ncx = gdf[\"centroid_ft\"].x.mean()\ncy = gdf[\"centroid_ft\"].y.mean()\ngeo_center_ft = Point(cx, cy) # Shapely\n\nprint(geo_center_ft)\n\nPOINT (997899.6796377342 198921.55457843072)\n\n\nGives us a Point position of the centroid of centroids, i.e., the geographic center of NYC. Although, that doesn’t tell us very much…\n\n\n5.2.3.5 Changing CRS\nRule of thumb: Operations which rely on distance and area should use a Projected CRS (m, ft, km, etc). Geographic CRS (degrees) is better for position information, like Lat/Lon of a location. We use .to_crs().\n\n# new GeoSeries is just a different projection of existing GeoSeries\ngdf['centroid_ll'] = gdf['centroid_ft'].to_crs(4326)\ngdf['centroid_ll']\n\nBoroName\nStaten Island     POINT (-74.1534 40.58085)\nQueens           POINT (-73.81847 40.70757)\nBrooklyn         POINT (-73.94768 40.64472)\nManhattan        POINT (-73.96719 40.77725)\nBronx            POINT (-73.86653 40.85262)\nName: centroid_ll, dtype: geometry\n\n\ncentroid_ll is centroid reprojected EPSG:4326, now giving coordinates. E.g., the center of the Bronx is at coordinates 40.85°N, 73.87°W.\n\nclon = gdf[\"centroid_ll\"].x.mean()\nclat = gdf[\"centroid_ll\"].y.mean()\ngeo_center_ll = Point(clon, clat) # Shapely\n\nprint(geo_center_ll)\n\nPOINT (-73.95065406867823 40.7126019492116)\n\n\n\n\n5.2.3.6 File Writing\nWhen I want to save my GeoDataFrame to the computer, we use GeoDataFrame.to_file. GeoPandas will infer by the file format by the file extension.\n\n# This won't run because geoJSON doesn't support multiple GeoSeries!\ngdf.to_file(\"nyc_boroughs.geojson\")\n\nI recommend using feather.\n\n# Use Feather!\nimport pyarrow\ngdf.to_feather(\"nyc_boroughs.feather\")\n\nJust as pandas can handle file types .csv, .feather, .html, etc., GeoPandas data can also be stored in multipe file types, like .shp, .geojson, and also .feather! These file types are are used by a variety of different GIS software, but they all contain information which can read as a GeoDataFrame.\n\n\n5.2.3.7 Other Useful Methods\n\nlength() Returns length of geometries (useful for LineStrings like roads).\nintersects(other)\nTrue if two geometries overlap or touch.\ncontains(other) True if one geometry fully contains another.\nbuffer(distance)\nCreates a new geometry expanded outward (or inward if negative) by the given distance.\nequals(other) Checks geometric equality\nis_valid Boolean check: are geometries valid (no self-intersections, etc.)?\n\n\n\n\n5.2.4 Plotting\nGeoPandas is capable of plotting/mapping spatial data on both static and interactive figures, using Matplotlib and Folium respectively.\n\n5.2.4.1 Static Maps\nPlotting operations are done on the active geometry column with Matplotlib syntax, .plot().\nThis code plots the polygons and colors them by their total area:\n\n# active geometry column = poly\ngdf = gdf.set_geometry(\"poly\")\ngdf.plot('area', legend=True)\n\n\n\n\n\n\n\n\nWe can map multiple GeoSeries by using one plot as an axis for another. First, we need to verify that centroid_ll and poly are in the same CRS by setting both to EPSG:4326, which gives Latitude and Longitude. I can use the .crs attribute from one GeoDataFrame as input for .to_crs to ensure a match. This code corrects the CRS and plots both centroids and polygons:\n\n# polygons in lat/lon\nax = gdf.set_geometry(\"poly\").to_crs(gdf[\"centroid_ll\"].crs).plot()\ngdf.set_geometry(\"centroid_ll\").plot(ax=ax, color=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n5.2.5 Interactive Maps\nGeopandas uses a Folium/Leaflet backend to make interactive maps very easy, using .explore().\nThis code gives an interactive map of the same data:\n\ngdf = gdf.set_geometry(\"poly\")\ngdf.explore(\"area\", legend=False, zoom_to_layer=True)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.2.6 Worked Out Example: NYC Collision Data\nThis section demonstrates the start-to-finish workflow of using GeoPandas on a real-world dataset. We will be using the cleaned NYC collision data, courtesy of Wilson Tang. Additionally, for our spatial data, I’ll be using the .shp file from the NYC Modified Zip Code Tabulation Areas (MODZCTA) dataset. ZIP codes can be reassigned and their boundaries changed;the goal of this dataset is to preserve the ZIP code shapes for geospatial analysis.\nGoal: overlay crash locations on a map of NYC.\n\n5.2.6.1 Load and Inspect Data\nWe begin by installing our required dependencies, and loading our datasets.\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pyarrow\n\ncrash_df = pd.read_feather('data/nyc_crashes_cleaned.feather')\n\nzip_gdf = gpd.read_feather(\"data/nyc_modzcta_gp.feather\")\n\nzip_gdf.head()\n\n\n\n\n\n\n\n\nmodzcta\nlabel\nzcta\npop_est\ngeometry\n\n\n\n\n0\n10001\n10001, 10118\n10001, 10119, 10199\n23072.0\nPOLYGON ((-73.98774 40.74407, -73.98819 40.743...\n\n\n1\n10002\n10002\n10002\n74993.0\nPOLYGON ((-73.9975 40.71407, -73.99709 40.7146...\n\n\n2\n10003\n10003\n10003\n54682.0\nPOLYGON ((-73.98864 40.72293, -73.98876 40.722...\n\n\n3\n10026\n10026\n10026\n39363.0\nMULTIPOLYGON (((-73.96201 40.80551, -73.96007 ...\n\n\n4\n10004\n10004\n10004\n3028.0\nMULTIPOLYGON (((-74.00827 40.70772, -74.00937 ...\n\n\n\n\n\n\n\ncrash_df is our pandas.DataFrame which contains collision data, and zip_gdf is our GeoPandas.GeoDataFrame which contains ZIP code data.\n\ncrash_df = crash_df.dropna(subset=[\"longitude\", \"latitude\"])\nzip_gdf = zip_gdf.drop(columns=[\"label\", \"zcta\"])\n\n\n\n5.2.6.2 Data Overlay\ncrash_df doesn’t yet have an active geometry column to use for plotting, but does have Latitude and Longitude information. Function gpd.points_from_xy() can take these inputs (which use EPSG:4326) to produce a GeoSeries of geometric shapely.Point objects. The new crash_gdf GeoDataFrame combines these two.\n\n# Create gdf so we can visualize\ncrash_gdf = gpd.GeoDataFrame(\n    crash_df,\n    geometry=gpd.points_from_xy(crash_df[\"longitude\"], \n                                crash_df[\"latitude\"]),\n    crs=\"EPSG:4326\"\n)\n\n# Double-check: ensure geodataframes same CRS\nzip_gdf = zip_gdf.to_crs(crash_gdf.crs)\n\nUsing Matplotlib syntax, I use add zip_gdf to the .plot(), specifying how I want them to appear. I do the same for crash_gdf.\n\n# Overlay crashes on Borough Polygons\nfig, ax = plt.subplots(figsize=(7, 7))\nzip_gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\", linewidth=0.8)\ncrash_gdf.plot(ax=ax, markersize=1, color=\"red\", alpha=0.5)\nax.set_title(\"NYC Crashes, Labor Day Week 2025\")\nplt.show()\n\n\n\n\n\n\n\n\nThis overlay uses two GeoSeries, each from a different GeoDataFrame. In this case, it was necessary to keep the two seperate, as they have fundamentally different structures.\n\n\n5.2.6.3 Spatial Joins\nThere are advantages of using just one GeoDataFrame. We use spatial join function gpd.sjoin(), which parallels pd.merge. This function combines two dataframes by matching keys and a join condition type.\n\n# Crashes put within zips\n# predicate=\"within\" requires all of a geometry's points to be within\n# the interior of the spatially joined geometry (and none on the exterior)\njoined = gpd.sjoin(crash_gdf, zip_gdf, predicate=\"within\", how=\"left\")\n\n# count number of crashes per ZIP; creates new Series\n# \"modzcta\" same as zip code. it says to group by zip code.\ncounts = joined.groupby(\"modzcta\").size().rename(\"n_crashes\")\n\n# Attach crash counts back to the polygon GeoDataFrame\nzip_counts = zip_gdf.merge(counts, on=\"modzcta\", how=\"left\").fillna({\"n_crashes\": 0})\nzip_counts = zip_counts.set_geometry(\"geometry\").to_crs(4326)\n\nzip_counts[[\"modzcta\", \"n_crashes\"]].head()\n\n\n\n\n\n\n\n\nmodzcta\nn_crashes\n\n\n\n\n0\n10001\n8.0\n\n\n1\n10002\n17.0\n\n\n2\n10003\n9.0\n\n\n3\n10026\n2.0\n\n\n4\n10004\n0.0\n\n\n\n\n\n\n\nThe new GeoDataFrame zip_counts gives us crash count by ZIP code, which allows us new plotting opportunities.\n\n\n5.2.6.4 Plotting Joined Data\nChoropleth maps provide an easy way to visualize how a variable varies across a geographic area or show the level of variability within a region.\n\n# Plot polygons colored by crash counts\nfig, ax = plt.subplots(figsize=(7, 7))\nzip_counts.plot(ax=ax, column=\"n_crashes\", legend=True)\nax.set_title(\"Crashes per NYC ZIP\")\nplt.show()\n\n\n\n\n\n\n\n\nInteractive choropleth maps are especially helpful on websites. tooltip specifies which two variables appear when I hover the mouse over a given polygon.\n\n# render and auto-fit to layer\nzip_counts.explore(column=\"n_crashes\",\n                   legend=True,\n                   tooltip=[\"modzcta\",\"n_crashes\"],\n                   zoom_to_layer=True)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n5.2.7 Further Readings\n\nGeoPandas: See more examples of GeoPandas uses.\nPandas: Not technically a dependency, but complete understanding of Pandas syntax is necessary to be successful with GeoPandas. See Pandas section in the classnotes.\nShapely: GeoPandas leverages Shapely for geometric object types and operations. You won’t interface much with Shapely directly, but is helpful to have a basic understanding of.\nMatplotlib: Static plotting operations use Matplotlib syntax.\nFolium: Interactive mapping uses a Folium backend. Folium is a very powerful tool for spatial visualization, which warrants its own topic presentation.\nEPSG: Familiarize yourself with the most common CRS, which are given by unique EPSG codes. The EPSG database currently contrains over 5000 unique entries.\n\n\n\n\n\n\nBansal, S. (2018). Python data visualisation made easy with plotnine: A how-to guide. https://medium.com/@suraj_bansal/python-data-visualisation-made-easy-with-plotnine-a-how-to-guide-f71e321bdef1; Medium.\n\n\nSarkar, D. (DJ). (2018). A comprehensive guide to the grammar of graphics for effective visualization of multi-dimensional data. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149/; Medium, Towards Data Science.\n\n\nWilkinson, L. (2012). The grammar of graphics. Springer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html",
    "href": "06-manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#introduction",
    "href": "06-manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#example-nyc-crash-data",
    "href": "06-manipulation.html#example-nyc-crash-data",
    "title": "6  Data Manipulation",
    "section": "6.2 Example: NYC Crash Data",
    "text": "6.2 Example: NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of August 31, 2025, on September 11, 2025, in CSC format.\n\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyc_crashes_lbdwk_2025.csv'\ndf = pd.read_csv(file_path,\n                 dtype={'LATITUDE': np.float32,\n                        'LONGITUDE': np.float32,\n                        'ZIP CODE': str})\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Check for missing values\ndf.isnull().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           284\nzip_code                          284\nlatitude                           12\nlongitude                          12\nlocation                           12\non_street_name                    456\ncross_street_name                 587\noff_street_name                  1031\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1       9\ncontributing_factor_vehicle_2     355\ncontributing_factor_vehicle_3    1358\ncontributing_factor_vehicle_4    1447\ncontributing_factor_vehicle_5    1474\ncollision_id                        0\nvehicle_type_code_1                17\nvehicle_type_code_2               475\nvehicle_type_code_3              1363\nvehicle_type_code_4              1452\nvehicle_type_code_5              1474\ndtype: int64\n\n\nTake a peek at the first five rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n08/31/2025\n12:49\nQUEENS\n11101\n40.753113\n-73.933701\n(40.753113, -73.9337)\n30 ST\n39 AVE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4838875\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n1\n08/31/2025\n15:30\nMANHATTAN\n10022\n40.760601\n-73.964317\n(40.7606, -73.96432)\nE 59 ST\n2 AVE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4839110\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n2\n08/31/2025\n19:00\nNaN\nNaN\n40.734234\n-73.722748\n(40.734234, -73.72275)\nCROSS ISLAND PARKWAY\nHILLSIDE AVENUE\nNaN\n...\nUnspecified\nUnspecified\nNaN\nNaN\n4838966\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n3\n08/31/2025\n1:19\nBROOKLYN\n11220\n40.648075\n-74.007034\n(40.648075, -74.007034)\nNaN\nNaN\n4415 5 AVE\n...\nUnspecified\nNaN\nNaN\nNaN\n4838563\nSedan\nE-Bike\nNaN\nNaN\nNaN\n\n\n4\n08/31/2025\n2:41\nMANHATTAN\n10036\n40.756561\n-73.986107\n(40.75656, -73.98611)\nW 43 ST\nBROADWAY\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4838922\nStation Wagon/Sport Utility Vehicle\nBike\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nA quick summary of the data types of the columns:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1487 entries, 0 to 1486\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1487 non-null   object \n 1   crash_time                     1487 non-null   object \n 2   borough                        1203 non-null   object \n 3   zip_code                       1203 non-null   object \n 4   latitude                       1475 non-null   float32\n 5   longitude                      1475 non-null   float32\n 6   location                       1475 non-null   object \n 7   on_street_name                 1031 non-null   object \n 8   cross_street_name              900 non-null    object \n 9   off_street_name                456 non-null    object \n 10  number_of_persons_injured      1487 non-null   int64  \n 11  number_of_persons_killed       1487 non-null   int64  \n 12  number_of_pedestrians_injured  1487 non-null   int64  \n 13  number_of_pedestrians_killed   1487 non-null   int64  \n 14  number_of_cyclist_injured      1487 non-null   int64  \n 15  number_of_cyclist_killed       1487 non-null   int64  \n 16  number_of_motorist_injured     1487 non-null   int64  \n 17  number_of_motorist_killed      1487 non-null   int64  \n 18  contributing_factor_vehicle_1  1478 non-null   object \n 19  contributing_factor_vehicle_2  1132 non-null   object \n 20  contributing_factor_vehicle_3  129 non-null    object \n 21  contributing_factor_vehicle_4  40 non-null     object \n 22  contributing_factor_vehicle_5  13 non-null     object \n 23  collision_id                   1487 non-null   int64  \n 24  vehicle_type_code_1            1470 non-null   object \n 25  vehicle_type_code_2            1012 non-null   object \n 26  vehicle_type_code_3            124 non-null    object \n 27  vehicle_type_code_4            35 non-null     object \n 28  vehicle_type_code_5            13 non-null     object \ndtypes: float32(2), int64(9), object(18)\nmemory usage: 325.4+ KB\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Drop the redundant `latitute` and `longitude` columns\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nLet’s get some basic frequency tables of borough and zip_code, whose values could be used to check their validity against the legitmate values.\n\n# Frequency table for 'borough' without filling missing values\nborough_freq = df['borough'].value_counts(dropna=False).reset_index()\nborough_freq.columns = ['borough', 'count']\n\n# Frequency table for 'zip_code' without filling missing values\nzip_code_freq = df['zip_code'].value_counts(dropna=False).reset_index()\nzip_code_freq.columns = ['zip_code', 'count']\nzip_code_freq\n\n\n\n\n\n\n\n\nzip_code\ncount\n\n\n\n\n0\nNaN\n284\n\n\n1\n11207\n33\n\n\n2\n11203\n29\n\n\n3\n11212\n23\n\n\n4\n11233\n21\n\n\n...\n...\n...\n\n\n159\n10028\n1\n\n\n160\n11426\n1\n\n\n161\n10307\n1\n\n\n162\n11365\n1\n\n\n163\n11694\n1\n\n\n\n\n164 rows × 2 columns\n\n\n\nA comprehensive list of ZIP codes by borough can be obtained, for example, from the New York City Department of Health’s UHF Codes. We can use this list to check the validity of the zip codes in the data.\n\n# List of valid NYC ZIP codes compiled from UHF codes\n# Define all_valid_zips based on the earlier extracted ZIP codes\nall_valid_zips = {\n    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,\n    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,\n    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,\n    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,\n    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,\n    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,\n    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,\n    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,\n    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,\n    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,\n    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,\n    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,\n    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,\n    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,\n    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,\n    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,\n    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,\n    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,\n    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,\n    10306, 10307, 10308, 10309, 10312\n}\n\n    \n# Convert set to list of strings\nall_valid_zips = list(map(str, all_valid_zips))\n\n# Identify invalid ZIP codes (including NaN)\ninvalid_zips = df[\n    df['zip_code'].isna() | ~df['zip_code'].isin(all_valid_zips)\n    ]['zip_code']\n\n# Calculate frequency of invalid ZIP codes\ninvalid_zip_freq = invalid_zips.value_counts(dropna=False).reset_index()\ninvalid_zip_freq.columns = ['zip_code', 'frequency']\n\ninvalid_zip_freq\n\n\n\n\n\n\n\n\nzip_code\nfrequency\n\n\n\n\n0\nNaN\n284\n\n\n1\n10000\n4\n\n\n2\n10065\n3\n\n\n3\n10075\n2\n\n\n4\n11430\n1\n\n\n\n\n\n\n\nAs it turns out, the collection of valid NYC zip codes differ from different sources. From United States Zip Codes, 10065 appears to be a valid NYC zip code. Under this circumstance, it might be safer to not remove any zip code from the data.\nTo be safe, let’s concatenate valid and invalid zips.\n\n# Convert invalid ZIP codes to a set of strings\ninvalid_zips_set = set(invalid_zip_freq['zip_code'].dropna().astype(str))\n\n# Convert all_valid_zips to a set of strings (if not already)\nvalid_zips_set = set(map(str, all_valid_zips))\n\n# Merge both sets\nmerged_zips = invalid_zips_set | valid_zips_set  # Union of both sets\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nnp.array([missing_cooccur, total_missing_zip_code, total_missing_borough])\n\narray([284, 284, 284])\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use module Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nget_zip_code(latitude, longitude)\n\n'11101'\n\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#accessing-census-data",
    "href": "06-manipulation.html#accessing-census-data",
    "title": "6  Data Manipulation",
    "section": "6.3 Accessing Census Data",
    "text": "6.3 Accessing Census Data\nThe U.S. Census Bureau provides extensive demographic, economic, and social data through multiple surveys, including the decennial Census, the American Community Survey (ACS), and the Economic Census. These datasets offer valuable insights into population trends, economic conditions, and community characteristics at multiple geographic levels.\nThere are multiple ways to access Census data. For example:\n\nCensus API: The Census API allows programmatic access to various datasets. It supports queries for different geographic levels and time periods.\ndata.census.gov: The official web interface for searching and downloading Census data.\nIPUMS USA: Provides harmonized microdata for longitudinal research. Available at IPUMS USA.\nNHGIS: Offers historical Census data with geographic information. Visit NHGIS.\n\nIn addition, Python tools simplify API access and data retrieval.\n\n6.3.1 Python Tools for Accessing Census Data\nSeveral Python libraries facilitate Census data retrieval:\n\ncensus: A high-level interface to the Census API, supporting ACS and decennial Census queries. See census on PyPI.\ncensusdis: Provides richer functionality: automatic discovery of variables, geographies, and datasets. Helpful if you don’t want to manually look up variable codes. See censusdis on PyPI.\nus: Often used alongside census libraries to handle U.S. state and territory information (e.g., FIPS codes). See us on PyPI.\n\n\n\n6.3.2 Zip-Code Level for NYC Crash Data\nNow that we have NYC crash data, we might want to analyze patterns at the zip-code level to understand whether certain demographic or economic factors correlate with traffic incidents. While the crash dataset provides details about individual accidents, such as location, time, and severity, it does not contain contextual information about the neighborhoods where these crashes occur.\nTo perform meaningful zip-code-level analysis, we need additional data sources that provide relevant demographic, economic, and geographic variables. For example, understanding whether high-income areas experience fewer accidents, or whether population density influences crash frequency, requires integrating Census data. Key variables such as population size, median household income, employment rate, and population density can provide valuable context for interpreting crash trends across different zip codes.\nSince the Census Bureau provides detailed estimates for these variables at the zip-code level, we can use the Census API or other tools to retrieve relevant data and merge it with the NYC crash dataset. To access the Census API, you need an API key, which is free and easy to obtain. Visit the Census API Request page and submit your email address to receive a key. Once you have the key, you must include it in your API requests to access Census data. The following demonstration assumes that you have registered, obtained your API key, and saved it in a file called censusAPIkey.txt.\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nimport io\n\napi_key = open(\"censusAPIkey.txt\").read().strip()\nc = Census(api_key)\n\nSuppose that we want to get some basic info from ACS data of the year of 2024 for all the NYC zip codes. The variable names can be found in the ACS variable documentation.\n\nACS_YEAR = 2024\nACS_DATASET = \"acs/acs5\"\n\n# Important ACS variables (including land area for density calculation)\nACS_VARIABLES = {\n    \"B01003_001E\": \"Total Population\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B02001_002E\": \"White Population\",\n    \"B02001_003E\": \"Black Population\",\n    \"B02001_005E\": \"Asian Population\",\n    \"B15003_022E\": \"Bachelor’s Degree Holders\",\n    \"B15003_025E\": \"Graduate Degree Holders\",\n    \"B23025_002E\": \"Labor Force\",\n    \"B23025_005E\": \"Unemployed\",\n    \"B25077_001E\": \"Median Home Value\"\n}\n\n# Convert set to list of strings\nmerged_zips = list(map(str, merged_zips))\n\nLet’s set up the query to request the ACS data, and process the returned data.\n\nacs_data = c.acs5.get(\n    list(ACS_VARIABLES.keys()), \n    {'for': f'zip code tabulation area:{\",\".join(merged_zips)}'}\n    )\n\n# Convert to DataFrame\ndf_acs = pd.DataFrame(acs_data)\n\n# Rename columns\ndf_acs.rename(columns=ACS_VARIABLES, inplace=True)\ndf_acs.rename(columns={\"zip code tabulation area\": \"ZIP Code\"}, inplace=True)\n\nWe could save the ACS data df_acs in feather format (see next Section).\n\ndf_acs.to_feather(\"data/acs2023.feather\")\n\nThe population density could be an important factor for crash likelihood. To obtain the population densities, we need the areas of the zip codes. The shape files can be obtained from NYC Open Data.\n\nimport requests\nimport zipfile\nimport geopandas as gpd\n\n# Define the NYC MODZCTA shapefile URL and extraction directory\nshapefile_url = \"https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile\"\nextract_dir = \"tmp/MODZCTA_Shapefile\"\n\n# Create the directory if it doesn't exist\nos.makedirs(extract_dir, exist_ok=True)\n\n# Step 1: Download and extract the shapefile\nprint(\"Downloading MODZCTA shapefile...\")\nresponse = requests.get(shapefile_url)\nwith zipfile.ZipFile(io.BytesIO(response.content), \"r\") as z:\n    z.extractall(extract_dir)\n\nprint(f\"Shapefile extracted to: {extract_dir}\")\n\nDownloading MODZCTA shapefile...\nShapefile extracted to: tmp/MODZCTA_Shapefile\n\n\nNow we process the shape file to calculate the areas of the polygons.\n\n# Step 2: Automatically detect the correct .shp file\nshapefile_path = None\nfor file in os.listdir(extract_dir):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join(extract_dir, file)\n        break  # Use the first .shp file found\n\nif not shapefile_path:\n    raise FileNotFoundError(\"No .shp file found in extracted directory.\")\n\nprint(f\"Using shapefile: {shapefile_path}\")\n\n# Step 3: Load the shapefile into GeoPandas\ngdf = gpd.read_file(shapefile_path)\n\n# Step 4: Convert to CRS with meters for accurate area calculation\ngdf = gdf.to_crs(epsg=3857)\n\n# Step 5: Compute land area in square miles\ngdf['land_area_sq_miles'] = gdf['geometry'].area / 2_589_988.11\n# 1 square mile = 2,589,988.11 square meters\n\nprint(gdf[['modzcta', 'land_area_sq_miles']].head())\n\nUsing shapefile: tmp/MODZCTA_Shapefile/geo_export_fc839ccd-10c1-440c-b689-2b3d894b1f7a.shp\n  modzcta  land_area_sq_miles\n0   10001            1.153516\n1   10002            1.534509\n2   10003            1.008318\n3   10026            0.581848\n4   10004            0.256876\n\n\nLet’s export this data frame for future usage in feather format (see next Section).\n\ngdf[['modzcta', 'land_area_sq_miles']].to_feather('data/nyc_zip_areas.feather')\n\nNow we are ready to merge the two data frames.\n\n# Merge ACS data (`df_acs`) directly with MODZCTA land area (`gdf`)\ngdf = gdf.merge(df_acs, left_on='modzcta', right_on='ZIP Code', how='left')\n\n# Calculate Population Density (people per square mile)\ngdf['popdensity_per_sq_mile'] = (\n    gdf['Total Population'] / gdf['land_area_sq_miles']\n    )\n\n# Display first few rows\nprint(gdf[['modzcta', 'Total Population', 'land_area_sq_miles',\n    'popdensity_per_sq_mile']].head())\n\n  modzcta  Total Population  land_area_sq_miles  popdensity_per_sq_mile\n0   10001           29079.0            1.153516            25209.019713\n1   10002           75517.0            1.534509            49212.471465\n2   10003           53825.0            1.008318            53380.992071\n3   10026           37113.0            0.581848            63784.749994\n4   10004            3875.0            0.256876            15085.082190\n\n\nSome visualization of population density.\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Set up figure and axis\nfig, ax = plt.subplots(figsize=(10, 12))\n\n# Plot the choropleth map\ngdf.plot(column='popdensity_per_sq_mile', \n         cmap='viridis',  # Use a visually appealing color map\n         linewidth=0.8, \n         edgecolor='black',\n         legend=True,\n         legend_kwds={'label': \"Population Density (per sq mile)\",\n             'orientation': \"horizontal\"},\n         ax=ax)\n\n# Add a title\nax.set_title(\"Population Density by ZCTA in NYC\", fontsize=14)\n\n# Remove axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#cross-platform-data-format-arrow",
    "href": "06-manipulation.html#cross-platform-data-format-arrow",
    "title": "6  Data Manipulation",
    "section": "6.4 Cross-platform Data Format Arrow",
    "text": "6.4 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\nThe following code processes the raw data in CSV format and write out in Arrow format.\n\n# File paths\ncsv_file = 'data/nyc_crashes_lbdwk_2025.csv'\nfeather_file = 'tmp/nyc_crashes_lbdwk_2025.feather'\n\nimport pandas as pd\n\n# Move 'crash_datetime' to the first column\ndf = df[['crash_datetime'] + df.drop(columns=['crash_datetime']).columns.tolist()]\n\ndf['zip_code'] = df['zip_code'].astype(str).str.rstrip('.0')\n\ndf = df.sort_values(by='crash_datetime')\n\ndf.to_feather(feather_file)\n\nLet’s compare the file sizes of the feather format and the CSV format.\n\nimport os\n\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nCSV file size: 0.27 MB\nFeather file size: 0.13 MB\n\n\nRead the feather file back in:\n#| eval: false\ndff = pd.read_feather(feather_file)\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#database-operation-with-sql",
    "href": "06-manipulation.html#database-operation-with-sql",
    "title": "6  Data Manipulation",
    "section": "6.5 Database operation with SQL",
    "text": "6.5 Database operation with SQL\nThis section was prepared by Lewis Milun.\n\n6.5.1 Introduction\nStructured Query Language (SQL) is one of the standard languages that is used to work with large databases. It uses tables to store and display data, creating an organized and comprehensible interface that makes it far easier to track and view your data.\nSome advantages of using SQL include its ability to handle large amounts of data while simultaneously simplifying the process of creating, updating, and retrieving any data you may want.\nThe biggest advantage of using SQL for the purposes of this class is that it can very easily connect with Python and R. This makes it so that we can have all of the benefits of working with SQL while still working in the Python environment we already have set up.\n\n\n6.5.2 Setting up Databases\nIn this section, we will be working with two databases, one that’s built into a Python package (nycflights13) and one that we used for our midterm project (311 Service Requests)\n\n6.5.2.1 nycflights13\nThis database uses the pandas package and includes flight data for all flights that left New York City airports in 2013. The database includes several tables including ones that detail each flight, airline, airport, and much more info regarding the flights. The data in this database is contained across several tables. Data stored like this would typically be irritating to deal with however it is proven simple when working with SQL.\nTo set up this database, we need to import the proper packages:\n\nimport sqlite3\nimport pandas as pd\nfrom nycflights13 import flights, airlines, airports\n\nHere we imported “sqlite3” which is the package to import when working with SQL. We also imported “pandas” which also includes the nycflights13 database and from there imported the three tables we will be working with.\nNow, we want to establish the connection between Python and our SQL database:\n\nnycconn = sqlite3.connect(\"nycflights13.db\")\n\nThis snippet both creates the nycflights13.db database and establishes nycconn as our connection to this database.\nNext step is to add the three tables we imported from nycflights13 to the database:\n\nflights.to_sql(\"flights\", nycconn, if_exists=\"replace\", index=False)\nairlines.to_sql(\"airlines\", nycconn, if_exists=\"replace\", index=False)\nairports.to_sql(\"airports\", nycconn, if_exists=\"replace\", index=False)\n\nWhat this does is converts the three pandas dataframes into tables in the SQL database. The if_exists argument handles what would happen if there is already a table in the database with the same name. The index argument is determining whether or not the first column of the dataframe should be handled as the index of the table in the dataframe.\n\n\n6.5.2.2 serviceRequests Database\nFirst, we need to import our serviceRequests data from the csv in the data folder:\n\ndf = pd.read_csv(\"data/serviceRequests06-09-2025_07-05-2025.csv\")\n\nNow we need to repeat the process from nycflights13 but for our serviceRequests:\n\nsrconn = sqlite3.connect(\"serviceRequests.db\")\n\ndf.to_sql(\"requests\", srconn, if_exists=\"replace\", index=False)\n\nHere we imported the dataframe we got from the csv file as the only table in the serviceRequests.db database.\n\n\n\n6.5.3 Query Basics\nNow we can move onto working with the data in our SQL databases. The most common use for SQL is writing a “query” which is a statement sent to the SQL database that returns a selection of rows and columns from the tables in a database.\nWe will be looking at the “flights” table in our nycflights13 database for this section.\n\n6.5.3.1 SELECT and FROM\nThe following is the most basic possible query one can perform:\n\nquery = \"\"\"\nSELECT *\nFROM flights;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n0\n2013\n1\n1\n517.0\n515\n2.0\n830.0\n819\n11.0\nUA\n1545\nN14228\nEWR\nIAH\n227.0\n1400\n5\n15\n2013-01-01T10:00:00Z\n\n\n1\n2013\n1\n1\n533.0\n529\n4.0\n850.0\n830\n20.0\nUA\n1714\nN24211\nLGA\nIAH\n227.0\n1416\n5\n29\n2013-01-01T10:00:00Z\n\n\n2\n2013\n1\n1\n542.0\n540\n2.0\n923.0\n850\n33.0\nAA\n1141\nN619AA\nJFK\nMIA\n160.0\n1089\n5\n40\n2013-01-01T10:00:00Z\n\n\n3\n2013\n1\n1\n544.0\n545\n-1.0\n1004.0\n1022\n-18.0\nB6\n725\nN804JB\nJFK\nBQN\n183.0\n1576\n5\n45\n2013-01-01T10:00:00Z\n\n\n4\n2013\n1\n1\n554.0\n600\n-6.0\n812.0\n837\n-25.0\nDL\n461\nN668DN\nLGA\nATL\n116.0\n762\n6\n0\n2013-01-01T11:00:00Z\n\n\n\n\n\n\n\nThis snippet represents the basic form of writing SQL queries in Python. We create a variable ‘query’ that contains the statement we intend to pass into SQL. The last line then uses the nycconn connection we created earlier to pass our query into SQL and it returns the head() of the result we get back.\nThis query is the most basic query, as it returns the entire flights table. The SELECT line of the query is where you put the names of which columns you want from your table. You specify which table you want to work with on the FROM line. We put “*” in our SELECT line which returns all columns. All SQL queries must end with “;”, otherwise you’ll get an error.\nNow let’s try to simplify what we’re seeing by only looking at the origins and destinations of the flights:\n\nquery = \"\"\"\nSELECT origin, dest\nFROM flights;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nIAH\n\n\n1\nLGA\nIAH\n\n\n2\nJFK\nMIA\n\n\n3\nJFK\nBQN\n\n\n4\nLGA\nATL\n\n\n\n\n\n\n\nHere we replaced the “*” in our SELECT statement with “origin, dest” This told SQL to only return those two columns from the database.\n\n\n6.5.3.2 ORDER BY\nThese columns are quite messy to look at so let’s try sorting them by origin:\n\nquery = \"\"\"\nSELECT origin, dest\nFROM flights\nORDER BY origin;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nIAH\n\n\n1\nEWR\nORD\n\n\n2\nEWR\nFLL\n\n\n3\nEWR\nSFO\n\n\n4\nEWR\nLAS\n\n\n\n\n\n\n\nHere we added a new “ORDER BY” line. This line tells SQL what columns to sort the list by.\nYou can sort by multiple columns just by listing them with commas in between:\n\nquery = \"\"\"\nSELECT origin, dest\nFROM flights\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nALB\n\n\n1\nEWR\nALB\n\n\n2\nEWR\nALB\n\n\n3\nEWR\nALB\n\n\n4\nEWR\nALB\n\n\n\n\n\n\n\n\n\n6.5.3.3 SELECT DISTINCT\nNow that the list is properly sorted, we can see that there are multiple flights from each origin to destination combination. If we want to only see the unique columns, we can do this:\n\nquery = \"\"\"\nSELECT DISTINCT origin, dest\nFROM flights\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nALB\n\n\n1\nEWR\nANC\n\n\n2\nEWR\nATL\n\n\n3\nEWR\nAUS\n\n\n4\nEWR\nAVL\n\n\n\n\n\n\n\nHere we replaced our “SELECT” statement with a “SELECT DISTINCT” statement. This tells SQL to only return the unique columns from the query.\n\n\n\n6.5.4 Conditionals\nUsually you wouldn’t want to just return all rows or all unique rows from a table You instead will have conditions that determine which rows are relevant to your query\n\n6.5.4.1 WHERE\nThe way you can add conditionals to your query is by adding a “WHERE” line. Let’s take the same list from the last section and filter it so that only the flights that departed from LGA are returned:\n\nquery = \"\"\"\nSELECT DISTINCT origin, dest\nFROM flights\nWHERE origin = 'LGA'\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nLGA\nATL\n\n\n1\nLGA\nAVL\n\n\n2\nLGA\nBGR\n\n\n3\nLGA\nBHM\n\n\n4\nLGA\nBNA\n\n\n\n\n\n\n\nYou add conditionals in the “WHERE” line by using the following comparators:\n\n‘=’\n‘&lt;’\n‘&gt;’\n‘&lt;=’\n‘&gt;=’\n‘!=’\n\nBe careful of the type of the data in a particular column!\n\n\n6.5.4.2 AND, OR, and NOT\nYou can add multiple conditionals by using AND, OR, and parentheses\n\nquery = \"\"\"\nSELECT origin, dest\nFROM flights\nWHERE origin = 'LGA' AND dest = 'ATL'\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nLGA\nATL\n\n\n1\nLGA\nATL\n\n\n2\nLGA\nATL\n\n\n3\nLGA\nATL\n\n\n4\nLGA\nATL\n\n\n\n\n\n\n\nThis uses AND to return all flights that departed from LGA and arrived in ATL\nWe can also use OR to return all flights that either departed from LGA or arrived in ATL:\n\nquery = \"\"\"\nSELECT DISTINCT origin, dest\nFROM flights\nWHERE origin = 'LGA' OR dest = 'ATL'\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nATL\n\n\n1\nJFK\nATL\n\n\n2\nLGA\nATL\n\n\n3\nLGA\nAVL\n\n\n4\nLGA\nBGR\n\n\n\n\n\n\n\nWhen you want to get more complicated with your conditionals, parenthesis can be used to ensure SQL is correctly mixing the AND and OR statements.\nUse NOT to return the opposite of a statement:\n\nquery = \"\"\"\nSELECT DISTINCT origin, dest\nFROM flights\nWHERE NOT (origin = 'LGA' OR dest = 'ATL')\nORDER BY origin, dest;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nALB\n\n\n1\nEWR\nANC\n\n\n2\nEWR\nAUS\n\n\n3\nEWR\nAVL\n\n\n4\nEWR\nBDL\n\n\n\n\n\n\n\n\n\n6.5.4.3 COUNT\nAn easy way in SQL to see the total number of rows that fit the conditions you’ve specified:\n\nquery = \"\"\"\nSELECT COUNT(DISTINCT dest)\nFROM flights\nWHERE NOT (origin = 'LGA' OR dest = 'ATL');\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nCOUNT(DISTINCT dest)\n\n\n\n\n0\n97\n\n\n\n\n\n\n\nThis returns the number of distinct destinations that fit the specified criteria\n\n\n6.5.4.4 LIMIT\nIf you don’t want to use .head() to only display the first few rows, this can be done in SQL using a LIMIT statement:\n\nquery = \"\"\"\nSELECT DISTINCT carrier, origin, dest\nFROM flights\nWHERE carrier = 'UA'\nORDER BY origin, dest\nLIMIT 5;\n\"\"\"\npd.read_sql_query(query, nycconn)\n\n\n\n\n\n\n\n\ncarrier\norigin\ndest\n\n\n\n\n0\nUA\nEWR\nANC\n\n\n1\nUA\nEWR\nATL\n\n\n2\nUA\nEWR\nAUS\n\n\n3\nUA\nEWR\nBDL\n\n\n4\nUA\nEWR\nBOS\n\n\n\n\n\n\n\nThis query added the “carrier” column that displays the airline that held the flight. We specified that we want all flights from the “UA” airline and limited the result to 5 rows.\nAn interesting thing you can do with conditionals in SQL is to filter by values in a column that you are not displaying:\n\nquery = \"\"\"\nSELECT DISTINCT origin, dest\nFROM flights\nWHERE carrier = 'UA'\nORDER BY origin, dest\nLIMIT 5;\n\"\"\"\npd.read_sql_query(query, nycconn)\n\n\n\n\n\n\n\n\norigin\ndest\n\n\n\n\n0\nEWR\nANC\n\n\n1\nEWR\nATL\n\n\n2\nEWR\nAUS\n\n\n3\nEWR\nBDL\n\n\n4\nEWR\nBOS\n\n\n\n\n\n\n\nHere we still filter by flights from the “UA” airline but we don’t display the column as that would be very redundant.\n\n\n\n6.5.5 Joins\nThe last query that we did was useful, however it isn’t realistic to expect users to memorize the two-digit codes for all airlines. Thankfully, there is the airlines table in our nycflights13.db database. Let’s take a look at it:\n\nquery = \"\"\"\nSELECT *\nFROM airlines;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nThis table is much simpler than the “flights” table as it only has two columns.\n\n6.5.5.1 INNER JOIN\nLogically, we wouldn’t want our outputted table to display the two-digit code that represents each airline but instead we’d want to see the name of the airline. Thankfully, SQL has a way to join the data from the two tables together:\n\nquery = \"\"\"\nSELECT DISTINCT a.name AS airline_name, f.origin, f.dest\nFROM flights AS f\nINNER JOIN airlines AS a\nON f.carrier = a.carrier;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nairline_name\norigin\ndest\n\n\n\n\n0\nUnited Air Lines Inc.\nEWR\nIAH\n\n\n1\nUnited Air Lines Inc.\nLGA\nIAH\n\n\n2\nAmerican Airlines Inc.\nJFK\nMIA\n\n\n3\nJetBlue Airways\nJFK\nBQN\n\n\n4\nDelta Air Lines Inc.\nLGA\nATL\n\n\n\n\n\n\n\nThis is a much more complicated query that returns a table of the distinct airline, origin, and destination in the flights database. We introduced three new statements in this query:\nAS is similar to how “as” is used when importing packages in Python. It gives us an opportunity to use a shorthand instead of needing to type out the full table names every time we mention a column. It can also be used in our SELECT line to name columns in our resultant table\nINNER JOIN connects a new table to our query (in this case, the new table is airlines).\nON lets SQL know what column in each table it should use to connect the tables. Here we told SQL that in every row, when it reaches the carrier column in flights, it should use that as its reference for what row in airlines to use for values in this row. For example, in the first row, SQL saw that the carrier name in flights was “UA”. SQL then looked in the carrier column in airlines and found the row in which “UA” was the value in that table’s carrier column. So when SQL was calculating the value for airline_name in the first row, it knew which column to search in airline to find “United Air Lines Inc.”\nJOIN statements are key when using SQL to display data. This is what allows SQL databases to be in such nice and concise structures.\nNow let’s also add in the values from the “airports” table so that we get the full names of the airports instead of their three-digit codes:\n\nquery = \"\"\"\nSELECT DISTINCT a.name AS airline_name,\n       orig.name AS origin_name,\n       dest.name AS dest_name\nFROM flights AS f\nINNER JOIN airlines AS a\n    ON f.carrier = a.carrier\nINNER JOIN airports AS orig\n    ON f.origin = orig.faa\nINNER JOIN airports AS dest\n    ON f.dest = dest.faa;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nairline_name\norigin_name\ndest_name\n\n\n\n\n0\nUnited Air Lines Inc.\nNewark Liberty Intl\nGeorge Bush Intercontinental\n\n\n1\nUnited Air Lines Inc.\nLa Guardia\nGeorge Bush Intercontinental\n\n\n2\nAmerican Airlines Inc.\nJohn F Kennedy Intl\nMiami Intl\n\n\n3\nDelta Air Lines Inc.\nLa Guardia\nHartsfield Jackson Atlanta Intl\n\n\n4\nUnited Air Lines Inc.\nNewark Liberty Intl\nChicago Ohare Intl\n\n\n\n\n\n\n\nHere we use three different INNER JOIN statements to connect the three tables properly. The reason we use two JOIN statements to connect the same “airports” table is to have SQL be able to look separately for the origin and destination airport names. This query also utilizes indenting to make the query far more readable.\n\n\n6.5.5.2 The difference between the three JOINs\nSQL has three different versions of JOIN statements: + INNER JOIN + LEFT JOIN + RIGHT JOIN\nThis allows the user to determine what rows get included as a result of joining two tables. Let’s take a look at the differences:\n\nquery = \"\"\"\nSELECT DISTINCT f.flight,\n       a.name AS airline_name,\n       orig.name AS origin_name,\n       dest.name AS dest_name\nFROM flights AS f\nINNER JOIN airlines AS a\n    ON f.carrier = a.carrier\nINNER JOIN airports AS orig\n    ON f.origin = orig.faa\nINNER JOIN airports AS dest\n    ON f.dest = dest.faa;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nflight\nairline_name\norigin_name\ndest_name\n\n\n\n\n0\n1545\nUnited Air Lines Inc.\nNewark Liberty Intl\nGeorge Bush Intercontinental\n\n\n1\n1714\nUnited Air Lines Inc.\nLa Guardia\nGeorge Bush Intercontinental\n\n\n2\n1141\nAmerican Airlines Inc.\nJohn F Kennedy Intl\nMiami Intl\n\n\n3\n461\nDelta Air Lines Inc.\nLa Guardia\nHartsfield Jackson Atlanta Intl\n\n\n4\n1696\nUnited Air Lines Inc.\nNewark Liberty Intl\nChicago Ohare Intl\n\n\n\n\n\n\n\nINNER JOIN is the most common. This is because the result will only include rows in which the values in question are included in both tables. For example, if there was a row in flights that included an airline code that was not present in airlines, then SQL will not include that row in the result.\n\nquery = \"\"\"\nSELECT DISTINCT f.flight,\n    a.name AS airline_name,\n    orig.name AS origin_name,\n    dest.name AS dest_name\nFROM flights AS f\nLEFT JOIN airlines AS a\n    ON f.carrier = a.carrier\nLEFT JOIN airports AS orig\n    ON f.origin = orig.faa\nLEFT JOIN airports AS dest\n    ON f.dest = dest.faa;\n\"\"\"\npd.read_sql_query(query, nycconn).head()\n\n\n\n\n\n\n\n\nflight\nairline_name\norigin_name\ndest_name\n\n\n\n\n0\n1545\nUnited Air Lines Inc.\nNewark Liberty Intl\nGeorge Bush Intercontinental\n\n\n1\n1714\nUnited Air Lines Inc.\nLa Guardia\nGeorge Bush Intercontinental\n\n\n2\n1141\nAmerican Airlines Inc.\nJohn F Kennedy Intl\nMiami Intl\n\n\n3\n725\nJetBlue Airways\nJohn F Kennedy Intl\nNone\n\n\n4\n461\nDelta Air Lines Inc.\nLa Guardia\nHartsfield Jackson Atlanta Intl\n\n\n\n\n\n\n\nLEFT JOIN will return all rows that are in flights regardless of if SQL was able to find matching rows on the other tables. This is perfectly represented in the fourth row of the output. On flights, the destination for flight 725 is “BQN” which is not an airport on the airports table. If you look back to the result of the INNER JOIN, this row was removed from the result however it is present here because we used flights as our only reference for rows and therefore the row is included but the destination name is left as “None”\n\nquery = \"\"\"\nSELECT DISTINCT f.flight,\n    f.origin,\n    dest.name AS dest_name\nFROM flights AS f\nRIGHT JOIN airports AS dest\n    ON f.dest = dest.faa;\n\"\"\"\npd.read_sql_query(query, nycconn)\n\n\n\n\n\n\n\n\nflight\norigin\ndest_name\n\n\n\n\n0\n1545.0\nEWR\nGeorge Bush Intercontinental\n\n\n1\n1714.0\nLGA\nGeorge Bush Intercontinental\n\n\n2\n1141.0\nJFK\nMiami Intl\n\n\n3\n461.0\nLGA\nHartsfield Jackson Atlanta Intl\n\n\n4\n1696.0\nEWR\nChicago Ohare Intl\n\n\n...\n...\n...\n...\n\n\n13275\nNaN\nNone\nBoston Back Bay Station\n\n\n13276\nNaN\nNone\nBlack Rock\n\n\n13277\nNaN\nNone\nNew Haven Rail Station\n\n\n13278\nNaN\nNone\nWilmington Amtrak Station\n\n\n13279\nNaN\nNone\nWashington Union Station\n\n\n\n\n13280 rows × 3 columns\n\n\n\nRIGHT JOIN is basically the opposite of LEFT JOIN. Instead of using flights as its basis for what rows to include, RIGHT JOIN uses the joined tables instead. As you can see, the fourth row is being skipped again because the airports table is being used as reference instead. Also, the table ends with rows for all destinations from the airports table. SQL includes them because it is looking for all rows that include destinations from the airports table. This includes all rows from the airports table itself.\n\n\n\n6.5.6 Creating Tables\nLet’s take a look at our serviceRequests.db’s requests table:\n\nquery = \"\"\"\nSELECT *\nFROM requests;\n\"\"\"\npd.read_sql_query(query, srconn).head()\n\n\n\n\n\n\n\n\nUnique Key\nCreated Date\nClosed Date\nAgency\nAgency Name\nComplaint Type\nDescriptor\nLocation Type\nIncident Zip\nIncident Address\n...\nVehicle Type\nTaxi Company Borough\nTaxi Pick Up Location\nBridge Highway Name\nBridge Highway Direction\nRoad Ramp\nBridge Highway Segment\nLatitude\nLongitude\nLocation\n\n\n\n\n0\n65477840\n07/05/2025 11:59:51 AM\nNone\nEDC\nEconomic Development Corporation\nNoise - Helicopter\nOther\nAbove Address\n11414.0\n88-12 151 AVENUE\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n40.668478\n-73.846874\n(40.6684777987759, -73.84687384879807)\n\n\n1\n65473303\n07/05/2025 11:59:38 AM\n07/05/2025 12:53:29 PM\nNYPD\nNew York City Police Department\nBlocked Driveway\nPartial Access\nStreet/Sidewalk\n11207.0\n737 CHAUNCEY STREET\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n40.685343\n-73.907410\n(40.685343220141434, -73.90741025722555)\n\n\n2\n65479182\n07/05/2025 11:59:30 AM\n07/05/2025 07:24:47 PM\nDHS\nDepartment of Homeless Services\nHomeless Person Assistance\nChronic\nStore/Commercial\n10014.0\n11 LITTLE WEST 12 STREET\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n40.739795\n-74.006708\n(40.739795176046364, -74.00670839867178)\n\n\n3\n65472680\n07/05/2025 11:59:27 AM\n07/05/2025 02:28:38 PM\nNYPD\nNew York City Police Department\nAbandoned Vehicle\nWith License Plate\nStreet/Sidewalk\n10039.0\n2743 FREDERICK DOUGLASS BOULEVARD\n...\nCar\nNone\nNone\nNone\nNone\nNone\nNone\n40.823646\n-73.941340\n(40.82364581471349, -73.94134045287531)\n\n\n4\n65479165\n07/05/2025 11:59:20 AM\n07/06/2025 01:13:50 AM\nNYPD\nNew York City Police Department\nEncampment\nNone\nResidential Building/House\n10456.0\n1398 GRAND CONCOURSE\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n40.838557\n-73.913883\n(40.83855707927812, -73.91388286014495)\n\n\n\n\n5 rows × 41 columns\n\n\n\nThis table is incredibly complicated and redundant through repeated information such as each row having both “Agency” and “Agency Name”. This redundancy clutters the table, making it difficult to read. It would be much better if the database wasn’t just one table and instead was formatted like the nycflights13.db\nTake note that we have begun using our other connection (srconn instead of nycconn) because we are working with our other database.\nThankfully, SQL makes it very simple to create new tables from the results of a query. Looking back at the redundancy with agencies, we can create the following query to see all the agency codes and their corresponding names:\n\nquery = \"\"\"\nSELECT DISTINCT Agency, \"Agency Name\"\nFROM requests\nORDER BY Agency;\n\"\"\"\npd.read_sql_query(query, srconn).head()\n\n\n\n\n\n\n\n\nAgency\nAgency Name\n\n\n\n\n0\nDCWP\nDepartment of Consumer and Worker Protection\n\n\n1\nDEP\nDepartment of Environmental Protection\n\n\n2\nDHS\nDepartment of Homeless Services\n\n\n3\nDOB\nDepartment of Buildings\n\n\n4\nDOE\nDepartment of Education\n\n\n\n\n\n\n\nSomething to note here is how one can handle a column name that includes a space or a comma. If you put the name of the column in quotes, then SQL will handle everything inside the quotes as the name of the column.\nNow that we have created this query, we can make the result into its own table in the database:\n\nquery = \"\"\"\nCREATE TABLE IF NOT EXISTS agencies AS\nSELECT DISTINCT Agency, \"Agency Name\"\nFROM requests\nORDER BY Agency;\n\"\"\"\nsrconn.execute(query)\nsrconn.commit()\n\nThere are a few key parts to this query:\nFirstly, we have the first line which tells SQL to create a table named agencies and the AS works here to tell SQL to make the table be the result of the query that follows the first line.\nSecond, there is the statement “IF NOT EXISTS”. This is incredibly useful to include in the query as it ensures that you do not override any tables that you have previously made.\nLastly, we use different commands outside of creating the query variable. Instead of pd.read_sql_query(query, srconn), we have our connection to SQL execute the query we created, then commit the changes to the database.\nNow that we have added the table to our database, we can query it:\n\nquery = \"\"\"\nSELECT *\nFROM agencies;\n\"\"\"\npd.read_sql_query(query, srconn).head()\n\n\n\n\n\n\n\n\nAgency\nAgency Name\n\n\n\n\n0\nDCWP\nDepartment of Consumer and Worker Protection\n\n\n1\nDEP\nDepartment of Environmental Protection\n\n\n2\nDHS\nDepartment of Homeless Services\n\n\n3\nDOB\nDepartment of Buildings\n\n\n4\nDOE\nDepartment of Education\n\n\n\n\n\n\n\nThis process can be repeated to clean up messy tables and messy databases.\n\n\n6.5.7 Statement Order\nStatements in a SQL query must go in a certain order, otherwise the query will return an error. The order is as follows:\n\nSELECT\nFROM\nJOIN\nWHERE\nGROUP BY\nHAVING\nORDER BY\nLIMIT/OFFSET\n\n\n\n6.5.8 Conclusion\nSQL makes working with large datasets much easier by organizing databases and simplifying the process of displaying data. Using either the queries shown here as well as much more complicated queries, one can turn complex tables into databases that don’t unnecessarily repeat data, and that consist of easily read tables.\n\n\n6.5.9 Further Readings\n\nW3 Schools SQL Tutorial\n[SQLite Tutorial (installing SQLite not necessary)] (https://www.sqlitetutorial.net/)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-exploration.html",
    "href": "07-exploration.html",
    "title": "7  Data Exploration",
    "section": "",
    "text": "7.1 Introduction\nData exploration is the disciplined process that connects raw records to credible analysis. Its goals are to identify data quality issues, understand what variables mean operationally, and generate falsifiable claims that later analysis can scrutinize. The work is iterative: initial checks surface inconsistencies or gaps; targeted cleaning follows; then renewed examination tests whether earlier conclusions still hold. Reproducibility is non-negotiable, so every step should live in code with a brief log explaining what changed and why. Critically, this phase is not confirmatory inference. Instead, it frames questions clearly, proposes measurable definitions, and records assumptions that later sections will test formally. Scope of this chapter.\nWe will develop practical habits for high-quality exploration. First, we present cleaning principles: consistency of formats and units, completeness and missing-data mechanisms, accuracy and duplicates, and integrity across related fields, together with clear documentation. Next, we practice numerically driven summaries: distributional statistics, grouped descriptives, cross-tabulations, and simple association checks that reveal promising relationships. Finally, we show how to state hypotheses correctly—with the null representing no effect or independence—and run appropriate tests in Python (ANOVA or Kruskal–Wallis for group means, Welch’s t-test or Mann–Whitney for two groups, OLS slope tests with robust errors, and Pearson or Spearman correlations), pairing p-values with effect sizes and intervals.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#getting-to-know-data",
    "href": "07-exploration.html#getting-to-know-data",
    "title": "7  Data Exploration",
    "section": "7.2 Getting to Know Data",
    "text": "7.2 Getting to Know Data\nAny analysis begins by becoming familiar with the dataset. This involves learning what the observations represent, what types of variables are recorded, and whether the structure meets the expectations of tidy data. Before turning to a specific example, we highlight general principles.\n\nUnits of observation. Each row of a dataset should correspond to a single unit, such as an individual, transaction, or property sale. Misalignment of units often leads to errors in later analysis.\nVariables and their types. Columns record attributes of units. These may be continuous measurements, counts, ordered categories, or nominal labels. Recognizing the correct type is essential because it dictates which summary statistics and hypothesis tests are appropriate.\nTidy data principles. In a tidy format, each row is one observation and each column is one variable. When data are stored otherwise, reshaping is necessary before analysis can proceed smoothly.\n\nWe now illustrate these ideas using a reduced version of the Ames Housing dataset (De Cock, 2009), which is available directly from OpenML. With a filtered subset of ~1460 observations, it drops older sales, keeps only certain years, and removes some variables to make modeling cleaner for beginners.\n\nimport openml\n\n# Load Ames Housing (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\n\n# Basic dimensions\ndf.shape\n\n(1460, 81)\n\n\nThe dataset contains nearly three thousand house sales and eighty variables.\nIt is useful to view the first few rows to confirm the structure.\n\n# First few rows\ndf.head()\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNone\nReg\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNone\nReg\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\nEach row corresponds to one property sale, while columns record attributes such as lot size, neighborhood, and sale price.\nUnderstanding whether variables are numeric, categorical, or temporal guides later exploration and cleaning.\n\n# Dtypes in pandas\ndf.dtypes.value_counts()\n\nobject     43\nint64      22\nuint8      13\nfloat64     3\nName: count, dtype: int64\n\n\n\n# Example: show a few variables with types\ndf.dtypes.head(10)\n\nId               int64\nMSSubClass       uint8\nMSZoning        object\nLotFrontage    float64\nLotArea          int64\nStreet          object\nAlley           object\nLotShape        object\nLandContour     object\nUtilities       object\ndtype: object\n\n\nMost features are numeric or categorical. Some, such as YearBuilt, are integers but represent calendar years.\nThe outcome of interest is the sale price.\n\n# The default target from OpenML\ndataset.default_target_attribute\n\n'SalePrice'\n\n\nNumeric summaries highlight scale and possible outliers.\n\n# Summary statistics for numeric columns\ndf.describe().T.head(10)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nId\n1460.0\n730.500000\n421.610009\n1.0\n365.75\n730.5\n1095.25\n1460.0\n\n\nMSSubClass\n1460.0\n56.897260\n42.300571\n20.0\n20.00\n50.0\n70.00\n190.0\n\n\nLotFrontage\n1201.0\n70.049958\n24.284752\n21.0\n59.00\n69.0\n80.00\n313.0\n\n\nLotArea\n1460.0\n10516.828082\n9981.264932\n1300.0\n7553.50\n9478.5\n11601.50\n215245.0\n\n\nOverallQual\n1460.0\n6.099315\n1.382997\n1.0\n5.00\n6.0\n7.00\n10.0\n\n\nOverallCond\n1460.0\n5.575342\n1.112799\n1.0\n5.00\n5.0\n6.00\n9.0\n\n\nYearBuilt\n1460.0\n1971.267808\n30.202904\n1872.0\n1954.00\n1973.0\n2000.00\n2010.0\n\n\nYearRemodAdd\n1460.0\n1984.865753\n20.645407\n1950.0\n1967.00\n1994.0\n2004.00\n2010.0\n\n\nMasVnrArea\n1452.0\n103.685262\n181.066207\n0.0\n0.00\n0.0\n166.00\n1600.0\n\n\nBsmtFinSF1\n1460.0\n443.639726\n456.098091\n0.0\n0.00\n383.5\n712.25\n5644.0\n\n\n\n\n\n\n\nCategorical summaries reveal balance among levels.\n\n# Frequency counts for categorical columns\ndf['Neighborhood'].value_counts().head()\n\nNeighborhood\nNAmes      225\nCollgCr    150\nOldTown    113\nEdwards    100\nSomerst     86\nName: count, dtype: int64\n\n\n\n# Another example\ndf['GarageType'].value_counts(dropna=False)\n\nGarageType\nAttchd     870\nDetchd     387\nBuiltIn     88\nNone        81\nBasment     19\nCarPort      9\n2Types       6\nName: count, dtype: int64\n\n\nSimple checks can detect implausible combinations.\n\n# Houses should not be sold before built\n(df['YrSold'] &lt; df['YearBuilt']).sum()\n\n0\n\n\n\n# Garage year built should not precede house year built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\n9\n\n\nThese checks confirm that while the dataset is well curated, certain quirks require careful interpretation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#data-cleaning-principles",
    "href": "07-exploration.html#data-cleaning-principles",
    "title": "7  Data Exploration",
    "section": "7.3 Data Cleaning Principles",
    "text": "7.3 Data Cleaning Principles\nExploration begins with data cleaning. The purpose is not to modify values casually but to identify issues, understand their sources, and decide on a transparent response. The following principles provide structure.\nConsistency. Variables should follow the same format and unit across all records. Dates should have a common representation, categorical labels should not differ by spelling, and measurements should use the same scale.\nCompleteness. Missing values are unavoidable. It is important to determine whether they arise from data entry errors, survey nonresponse, or structural absence. For example, a missing value in FireplaceQu often indicates that a house has no fireplace rather than missing information.\nAccuracy. Values should be plausible. Obvious errors include negative square footage or sale years in the future. Duplicate records also fall under this category.\nIntegrity. Relationships between variables should be logically consistent. A house cannot be sold before it was built. If related totals exist, the sum of parts should match the total.\nTransparency. All cleaning decisions should be recorded. Reproducibility requires that another analyst can understand what was changed and why.\n\n7.3.1 Illustration with Ames Housing\nWe apply these principles to the Ames dataset. The first step is to inspect missing values.\n\n# Count missing values in each column\ndf.isna().sum().sort_values(ascending=False).head(15)\n\nPoolQC          1453\nMiscFeature     1406\nAlley           1369\nFence           1179\nFireplaceQu      690\nLotFrontage      259\nGarageYrBlt       81\nGarageCond        81\nGarageType        81\nGarageFinish      81\nGarageQual        81\nBsmtFinType2      38\nBsmtExposure      38\nBsmtQual          37\nBsmtCond          37\ndtype: int64\n\n\nSeveral variables, such as PoolQC, MiscFeature, and Alley, contain many missing entries. Documentation shows that these are structural, indicating the absence of the feature.\n\n# Example: check PoolQC against PoolArea\n(df['PoolQC'].isna() & (df['PoolArea'] &gt; 0)).sum()\n\n0\n\n\nThe result is zero, confirming that missing PoolQC means no pool.\nConsistency can be checked by reviewing categorical labels.\n\n# Distinct values in Exterior1st\ndf['Exterior1st'].unique()\n\narray(['VinylSd', 'MetalSd', 'Wd Sdng', 'HdBoard', 'BrkFace', 'WdShing',\n       'CemntBd', 'Plywood', 'AsbShng', 'Stucco', 'BrkComm', 'AsphShn',\n       'Stone', 'ImStucc', 'CBlock'], dtype=object)\n\n\nIf spelling variants are detected, they should be harmonized.\nAccuracy checks involve searching for implausible values.\n\n# Negative or zero living area\n(df['GrLivArea'] &lt;= 0).sum()\n\n0\n\n\nIntegrity checks verify logical relationships.\n\n# Houses sold before they were built\n(df['YrSold'] &lt; df['YearBuilt']).sum()\n\n0\n\n\n\n# Garage built before house built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\n9\n\n\nThese checks help identify issues to document and, if appropriate, correct in a reproducible way.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#common-exploration-practices",
    "href": "07-exploration.html#common-exploration-practices",
    "title": "7  Data Exploration",
    "section": "7.4 Common Exploration Practices",
    "text": "7.4 Common Exploration Practices\nAfter establishing data cleaning principles, the next step is to compute summaries that describe the distributions of variables and their relationships. This section avoids graphics, relying instead on tables and statistics.\n\n7.4.1 Univariate summaries\nSimple statistics reveal scale, central tendency, and variability.\n\n# Sale price distribution\ndf['SalePrice'].describe()\n\ncount      1460.000000\nmean     180921.195890\nstd       79442.502883\nmin       34900.000000\n25%      129975.000000\n50%      163000.000000\n75%      214000.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n\n\nThe sale price is right-skewed, with a mean higher than the median.\n\n# Lot area distribution\ndf['LotArea'].describe()\n\ncount      1460.000000\nmean      10516.828082\nstd        9981.264932\nmin        1300.000000\n25%        7553.500000\n50%        9478.500000\n75%       11601.500000\nmax      215245.000000\nName: LotArea, dtype: float64\n\n\nLot size shows extreme variation, indicating possible outliers.\n\n\n7.4.2 Grouped summaries\nComparisons across categories highlight differences in central tendency.\n\n# Mean sale price by neighborhood\ndf.groupby('Neighborhood')['SalePrice'].mean().sort_values().head()\n\nNeighborhood\nMeadowV     98576.470588\nIDOTRR     100123.783784\nBrDale     104493.750000\nBrkSide    124834.051724\nEdwards    128219.700000\nName: SalePrice, dtype: float64\n\n\nNeighborhoods differ substantially in average sale price.\n\n# Median sale price by overall quality\ndf.groupby('OverallQual')['SalePrice'].median()\n\nOverallQual\n1      50150.0\n2      60000.0\n3      86250.0\n4     108000.0\n5     133000.0\n6     160000.0\n7     200141.0\n8     269750.0\n9     345000.0\n10    432390.0\nName: SalePrice, dtype: float64\n\n\nHigher quality ratings correspond to higher prices.\n\n\n7.4.3 Cross-tabulations\nCross-tabulations summarize associations between categorical variables.\n\nimport pandas as pd\n# Neighborhood by garage type\npd.crosstab(df['Neighborhood'], df['GarageType']).head()\n\n\n\n\n\n\n\nGarageType\n2Types\nAttchd\nBasment\nBuiltIn\nCarPort\nDetchd\n\n\nNeighborhood\n\n\n\n\n\n\n\n\n\n\nBlmngtn\n0\n17\n0\n0\n0\n0\n\n\nBlueste\n0\n2\n0\n0\n0\n0\n\n\nBrDale\n0\n2\n0\n0\n0\n13\n\n\nBrkSide\n0\n3\n0\n1\n0\n44\n\n\nClearCr\n0\n24\n0\n1\n0\n2\n\n\n\n\n\n\n\nSome garage types are common only in specific neighborhoods.\n\n\n7.4.4 Correlations\nCorrelation coefficients capture linear associations between numeric variables.\n\n# Correlation between living area and sale price\ndf[['GrLivArea','SalePrice']].corr()\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\nGrLivArea\n1.000000\n0.708624\n\n\nSalePrice\n0.708624\n1.000000\n\n\n\n\n\n\n\n\n# Correlation between lot area and sale price\ndf[['LotArea','SalePrice']].corr()\n\n\n\n\n\n\n\n\nLotArea\nSalePrice\n\n\n\n\nLotArea\n1.000000\n0.263843\n\n\nSalePrice\n0.263843\n1.000000\n\n\n\n\n\n\n\nLiving area is strongly correlated with price, while lot area shows a weaker association.\n\n\n7.4.5 Conditional summaries\nExamining distributions within subgroups can surface interaction patterns.\n\n# Average sale price by house style\ndf.groupby('HouseStyle')['SalePrice'].mean().sort_values()\n\nHouseStyle\n1.5Unf    110150.000000\nSFoyer    135074.486486\n1.5Fin    143116.740260\n2.5Unf    157354.545455\nSLvl      166703.384615\n1Story    175985.477961\n2Story    210051.764045\n2.5Fin    220000.000000\nName: SalePrice, dtype: float64\n\n\nHouse style is another factor associated with variation in price.\nThese practices provide a numerical portrait of the data, guiding later steps where hypotheses will be stated explicitly and tested with appropriate statistical methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#forming-and-testing-hypotheses",
    "href": "07-exploration.html#forming-and-testing-hypotheses",
    "title": "7  Data Exploration",
    "section": "7.5 Forming and Testing Hypotheses",
    "text": "7.5 Forming and Testing Hypotheses\nExploration becomes more rigorous when we state hypotheses formally and run appropriate tests. The null hypothesis always represents no effect, no difference, or no association. The alternative expresses the presence of an effect. The examples below use the Ames Housing data.\n\n7.5.1 Neighborhood and sale price\nHypothesis: - H0: The mean sale prices are equal across neighborhoods. - H1: At least one neighborhood has a different mean.\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\n\nsub = df[['SalePrice','Neighborhood']].dropna()\nmodel = smf.ols('SalePrice ~ C(Neighborhood)', data=sub).fit()\nanova_lm(model, typ=2)\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(Neighborhood)\n5.023606e+12\n24.0\n71.784865\n1.558600e-225\n\n\nResidual\n4.184305e+12\n1435.0\nNaN\nNaN\n\n\n\n\n\n\n\nANOVA tests equality of group means. If significant, post-hoc comparisons can identify which neighborhoods differ.\n\n\n7.5.2 Year built and sale price\nHypothesis: - H0: The slope for YearBuilt is zero; no linear relationship. - H1: The slope is not zero.\n\nmodel = smf.ols('SalePrice ~ YearBuilt', data=df).fit(cov_type='HC3')\nmodel.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.53e+06\n1.36e+05\n-18.667\n0.000\n-2.8e+06\n-2.26e+06\n\n\nYearBuilt\n1375.3735\n68.973\n19.941\n0.000\n1240.189\n1510.558\n\n\n\n\n\nThe regression slope test checks whether newer houses tend to sell for more.\n\n\n7.5.3 Lot area and sale price\nHypothesis: - H0: The population correlation is zero. - H1: The correlation is not zero.\n\nfrom scipy import stats\nsub = df[['LotArea','SalePrice']].dropna()\nstats.pearsonr(sub['LotArea'], sub['SalePrice'])\n\nPearsonRResult(statistic=0.26384335387140573, pvalue=1.123139154918551e-24)\n\n\nA Pearson correlation tests linear association. A Spearman rank correlation can be used when distributions are skewed.\n\n\n7.5.4 Fireplaces and sale price\nHypothesis: - H0: The mean sale price is the same for houses with and without a fireplace. - H1: The mean sale prices differ.\n\nsub = df[['SalePrice','Fireplaces']].dropna()\nsub['has_fp'] = (sub['Fireplaces'] &gt; 0).astype(int)\n\ng1 = sub.loc[sub['has_fp']==1, 'SalePrice']\ng0 = sub.loc[sub['has_fp']==0, 'SalePrice']\n\nstats.ttest_ind(g1, g0, equal_var=False)\n\nTtestResult(statistic=21.105376324953664, pvalue=4.666259945494159e-84, df=1171.6295727321062)\n\n\nWelch’s t-test compares means when variances differ.\n\n\n7.5.5 Garage type and neighborhood\nHypothesis: - H0: Garage type and neighborhood are independent. - H1: Garage type and neighborhood are associated.\n\nimport pandas as pd\nct = pd.crosstab(df['GarageType'], df['Neighborhood'])\nstats.chi2_contingency(ct)\n\nChi2ContingencyResult(statistic=794.6871326886048, pvalue=5.179037846292559e-100, dof=120, expected_freq=array([[7.39666425e-02, 8.70195794e-03, 6.52646846e-02, 2.08846991e-01,\n        1.17476432e-01, 6.43944888e-01, 2.21899927e-01, 3.39376360e-01,\n        3.43727339e-01, 1.26178390e-01, 5.22117476e-02, 1.91443075e-01,\n        9.52864394e-01, 3.91588107e-02, 3.17621465e-01, 1.78390138e-01,\n        3.35025381e-01, 4.39448876e-01, 8.70195794e-02, 3.08919507e-01,\n        2.52356780e-01, 3.74184191e-01, 1.08774474e-01, 1.65337201e-01,\n        4.78607687e-02],\n       [1.07251632e+01, 1.26178390e+00, 9.46337926e+00, 3.02828136e+01,\n        1.70340827e+01, 9.33720087e+01, 3.21754895e+01, 4.92095722e+01,\n        4.98404641e+01, 1.82958666e+01, 7.57070341e+00, 2.77592458e+01,\n        1.38165337e+02, 5.67802756e+00, 4.60551124e+01, 2.58665700e+01,\n        4.85786802e+01, 6.37200870e+01, 1.26178390e+01, 4.47933285e+01,\n        3.65917331e+01, 5.42567078e+01, 1.57722988e+01, 2.39738941e+01,\n        6.93981146e+00],\n       [2.34227701e-01, 2.75562001e-02, 2.06671501e-01, 6.61348803e-01,\n        3.72008702e-01, 2.03915881e+00, 7.02683104e-01, 1.07469181e+00,\n        1.08846991e+00, 3.99564902e-01, 1.65337201e-01, 6.06236403e-01,\n        3.01740392e+00, 1.24002901e-01, 1.00580131e+00, 5.64902103e-01,\n        1.06091371e+00, 1.39158811e+00, 2.75562001e-01, 9.78245105e-01,\n        7.99129804e-01, 1.18491661e+00, 3.44452502e-01, 5.23567803e-01,\n        1.51559101e-01],\n       [1.08484409e+00, 1.27628716e-01, 9.57215373e-01, 3.06308920e+00,\n        1.72298767e+00, 9.44452502e+00, 3.25453227e+00, 4.97751994e+00,\n        5.04133430e+00, 1.85061639e+00, 7.65772299e-01, 2.80783176e+00,\n        1.39753445e+01, 5.74329224e-01, 4.65844815e+00, 2.61638869e+00,\n        4.91370558e+00, 6.44525018e+00, 1.27628716e+00, 4.53081943e+00,\n        3.70123278e+00, 5.48803481e+00, 1.59535896e+00, 2.42494561e+00,\n        7.01957941e-01],\n       [1.10949964e-01, 1.30529369e-02, 9.78970268e-02, 3.13270486e-01,\n        1.76214648e-01, 9.65917331e-01, 3.32849891e-01, 5.09064540e-01,\n        5.15591008e-01, 1.89267585e-01, 7.83176215e-02, 2.87164612e-01,\n        1.42929659e+00, 5.87382161e-02, 4.76432197e-01, 2.67585207e-01,\n        5.02538071e-01, 6.59173314e-01, 1.30529369e-01, 4.63379260e-01,\n        3.78535170e-01, 5.61276287e-01, 1.63161711e-01, 2.48005801e-01,\n        7.17911530e-02],\n       [4.77084844e+00, 5.61276287e-01, 4.20957215e+00, 1.34706309e+01,\n        7.57722988e+00, 4.15344453e+01, 1.43125453e+01, 2.18897752e+01,\n        2.21704133e+01, 8.13850616e+00, 3.36765772e+00, 1.23480783e+01,\n        6.14597534e+01, 2.52574329e+00, 2.04865845e+01, 1.15061639e+01,\n        2.16091371e+01, 2.83444525e+01, 5.61276287e+00, 1.99253082e+01,\n        1.62770123e+01, 2.41348803e+01, 7.01595359e+00, 1.06642495e+01,\n        3.08701958e+00]]))\n\n\nA chi-square test checks for association between two categorical variables.\n\n\n\n7.5.6 Quick reference table for common tests\n\n\n\n\n\n\n\n\n\nSituation\nNull hypothesis\nTest\nPython tool\n\n\n\n\nk-group mean comparison\nAll group means equal\nOne-way ANOVA\nanova_lm\n\n\nk-group, nonparametric\nAll group distributions equal\nKruskal–Wallis\nstats.kruskal\n\n\nTwo means, unequal variance\nMeans equal\nWelch’s t-test\nstats.ttest_ind\n\n\nTwo groups, nonparametric\nDistributions equal\nMann–Whitney U\nstats.mannwhitneyu\n\n\nLinear relationship\nSlope = 0\nOLS slope test\nols + robust SE\n\n\nContinuous association\nCorrelation = 0\nPearson correlation\nstats.pearsonr\n\n\nMonotone association\nCorrelation = 0\nSpearman correlation\nstats.spearmanr\n\n\nCategorical association\nIndependence\nChi-square test\nstats.chi2_contingency\n\n\n\n\nThese examples illustrate how hypotheses guide exploration. Each test produces a statistic, a p-value, and often an effect size. Results are provisional and informal, but they shape which relationships merit deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#iterative-nature-of-exploration",
    "href": "07-exploration.html#iterative-nature-of-exploration",
    "title": "7  Data Exploration",
    "section": "7.6 Iterative Nature of Exploration",
    "text": "7.6 Iterative Nature of Exploration\nExploration is rarely linear. Cleaning, summarizing, and testing feed back into each other. Each new discovery can prompt a return to earlier steps.\n\n7.6.1 Cycle of exploration\n\nInspect variables and detect anomalies.\nClean data based on what anomalies reveal.\nSummarize distributions and relationships.\nFormulate and test new hypotheses.\nRevisit cleaning when results suggest overlooked issues.\n\n\n\n7.6.2 Example: Garage year built\nInitial inspection may suggest that many values of GarageYrBlt are missing. Documentation indicates that missing means no garage.\n\n# Count missing garage years\ndf['GarageYrBlt'].isna().sum()\n\n81\n\n\nWhen checking integrity, we may notice that some garage years precede the house year built.\n\n# Garage built before house built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\n9\n\n\nThis prompts a decision: treat as data entry error, keep with caution, or exclude in certain analyses.\n\n\n7.6.3 Example: Living area and sale price\nA strong correlation between GrLivArea and SalePrice may surface.\n\n# Correlation\nsub = df[['GrLivArea','SalePrice']].dropna()\nsub.corr()\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\nGrLivArea\n1.000000\n0.708624\n\n\nSalePrice\n0.708624\n1.000000\n\n\n\n\n\n\n\nIf a few extremely large houses are driving the correlation, it may be necessary to investigate further.\n\n# Identify extreme values\nsub[sub['GrLivArea'] &gt; 4000][['GrLivArea','SalePrice']]\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\n523\n4676\n184750\n\n\n691\n4316\n755000\n\n\n1182\n4476\n745000\n\n\n1298\n5642\n160000\n\n\n\n\n\n\n\nThese observations may be genuine luxury properties, or they may distort summary statistics. The decision is context-dependent and should be documented.\nExploration is not a one-pass process. Findings in one step often require revisiting previous steps. Clear documentation ensures that these iterations are transparent and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#good-practices-in-data-exploration",
    "href": "07-exploration.html#good-practices-in-data-exploration",
    "title": "7  Data Exploration",
    "section": "7.7 Good Practices in Data Exploration",
    "text": "7.7 Good Practices in Data Exploration\nClear habits in exploration make later analysis more reliable and easier to share. The following practices help ensure quality and reproducibility.\n\n7.7.1 Reproducibility\n\nKeep all work in notebooks or scripts, never only in spreadsheets.\nEnsure that another analyst can rerun the code and obtain identical results.\n\n\n# Example: set a random seed for reproducibility\nimport numpy as np\nnp.random.seed(20250923)\n\n\n\n7.7.2 Documentation\n\nRecord cleaning decisions explicitly. For example, note that NA in PoolQC means no pool.\nKeep a running log of questions, anomalies, and decisions.\n\n\n# Example: create an indicator for presence of a pool\ndf['HasPool'] = df['PoolArea'] &gt; 0\n\n\n\n7.7.3 Balanced curiosity and rigor\n\nExploration can generate many possible stories. Avoid over-interpreting patterns before formal testing.\nDistinguish clearly between exploratory checks and confirmatory analysis.\n\n\n\n7.7.4 Effect sizes and intervals\n\nReport not only p-values but also effect sizes and confidence intervals.\nThis practice keeps focus on the magnitude of relationships.\n\n\n# Example: compute Cohen's d for fireplace vs no fireplace\nsub = df[['SalePrice','Fireplaces']].dropna()\nsub['has_fp'] = (sub['Fireplaces'] &gt; 0).astype(int)\n\nmean_diff = sub.groupby('has_fp')['SalePrice'].mean().diff().iloc[-1]\npooled_sd = sub.groupby('has_fp')['SalePrice'].std().mean()\ncohens_d = mean_diff / pooled_sd\ncohens_d\n\n1.144008229281349\n\n\n\n\n7.7.5 Transparency\n\nShare both code and notes with collaborators.\nVersion control with Git helps track changes and decisions.\n\nGood practices keep exploration structured and reproducible. They also create a record of reasoning that improves collaboration and supports later analysis.\n\n\n\n\n\nDe Cock, D. (2009). Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. Journal of Statistics Education, 17(3), 1–13. https://doi.org/10.1080/10691898.2009.11889627",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "08-regression.html",
    "href": "08-regression.html",
    "title": "8  Regression Models",
    "section": "",
    "text": "8.1 Introduction\nRegression is a fundamental tool in data science and statistics for modeling relationships between variables. It provides a framework to explain how a response variable changes when one or more explanatory variables vary, and it serves as a foundation for prediction, interpretation, and decision making. Regression models are used in a wide range of applications, from estimating the effect of education on income to predicting housing prices based on property characteristics.\nThis chapter introduces regression through a unified set of examples using the Ames Housing dataset. The dataset contains detailed information about housing sales in Ames, Iowa. It has become a popular benchmark for regression tasks, replacing the older Boston Housing dataset due to its larger size and richer set of features. Throughout the chapter, we will use this dataset to illustrate concepts of regression modeling, including model formulation, fitting, diagnosis, and extensions such as regularization, GLM, and GAM. Because many variables in the dataset record absence with NA (for example, NA in the alley variable indicates no alley access), careful preprocessing is required before modeling.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#introduction",
    "href": "08-regression.html#introduction",
    "title": "8  Regression Models",
    "section": "",
    "text": "8.1.1 Ames Housing Data Cleaning\nThe Ames housing data will be used for illustration, but it requires careful preprocessing. A distinctive feature of this dataset is that many NA values do not represent missing information but instead denote the absence of a feature. Treating them as missing would discard useful signals, so they should be recoded explicitly.\nFirst, we retrieve the data to ensure reproducibility.\n\nimport openml\nimport pandas as pd\n\n# Load Ames Housing dataset (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\n\nFor categorical variables, NA often means that the property does not have the feature. For example, Alley is NA when no alley access exists, FireplaceQu is NA when no fireplace is present, and PoolQC is NA when the house does not have a pool. In these cases, NA should be replaced with an explicit category such as “None.”\n\nnone_cols = [\n    \"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\",\n    \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n    \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n    \"BsmtFinType2\", \"MasVnrType\"\n]\nfor col in none_cols:\n    df[col] = df[col].fillna(\"None\")\n\nFor numeric variables, NA may also indicate absence. Examples include GarageCars, GarageArea, and basement square footage variables. When no garage or basement is present, the correct encoding is zero. Thus, these columns should be filled with 0 rather than treated as missing.\n\nzero_cols = [\n    \"GarageCars\", \"GarageArea\",\n    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n    \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\",\n    \"MasVnrArea\"\n]\nfor col in zero_cols:\n    df[col] = df[col].fillna(0)\n\nFinally, some variables contain genuinely missing data. A common example is LotFrontage, which records the linear feet of street connected to a property. Here NA values reflect unavailable measurements. These can be imputed using summary statistics such as the median or by more advanced methods if desired.\n\ndf[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].median())\n\nThis structured cleaning step ensures that absence is distinguished from missingness, numeric zero values are meaningful, and true missing values are handled appropriately. Only after this preparation is the dataset ready for modeling.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#linear-regression-model",
    "href": "08-regression.html#linear-regression-model",
    "title": "8  Regression Models",
    "section": "8.2 Linear Regression Model",
    "text": "8.2 Linear Regression Model\nThe starting point of regression analysis is the specification of a model that links a response variable to one or more explanatory variables. In the simplest form, the relationship is described by a linear function plus an error term:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i,\n\\]\nwhere \\(Y_i\\) is the response for observation \\(i\\), \\(X_{ij}\\) are the explanatory variables, \\(\\beta_j\\) are unknown coefficients, and \\(\\\\varepsilon_i\\) is a random error term. The model asserts that systematic variation in the response is captured by a linear combination of predictors, while unsystematic variation is left to the error.\nFor linear regression to yield valid estimates and inference, several assumptions are commonly made. The form of the mean function is assumed linear in parameters. The error terms are assumed to have mean zero and constant variance, and to be independent across observations. When sample sizes are small, normality of the errors is sometimes assumed to justify exact inference. With larger samples, asymptotic results make this assumption less critical, and estimation of coefficients by least squares does not require it. Finally, explanatory variables should not be perfectly collinear. These assumptions guide model fitting and motivate the diagnostic checks that follow.\n\n8.2.1 Fitting\nFitting a regression model means finding estimates of the coefficients that make the model align with observed data. The most common approach is ordinary least squares, which minimizes the sum of squared residuals:\n\\[\nL(\\beta) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_{i1} - \\cdots - \\beta_p X_{ip})^2.\n\\]\nHere \\(L(\\beta)\\) is the loss function, measuring how far predictions are from observed responses. Minimizing this quadratic loss yields closed form solutions for the coefficient estimates when predictors are not perfectly collinear. Computationally, this involves solving the normal equations or using matrix decompositions such as QR or singular value decomposition, which provide stable and efficient solutions.\nThis framework also sets the stage for extensions. By modifying the loss function to include penalty terms, one obtains regularization methods such as ridge regression or the lasso. The optimization remains similar in spirit but balances data fit with model complexity. Later sections will show how these modifications improve prediction and interpretability when many predictors are involved.\nHousing prices are highly skewed, with a long right tail. To stabilize variance and make the model fit better, it is common to use the log of SalePrice as the response:\n\\[\nY_i = \\log(\\text{SalePrice}_i).\n\\]\nWe add this transformed response to the dataset.\n\nimport numpy as np\n\ndf[\"LogPrice\"] = np.log(df[\"SalePrice\"])\n\nMany studies and analyses of the Ames data have found certain variables to be consistently important for predicting sale price. These include OverallQual (overall material and finish quality), GrLivArea (above- ground living area), GarageCars (garage capacity), TotalBsmtSF (total basement area), YearBuilt (construction year), FullBath (number of full bathrooms), and KitchenQual (kitchen quality). We will focus on these predictors to illustrate model fitting.\nInstead of manually creating dummy variables, we can use the formula API from statsmodels, which handles categorical predictors internally.\n\nimport statsmodels.formula.api as smf\n\nformula = (\n    \"LogPrice ~ OverallQual + GrLivArea + GarageCars + \"\n    \"TotalBsmtSF + YearBuilt + FullBath + C(KitchenQual)\"\n)\n\nWe then fit the regression model directly with the formula.\n\nmodel = smf.ols(formula, data=df).fit()\n\nFinally, we examine the regression results, which highlight the most important predictors of log sale price.\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nLogPrice\nR-squared:\n0.819\n\n\nModel:\nOLS\nAdj. R-squared:\n0.818\n\n\nMethod:\nLeast Squares\nF-statistic:\n727.5\n\n\nDate:\nTue, 18 Nov 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n15:23:41\nLog-Likelihood:\n515.19\n\n\nNo. Observations:\n1460\nAIC:\n-1010.\n\n\nDf Residuals:\n1450\nBIC:\n-957.5\n\n\nDf Model:\n9\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n6.8033\n0.419\n16.219\n0.000\n5.980\n7.626\n\n\nC(KitchenQual)[T.Fa]\n-0.2146\n0.037\n-5.780\n0.000\n-0.287\n-0.142\n\n\nC(KitchenQual)[T.Gd]\n-0.0653\n0.020\n-3.257\n0.001\n-0.105\n-0.026\n\n\nC(KitchenQual)[T.TA]\n-0.1348\n0.023\n-5.931\n0.000\n-0.179\n-0.090\n\n\nOverallQual\n0.0882\n0.006\n15.841\n0.000\n0.077\n0.099\n\n\nGrLivArea\n0.0002\n1.34e-05\n17.987\n0.000\n0.000\n0.000\n\n\nGarageCars\n0.0822\n0.008\n10.056\n0.000\n0.066\n0.098\n\n\nTotalBsmtSF\n0.0001\n1.28e-05\n9.134\n0.000\n9.2e-05\n0.000\n\n\nYearBuilt\n0.0021\n0.000\n9.686\n0.000\n0.002\n0.003\n\n\nFullBath\n-0.0087\n0.012\n-0.727\n0.467\n-0.032\n0.015\n\n\n\n\n\n\n\n\nOmnibus:\n962.926\nDurbin-Watson:\n1.990\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n36561.376\n\n\nSkew:\n-2.522\nProb(JB):\n0.00\n\n\nKurtosis:\n26.991\nCond. No.\n2.57e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.57e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe output gives estimated coefficients, standard errors, and measures of fit. At this stage, several questions naturally arise:\n\nHow should we interpret a coefficient when the outcome is on the log scale? For instance, what does a one-unit increase in OverallQual imply for expected sale price?\nHow do we compare the importance of variables measured on different scales, such as square footage and construction year?\nWhat role do categorical variables like KitchenQual play, and how do we interpret their dummy coefficients relative to the baseline?\nWhich predictors are statistically significant, and does significance necessarily imply practical importance?\nHow well does the model explain variation in housing prices, and what limitations might remain?\n\nThese questions guide us in interpreting the fitted model and connect directly to the diagnostic checks discussed in the next section.\n\n\n8.2.2 Diagnosis\nOnce a regression model has been fitted, it is essential to examine whether the underlying assumptions hold and whether the model provides a useful description of the data. Diagnostics help identify potential problems such as non-linearity, heteroscedasticity, influential points, and violations of independence.\nThe first step is to examine residuals, defined as the difference between observed and fitted values:\n\\[\n\\hat{\\varepsilon}_i = Y_i - \\hat{Y}_i.\n\\]\nPlotting residuals against fitted values reveals whether variance is constant and whether systematic patterns remain.\n\nimport matplotlib.pyplot as plt\n\nfitted_vals = model.fittedvalues\nresiduals = model.resid\n\nplt.scatter(fitted_vals, residuals, alpha=0.5)\nplt.axhline(0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs Fitted\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother check is the distribution of residuals. A histogram or Q-Q plot can indicate whether residuals are approximately normal, which is most relevant for inference in small samples.\n\nplt.hist(residuals, bins=30, edgecolor=\"black\")\nplt.xlabel(\"Residuals\")\nplt.title(\"Histogram of residuals\")\nplt.show()\n\ninfluence = model.get_influence()\nstd_resid = influence.resid_studentized_internal\n\nimport statsmodels.api as sm\nsm.qqplot(std_resid, line=\"45\")\nplt.title(\"Q-Q plot of residuals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential observations can distort regression results. Leverage and Cook’s distance are standard measures to detect such points.\n\ncooks = influence.cooks_distance[0]\n\nplt.scatter(fitted_vals, cooks, alpha=0.5)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Cook's distance\")\nplt.title(\"Influence diagnostics\")\nplt.show()\n\n\n\n\n\n\n\n\nKey questions to raise at this stage are:\n\nDo residuals appear randomly scattered, suggesting the linear model is adequate?\nIs there evidence of non-constant variance or other systematic patterns?\nAre residuals approximately normal, and does this matter given the sample size?\nWhich points exert disproportionate influence on the fitted model?\n\nThese diagnostic tools guide improvements, such as transformations, adding interaction terms, or considering alternative modeling approaches.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#regularized-regression",
    "href": "08-regression.html#regularized-regression",
    "title": "8  Regression Models",
    "section": "8.3 Regularized Regression",
    "text": "8.3 Regularized Regression\n\n8.3.1 Motivation\nOrdinary least squares can perform poorly when there are many predictors or when predictors are highly correlated. In such cases, estimated coefficients become unstable and prediction accuracy suffers. Regularization introduces penalties on the size of coefficients, leading to simpler and more robust models.\nTo illustrate, we return to the Ames data. Suppose we fit a model with a large set of predictors. Ordinary least squares will attempt to explain every fluctuation in the data, potentially overfitting. A regularized approach reduces this risk by shrinking coefficients, improving out-of-sample prediction.\n\n\n8.3.2 Formulation\nThe penalized regression framework modifies the least squares objective:\n\\[\nL(\\beta) = \\sum_{i=1}^n (Y_i - X_i^\\top \\beta)^2 + \\lambda P(\\beta),\n\\]\nwhere \\(P(\\beta)\\) is a penalty function and \\(\\lambda\\) controls its strength.\nFor ridge regression the penalty is\n\\[\nP(\\beta) = \\sum_j \\beta_j^2,\n\\]\nwhich shrinks coefficients smoothly toward zero but never sets them exactly to zero. For lasso regression the penalty is\n\\[\nP(\\beta) = \\sum_j |\\beta_j|.\n\\]\nBecause the absolute value has a sharp corner at zero, lasso can shrink some coefficients exactly to zero. This property allows lasso to perform variable selection and estimation in a single step, producing sparse models in which unimportant predictors are excluded automatically. Elastic net combines both types of penalties.\n\n\n8.3.3 Algorithms\nRidge regression has a closed-form solution obtained by modifying the normal equations. Lasso and elastic net require iterative algorithms, with coordinate descent being the most widely used. In practice, these algorithms are efficient and scale well to high-dimensional data.\nWhen using scikit-learn, predictors must be numeric. Since the Ames data include categorical variables such as KitchenQual, we need to encode them. We use a OneHotEncoder inside a ColumnTransformer. This transforms categorical variables into binary indicator columns while keeping numeric variables unchanged. The drop=\"first\" option avoids perfect collinearity by omitting one reference category.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\n\nnumeric_features = [\n    \"OverallQual\", \"GrLivArea\", \"GarageCars\",\n    \"TotalBsmtSF\", \"YearBuilt\", \"FullBath\"\n]\ncategorical_features = [\"KitchenQual\"]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n    ]\n)\n\nX = df[numeric_features + categorical_features]\ny = df[\"LogPrice\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nridge = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", Ridge(alpha=10))\n]).fit(X_train, y_train)\n\nlasso = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", Lasso(alpha=0.1))\n]).fit(X_train, y_train)\n\n\n\n8.3.4 Solution Paths\nAs the penalty parameter \\(\\lambda\\) decreases, the behavior of coefficients changes. Ridge coefficients approach ordinary least squares estimates, while lasso coefficients enter the model sequentially, illustrating variable selection.\nBecause predictors must be on the same scale for a fair comparison, it is important to standardize the numeric variables before computing the lasso path. Without standardization, some coefficients can appear flat or dominate others due to differences in scale.\nIn our Ames example, we have six numeric predictors and three dummy variables for KitchenQual (since one category was dropped). This means we are estimating nine coefficients in total, excluding the intercept. All nine should appear in the solution path once variables are properly scaled.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import lasso_path\nimport numpy as np\n\n# Standardized predictors\nX_proc = preprocessor.fit_transform(X_train)\nscaler = StandardScaler(with_mean=False)\nX_proc_std = scaler.fit_transform(X_proc)\n\n# Compute lambda_max\nn_samples = X_proc_std.shape[0]\nlambda_max = np.max(np.abs(X_proc_std.T @ y_train)) / n_samples\n\n# Define a grid of lambda values\nalphas = np.logspace(np.log10(lambda_max), -3, 50)\n\n# Compute solution path\nalphas, coefs, _ = lasso_path(X_proc_std, y_train, alphas=alphas)\n\nplt.plot(alphas, coefs.T)\nplt.xscale(\"log\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Coefficients\")\nplt.title(\"Lasso solution paths starting at lambda_max\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals how each coefficient evolves as \\(\\lambda\\) changes. At large values of \\(\\lambda\\), coefficients are shrunk close to zero. As \\(\\lambda\\) decreases, more predictors enter the model. The intercept is not included in the path and should be ignored when interpreting these curves.\n\n\n8.3.5 Tuning Parameter Selection\nChoosing \\(\\lambda\\) is critical. Too large, and the model is oversmoothed; too small, and the penalty has little effect. This parameter controls the trade-off between model fit and complexity:\n\nWhen \\(\\lambda = 0\\), the model reduces to the unpenalized regression.\nAs \\(\\lambda \\to \\infty\\), coefficients shrink toward zero, increasing bias but reducing variance.\n\nChoosing \\(\\lambda\\) appropriately is crucial. A general principle is to define a selection criterion \\(C(\\lambda)\\), which measures the predictive or explanatory performance of the fitted model, and then select \\(\\hat{\\lambda} = \\arg\\min_{\\lambda} C(\\lambda)\\).\nCommon criteria:\n\n\\(R^2\\) on a validation set: select \\(\\lambda\\) that maximizes explained variance.\nInformation criteria (AIC, BIC): less common in practice for penalized regression.\nCross-validation (CV): partition data, fit on training folds, evaluate on holdout fold, average prediction error across folds.\n\nThe grid of candidate values is not arbitrary:\n\nMaximum value:\n\\[\n\\lambda_{\\max} = \\max_j \\tfrac{1}{n} |x_j^\\top y|,\n\\]\nwith standardized predictors. At this level, all coefficients are zero.\nMinimum value: \\[\n\\lambda_{\\min} = \\epsilon \\cdot \\lambda_{\\max},\n\\] with \\(\\epsilon = 10^{-3}\\) if \\(n &gt; p\\) and \\(\\epsilon = 10^{-2}\\) otherwise.\nGrid: values are log-spaced between \\(\\lambda_{\\max}\\) and \\(\\lambda_{\\min}\\) (default 100 points in scikit-learn).\n\nIn the Ames example, we can use \\(k\\)-fold cross-validation to evaluate ridge and lasso models.\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\n\n# Lasso with 10-fold CV\nlasso_cv = LassoCV(\n    cv=KFold(n_splits=10, shuffle=True, random_state=123),\n    random_state=123\n)\n\n# Pipeline with preprocessing + model\npipe = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", lasso_cv)\n])\n\npipe.fit(X, y)\n\n# Selected lambda\nbest_lambda = pipe.named_steps[\"model\"].alpha_\nprint(\"Best lambda (alpha) selected by CV:\", best_lambda)\n\nBest lambda (alpha) selected by CV: 0.00032631403363203007\n\n\nThis process identifies the tuning parameter that balances bias and variance most effectively, yielding a model that generalizes well beyond the training data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#generalized-linear-models",
    "href": "08-regression.html#generalized-linear-models",
    "title": "8  Regression Models",
    "section": "8.4 Generalized Linear Models",
    "text": "8.4 Generalized Linear Models\n\n8.4.1 Introduction\nLinear regression is a powerful tool for continuous outcomes under Gaussian assumptions, but many response variables encountered in practice are not continuous or normally distributed. For example, an indicator of whether a house sale price is above the median is binary, the number of bathrooms is a count, and proportions such as the fraction of remodeled homes lie between 0 and 1. Using linear regression in these settings can yield nonsensical predictions (e.g., negative counts or probabilities outside the unit interval).\nGeneralized linear models (GLMs) extend linear regression to cover a wider range of outcomes. The key idea is to preserve the familiar linear predictor structure, while linking it to the mean of the outcome through a function that reflects the nature of the data. The formal framework was introduced by Nelder & Wedderburn (1972) and remains central in modern statistics. Today, GLMs are viewed more flexibly: the distributional assumption provides a convenient likelihood-based loss function, but in practice one can proceed with quasi-likelihood or even direct loss minimization without strict distributional commitment.\n\n\n8.4.2 Framework\nGLMs extend linear regression by introducing a link function between the linear predictor and the conditional mean of the response. In linear regression we write\n\\[\nY_i = X_i^\\top \\beta + \\varepsilon_i\n\\]\nwith mean zero error \\(\\varepsilon_i\\)’s.\nThe mean is simply \\(\\mu_i = X_i^\\top \\beta\\). In a GLM, we allow non-Gaussian outcomes by defining\n\\[\n\\eta_i = X_i^\\top \\beta, \\quad g(\\mu_i) = \\eta_i,\n\\]\nwhere \\(g(\\cdot)\\) is a monotone link function, \\(\\mu_i = \\mathbb{E}(Y_i)\\), and \\(\\beta\\) are regression coefficients. The coefficients maintain the same interpretation as in linear regression: a one-unit change in a predictor shifts the linear predictor \\(\\eta_i\\) by its coefficient, with an indirect effect on \\(\\mu_i\\) through the link.\nThe variance of \\(Y_i\\) depends on the mean: \\(\\text{Var}(Y_i) = V(\\mu_i)\n\\cdot \\phi\\), where \\(V(\\cdot)\\) is the variance function and \\(\\phi\\) is a dispersion parameter. This structure arises naturally from the exponential family, which provides a unifying framework for GLMs. While exact distributional assumptions can be specified, the mean–variance relationship is often sufficient.\n\n\n8.4.3 Special Cases\n\nLogistic regression\nFor binary outcomes,\n\\[\nY_i \\sim \\text{Bernoulli}(\\mu_i), \\qquad g(\\mu_i) = \\log \\frac{\\mu_i}{1-\\mu_i}.\n\\]\nThe coefficient \\(\\beta_j\\) quantifies the log-odds change of success for a one-unit increase in \\(x_{ij}\\), with other covariates held fixed.\nPoisson regression\nFor count data,\n\\[\nY_i \\sim \\text{Poisson}(\\mu_i), \\qquad g(\\mu_i) = \\log(\\mu_i).\n\\]\nThe coefficient \\(\\beta_j\\) is interpreted as the log rate ratio, where \\(\\exp(\\beta_j)\\) gives the multiplicative change in expected count for a one-unit increase in \\(x_{ij}\\).\nGaussian regression\nFor continuous responses,\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2), \\qquad g(\\mu_i) = \\mu_i.\n\\]\nThe coefficient \\(\\beta_j\\) represents the expected change in the response for a one-unit increase in \\(x_{ij}\\).\n\nThus, GLMs preserve the linear predictor while flexibly adapting the link and variance structure to suit binary, count, and continuous data.\n\n\n8.4.4 Fitting and Diagnosis\nEstimation in generalized linear models is typically carried out by maximum likelihood. The parameters \\(\\beta\\) are obtained by solving the score equations, which in practice are computed through numerical optimization. A common algorithm is iteratively reweighted least squares (IRLS), which updates coefficient estimates using weighted least squares until convergence. In Python, functions such as statsmodels.api.GLM or sklearn.linear_model.LogisticRegression implement this estimation automatically, providing coefficient estimates along with standard errors and confidence intervals when applicable.\nAfter fitting a GLM, model adequacy should be checked through diagnostics. Residuals such as deviance residuals or Pearson residuals can reveal lack of fit and highlight influential observations. Goodness of fit can also be assessed with deviance statistics, likelihood ratio tests, or pseudo-\\(R^2\\) measures. In Python, statsmodels provides methods like .resid_deviance, .resid_pearson, and influence statistics to assess model fit. Visual inspection through residual plots remains a practical tool to detect systematic deviations from model assumptions.\n\n\n8.4.5 Regularized GLM\nRegularization extends generalized linear models by adding a penalty to the log-likelihood, which stabilizes estimation in high-dimensional settings and enables variable selection. The optimization problem can be formulated as\n\\[\n\\hat{\\beta} = \\arg\\min_{\\beta} \\Big\\{ -\\ell(\\beta) +\n\\lambda P(\\beta) \\Big\\},\n\\]\nwhere \\(\\ell(\\beta)\\) is the log-likelihood, \\(P(\\beta)\\) is a penalty function, and \\(\\lambda\\) is a tuning parameter controlling the strength of shrinkage.\nCommon choices of \\(P(\\beta)\\) include:\n\nRidge (\\(L_2\\)): \\(\\sum_j \\beta_j^2\\).\nLasso (\\(L_1\\)): \\(\\sum_j |\\beta_j|\\).\nElastic Net: \\(\\alpha \\sum_j |\\beta_j| + (1-\\alpha)\\sum_j \\beta_j^2\\).\n\nThe fitting algorithm typically involves coordinate descent or gradient- based optimization methods, which are efficient for large-scale data and sparse solutions. For example, the glmnet algorithm uses cyclical coordinate descent with warm starts.\nSelection of the tuning parameter \\(\\lambda\\) is crucial. A standard approach is cross-validation, where data are split into folds, the model is fitted on training folds for a grid of \\(\\lambda\\) values, and performance is evaluated on validation folds. The \\(\\lambda\\) yielding the lowest prediction error is chosen, sometimes with an additional rule to prefer more parsimonious models (the “one standard error rule”).\nIn Python, sklearn.linear_model.LogisticRegressionCV or sklearn.linear_model.ElasticNetCV implement these ideas, providing automatic cross-validation for regularized GLMs.\nThe general workflow for fitting a regularized logistic regression model is:\n\nDefine predictors and outcome: choose relevant numeric and categorical features, and specify the binary response.\nPreprocess features: standardize numeric predictors with StandardScaler() and encode categorical predictors with OneHotEncoder().\nSet up the pipeline: combine preprocessing with the logistic regression model in a unified workflow.\nFit with cross-validation: use LogisticRegressionCV with lasso (L1) or elastic net penalties. Cross-validation automatically selects the tuning parameter \\(\\lambda\\).\nInspect coefficients: identify which predictors remain with nonzero coefficients, interpreting them as important contributors to the outcome.\nEvaluate performance: measure predictive accuracy or AUC (see chapter on classification) on a held-out test set to assess generalization.\n\nThis structured process ensures stability, interpretability, and good predictive performance when fitting regularized logistic models.\n\n\n8.4.6 Example: Regularized GLM with Ames Housing Data\nWe continue with the processed Ames housing dataset (OpenML id 42165) from earlier in the regression chapter. The task is to predict whether a home is “expensive” (above the median sale price) using selected predictors. We fit a logistic regression model with an \\(L_1\\) (lasso) penalty to enable variable selection.\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import Pipeline\n\n# Binary outcome: 1 if SalePrice &gt; median\nmedian_price = df[\"SalePrice\"].median()\ny = (df[\"SalePrice\"] &gt; median_price).astype(int)\n\n# Logistic regression with L1 penalty and cross-validation\nlogit_lasso_cv = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", LogisticRegressionCV(\n         Cs=20, cv=5, penalty=\"l1\", solver=\"saga\",\n         scoring=\"accuracy\", max_iter=5000,\n         random_state=0\n    ))\n])\n\nlogit_lasso_cv.fit(X, y)\n\n# Extract selected coefficients\nmodel = logit_lasso_cv.named_steps[\"model\"]\nfeature_names = (\n    numeric_features +\n    list(logit_lasso_cv.named_steps[\"preprocessor\"]\n        .named_transformers_[\"cat\"]\n        .get_feature_names_out(categorical_features))\n)\n\nprint(\"Selected coefficients (lasso):\")\nfor name, coef in zip(feature_names, model.coef_[0]):\n    print(f\"{name}: {coef:.4f}\")\n\nprint(\"\\nBest C (inverse of lambda):\", model.C_[0])\n\nSelected coefficients (lasso):\nOverallQual: 1.1998\nGrLivArea: 1.6516\nGarageCars: 0.6534\nTotalBsmtSF: 0.5812\nYearBuilt: 0.8845\nFullBath: 0.1628\nKitchenQual_Fa: -1.2791\nKitchenQual_Gd: -0.0467\nKitchenQual_TA: -0.9568\n\nBest C (inverse of lambda): 1.623776739188721\n\n\nThis example demonstrates how lasso-penalized logistic regression can be used within the GLM framework. The penalty shrinks coefficients toward zero, with some set exactly to zero if uninformative, thereby improving interpretability and predictive stability.\nWe can evaluate classification performance by computing the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Predictions\ny_pred = logit_lasso_cv.predict(X)\n\n# Confusion matrix\ncm = confusion_matrix(y, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\ndisplay_labels=[\"Not Expensive\", \"Expensive\"])\ndisp.plot(cmap=\"Blues\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#spatial-statistical-methods",
    "href": "08-regression.html#spatial-statistical-methods",
    "title": "8  Regression Models",
    "section": "8.5 Spatial Statistical Methods",
    "text": "8.5 Spatial Statistical Methods\nThis section was written by Quinn Saltus, an undergraduate student pursuing a dual degree in Applied Data Analysis and Geograpic Information Systems. They were motivated to write about this topic because they wanted to bridge the geographic and statistical approaches to problem solving.\n\n8.5.1 Introduction\nSpatially dependent data can cause problems for standard statistical methods. Independence of errors is an assumption for most commonly-used tools, but if location affects your data, that assumption is untenable. Spatial Statistics can quantify the spatial dependence among data and improve predictive models’ accuracy / statistical rigor.\n\n\n8.5.2 Dataset\nThe dataset used in this demonstration is the Turnout by County for the 1980 Election from Pace & Barry (1997).\n\nfrom plotnine import *\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\ndataset = fetch_openml(name='space_ga', version=1)\n\ndf = dataset.data\ndf.columns = df.columns.str.lower()\n\n# convert log-proportion to percentage for ease of understanding\ndf[\"pct_voter\"] = np.exp(dataset.target) * 100\n\n# convert coordinates to degrees\ndf[\"xcoord\"] = df[\"xcoord\"] / 10**6\ndf[\"ycoord\"] = df[\"ycoord\"] / 10**6\n\n\n\nCode\n(ggplot(df, aes(x=\"xcoord\", y=\"ycoord\", color=\"pct_voter\"))\n    + geom_point(size=0.6)\n    + scale_color_continuous(cmap_name=\"Spectral\")\n    + coord_equal(ratio=1.3)\n    + labs(\n        title=\"Turnout Percentage By County for the 1980 Election\",\n        x=\"Longitude\",\n        y=\"Latitude\"\n    )\n    + theme_light()\n    + theme(\n        panel_background=element_rect(fill=\"#cccccc\")\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\n8.5.3 Libraries\nThe libraries used are The Python Spatial Analysis Library and PyKrige. These libraries haven’t been used before in this course, so you may need to install them yourself.\npip install pykrige\npip install pysal\nI’ll also use Scipy‘s KDTree class, which greatly reduces the amount of computation required to compare data points’ distances. This is important for speed with large datasets.\n\nimport libpysal\nfrom scipy.spatial import KDTree\nfrom pysal.explore import esda\nimport pykrige as pk\n\n\n\n8.5.4 Spatial Weights\nStandard regression has error terms that are completely random:\n\\[\nY = \\beta X + \\epsilon\n\\]\nThe spatial error model (SEM) formulates residuals as a weighted mean of nearby errors plus a random error term:\n\\[\nY = \\beta X + u, u = \\lambda W u + \\epsilon\n\\]\nWith PySAL, we need to precompute the weights matrix \\(W\\) to give to analysis functions. Inverse square distance is the most popular and will be used here. Other methods like K-Nearest-Neighbors (libpysal.weights.KNN) and Gabriel Graphs (libpysal.weights.Gabriel) are available.\n\ndist_kdtree = KDTree(df[[\"xcoord\", \"ycoord\"]])\nweights = libpysal.weights.DistanceBand(\n    dist_kdtree,\n    threshold=2.5, # limit search distance\n    binary=False, # use numeric weights\n    alpha=-2 # use inverse square distance\n)\n\n\n\n8.5.5 Moran’s I\nMoran’s I is a common exploratory measure of spatial autocorrelation based on each point’s similarity to its neighbors. Here are some example I-values using adjacency weights. Moran’s I is increased when nearby values are similar, and decreased when nearby values are heterogeneous.\n\n\n\nThe pysal.esda module implements Moran’s I for us.\n\nmoran = esda.Moran(y = df[\"pct_voter\"], w=weights)\nmoran.plot_scatter()\nprint(f\"Moran's I = {moran.I:.4f}\")\nprint(f\"Moran's I p-value = {moran.p_rand:.4f}\")\n\nMoran's I = 0.5780\nMoran's I p-value = 0.0000\n\n\n\n\n\n\n\n\n\nMoran’s I is significant and positive, indicating that this dataset has positive spatial autocorrelation. The Moran Scatterplot compares the (standardized) target variable to the weighted mean of nearby values (spatial lag). It confirms the I-value’s assessment of positive autocorrelation.\n\n\n8.5.6 Kriging\nKriging is a nonparametric method of interpolation. Using the OrdinaryKriging class from PyKrige, we can estimate the mean of a parameter over the spatial plane.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=1234)\n\n\n\nkrige = pk.OrdinaryKriging(\n    x=df_train[\"xcoord\"],\n    y=df_train[\"ycoord\"],\n    z=df_train[\"pct_voter\"],\n    coordinates_type=\"geographic\" # use spherical coordinate math\n)\n\n# use trained model to estimate test data\ndf_test[\"krige_estimate\"], _ = krige.execute(\n    \"points\", df_test[\"xcoord\"], df_test[\"ycoord\"]\n)\n\ndf_test[\"krige_residual\"] = df_test[\"pct_voter\"] - df_test[\"krige_estimate\"]\n\nWith the model trained, we can check that it performs as expected on the test set.\n\n\nCode\n(ggplot(df_test, aes(x=\"xcoord\", y=\"ycoord\", color=\"krige_estimate\"))\n    + geom_point(size=0.6)\n    + scale_color_continuous(cmap_name=\"Spectral\")\n    + coord_equal(ratio=1.3)\n    + labs(\n        title=\"Kriging-Estimated Turnout\",\n        x=\"Longitude\",\n        y=\"Latitude\"\n    )\n    + theme_light()\n    + theme(\n        panel_background=element_rect(fill=\"#cccccc\"),\n        figure_size=(6, 3.5)\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n(ggplot(df_test, aes(x=\"xcoord\", y=\"ycoord\", color=\"krige_residual\"))\n    + geom_point(size=0.6)\n    + scale_color_continuous(\n        cmap_name=\"PuOr\",\n        limits=(\n            max(abs(df_test[\"krige_residual\"])),\n            -max(abs(df_test[\"krige_residual\"]))\n        )\n    )\n    + coord_equal(ratio=1.3)\n    + labs(\n        title=\"Residuals of Kriging Estimates of Turnout\",\n        x=\"Longitude\",\n        y=\"Latitude\"\n    )\n    + theme_light()\n    + theme(\n        panel_background=element_rect(fill=\"#cccccc\"),\n        figure_size=(6, 3.5)\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n(ggplot(df_test, aes(x=\"krige_residual\", fill=\"krige_residual\"))\n    + geom_histogram(\n        bins=20,\n        fill=\"#99bbff\",\n        color=\"#000000\"\n    )\n    + labs(\n        title=\"Residuals of Kriging (No Regression)\",\n        x=f\"Error (Percentage Points)\"\n    )\n    + theme_light()\n    + theme(\n        figure_size=(6, 2.5)\n    )\n)\n\n\n\n\n\n\n\n\n\nVariance seems to be higher in the west, where counties are farther apart, but the overall trend seems to be fit well. The errors seem approximately normal and the geographic patterns seen in the full dataset seem to be captured accurately by the method.\n\n\n8.5.7 SEM Regression With Kriging\nTo compare performance with and without spatial components, we will use a basic linear regression model.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X := df[[\"pop\", \"education\", \"houses\", \"income\"]],\n    Y := df[\"pct_voter\"],\n    test_size=0.2, random_state=1234\n)\n\nmodel_basic = LinearRegression().fit(X_train, Y_train)\n\n\n\nCode\nprint(f\"Test R-squared: {model_basic.score(X_test, Y_test):.4f}\")\n\n(ggplot(\n        pd.DataFrame({\n            \"Predicted Value\" : model_basic.predict(X_test),\n            \"True Value\" : Y_test,\n            }), \n        aes(x=\"Predicted Value\", y=\"True Value\")\n    )\n    + geom_point(color=\"#00000088\")\n    + geom_abline(slope=1, intercept=0, color=\"red\", linetype=\"dashed\")\n    + theme_light()\n    + theme(figure_size=(4, 3))\n)\n\n\nTest R-squared: 0.4694\n\n\n\n\n\n\n\n\n\nThe data seem generally well fit, but there is a lot of unexplained variability. We can use Kriging on the residuals of the basic model to augment its performance.\n\n\nCode\ndf_train = df_train.reset_index()\ndf_test = df_test.reset_index()\n\np_train = df_train[[\"pop\", \"education\", \"houses\", \"income\"]].to_numpy()\nx_train = df_train[[\"xcoord\", \"ycoord\"]].to_numpy()\ny_train = df_train[\"pct_voter\"].to_numpy()\n\np_test = df_test[[\"pop\", \"education\", \"houses\", \"income\"]].to_numpy()\nx_test = df_test[[\"xcoord\", \"ycoord\"]].to_numpy()\ny_test = df_test[\"pct_voter\"].to_numpy()\n\n\n\nfrom pykrige.rk import RegressionKriging\n\n# any sklearn machine learning model can be used as the regression_model\nmodel_rk = RegressionKriging(regression_model=LinearRegression())\n\n# Note that predictors, points, and targets must be np arrays, not DataFrame\nmodel_rk.fit(p_train, x_train, y_train)\n\nFinished learning regression model\nFinished kriging residuals\n\n\n\n\nCode\nprint(\n    \"R-squared (Regression + Kriging):\",\n    f\"{model_rk.score(p_test, x_test, y_test):.4f}\"\n)\n(ggplot(\n        pd.DataFrame({\n            \"Predicted Value\" : model_rk.predict(p_test, x_test),\n            \"True Value\" : Y_test,\n            }),\n        aes(x=\"Predicted Value\", y=\"True Value\")\n    )\n    + geom_point(color=\"#00000088\")\n    + geom_abline(slope=1, intercept=0, color=\"red\", linetype=\"dashed\")\n    + labs(title=\"Regression + Kriging\")\n    + theme_light()\n    + theme(figure_size=(4, 3))\n)\n\n\nR-squared (Regression + Kriging): 0.6684\n\n\n\n\n\n\n\n\n\nSpatially modeling the errors of the basic model allowed us to improve test set performance by over 40%.\n\n\n8.5.8 Conclusions\nPySAL offers strong tools for spatial data exploration and visualization. PyKrige lets you drop in spatial predictors or post-processing.\nSpatial analysis is flexible. You can use spatial variables as part of the initial training (run OrdinaryKriging and use it as a predictor), or apply it at the end according to the SEM as demonstrated.\nThat being said, you have to validate the method. I had intended to demonstrate with the NYC Collision Data, but when I checked Moran’s I, the dataset had nonsignificant spatial autocorrelation. Crashes are largely independent events, so a spatial analysis would be fruitless. These tools are powerful, but can only be used in appropriate contexts.\n\n\n8.5.9 Further Readings\nESRI’s explanation of spatial autoregression\nThe PySAL documentation\nThe PyKrige documentation\nIf you’re particularly interested in spatial problems and tools, GEOG 2500 (Introduction to Geographic Information Systems) is a good place to start learning about geographic methods.\nThe example image for Moran’s I and the Wikipedia article I got it from.\n\n\n\n\nNelder, J. A., & Wedderburn, R. W. M. (1972). Generalized linear models. Journal of the Royal Statistical Society Series A: Statistics in Society, 135(3), 370–384.\n\n\nPace, R. K., & Barry, R. (1997). Quick computation of regressions with a spatially autoregressive dependent variable. Geographical Analysis, 29(3), 232–247.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "09-classification.html",
    "href": "09-classification.html",
    "title": "9  Classification",
    "section": "",
    "text": "9.1 Introduction to Classification\nClassification is one of the most widely used tasks in data science, concerned with predicting categorical outcomes rather than continuous quantities. Many real-world problems can be framed as classification, such as diagnosing a disease from medical records, determining whether a loan applicant is likely to default, or identifying spam emails. Compared with regression, which models numeric responses, classification methods aim to assign observations into predefined classes based on their features. Logistic regression, introduced in the previous chapter, provides a natural transition: it uses a regression framework to model the probability of class membership. In this chapter, we expand beyond logistic regression to study how classification models are evaluated and to introduce other methods developed for classification tasks.\nClassification problems arise when the outcome of interest is categorical rather than continuous. Instead of predicting a numerical quantity, the task is to assign each observation to one of several predefined classes. Examples include deciding whether an email is spam or not, predicting a patient’s disease status from clinical measures, or determining whether a financial transaction is fraudulent. These problems are ubiquitous across domains and often require different tools from those used in regression.\nA widely used dataset for illustrating binary classification is the Breast Cancer Wisconsin (Diagnostic) dataset. It contains information on 569 patients, each described by 30 numerical features computed from digitized images of fine needle aspirates of breast masses. These features summarize characteristics of the cell nuclei, such as radius, texture, perimeter, smoothness, and concavity, with versions capturing mean, variation, and extreme values. The outcome variable records whether the tumor is malignant or benign. Because the features are all numeric and the outcome is binary, this dataset provides an ideal setting for introducing classification methods and performance evaluation.\nBefore building classification models, it is useful to perform exploratory data analysis (EDA) to understand the structure of the data. We first load the dataset from scikit-learn.\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\n# Load dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf[\"diagnosis\"] = data.target\n\n# Map diagnosis: 0 = malignant, 1 = benign\ndf[\"diagnosis\"] = df[\"diagnosis\"].map({0: \"malignant\", 1: \"benign\"})\ndf.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ndiagnosis\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nmalignant\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nmalignant\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nmalignant\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nmalignant\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nmalignant\n\n\n\n\n5 rows × 31 columns\nWe check the class distribution to see whether the dataset is balanced.\ndf[\"diagnosis\"].value_counts()\n\ndiagnosis\nbenign       357\nmalignant    212\nName: count, dtype: int64\nWe can also examine summary statistics of the numeric features.\ndf.describe().T.head(10)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nmean radius\n569.0\n14.127292\n3.524049\n6.98100\n11.70000\n13.37000\n15.78000\n28.11000\n\n\nmean texture\n569.0\n19.289649\n4.301036\n9.71000\n16.17000\n18.84000\n21.80000\n39.28000\n\n\nmean perimeter\n569.0\n91.969033\n24.298981\n43.79000\n75.17000\n86.24000\n104.10000\n188.50000\n\n\nmean area\n569.0\n654.889104\n351.914129\n143.50000\n420.30000\n551.10000\n782.70000\n2501.00000\n\n\nmean smoothness\n569.0\n0.096360\n0.014064\n0.05263\n0.08637\n0.09587\n0.10530\n0.16340\n\n\nmean compactness\n569.0\n0.104341\n0.052813\n0.01938\n0.06492\n0.09263\n0.13040\n0.34540\n\n\nmean concavity\n569.0\n0.088799\n0.079720\n0.00000\n0.02956\n0.06154\n0.13070\n0.42680\n\n\nmean concave points\n569.0\n0.048919\n0.038803\n0.00000\n0.02031\n0.03350\n0.07400\n0.20120\n\n\nmean symmetry\n569.0\n0.181162\n0.027414\n0.10600\n0.16190\n0.17920\n0.19570\n0.30400\n\n\nmean fractal dimension\n569.0\n0.062798\n0.007060\n0.04996\n0.05770\n0.06154\n0.06612\n0.09744\nVisualization helps reveal differences between classes. For example, we can compare the distributions of a few key features by diagnosis.\nfrom plotnine import ggplot, aes, geom_histogram, facet_wrap, labs\n\nfeatures = [\"mean radius\", \"mean texture\", \"mean area\"]\n\nfor feature in features:\n    p = (\n        ggplot(df, aes(x=feature, fill=\"diagnosis\")) \n        + geom_histogram(bins=20, alpha=0.5, position=\"identity\")\n        + labs(title=feature)\n    )\n    p\nThese plots suggest that malignant and benign tumors differ in several features, such as mean radius and mean area. Such separation indicates that classification methods can be effective in distinguishing between the two groups.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "09-classification.html#evaluating-classifiers",
    "href": "09-classification.html#evaluating-classifiers",
    "title": "9  Classification",
    "section": "9.2 Evaluating Classifiers",
    "text": "9.2 Evaluating Classifiers\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\n\n9.2.1 Confusion Matrix\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity or recall.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour other rates with predicted (column) margins:\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n9.2.2 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n9.2.3 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n9.2.4 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n9.2.5 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n9.2.6 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\). The ROC curve always starts at (0, 0) and ends at (1, 1) because these points represent the extreme threshold settings of the classifier. When the threshold is so high that all predictions are negative, both TPR and the TPR are zero—corresponding to the point (0, 0). When the threshold is so low that all predictions are positive, both TPR and FPR are one—corresponding to the point (1, 1).\nBest classification passes \\((0, 1)\\). The ideal classifier would achieve a TPR of 1 while keeping the FPR at 0. This corresponds to the point (0, 1) in the ROC space. In practice, the closer a classifier’s ROC curve approaches this top-left corner, the better its discriminative performance.\nClassification by random guess gives the 45-degree line. For every threshold, the TPR equals the FPR, because the classifier is just as likely to label a negative instance as positive as it is to label a positive instance correctly. Thus, its points fall on the line where TPR = FPR. This diagonal serves as a baseline: a model whose ROC curve lies on or below this line has no discriminative ability, equivalent to random guessing. Any useful classifier should produce a curve that bows above the diagonal, showing higher TPRs than FPRs across thresholds. The greater the area between the ROC curve and the diagonal, the more informative the model.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results. A higher AUC indicates that the model ranks positive instances higher than negative ones more consistently. Thus, a larger AUC reflects stronger separability between the classes and a more powerful classifier. An AUC of 1 means perfect discrimination, whereas an AUC of 0.5 means random guessing.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n9.2.7 Breast Cancer Example\nSince logistic regression provides a natural starting point for classification, we will apply it to the breast cancer data using a subset of features for simplicity to illustrate the metrics.\nThe response variable y is a binary array indicating whether a tumor is malignant or benign.\n\ny = 0 corresponds to malignant (cancerous) tumors.\ny = 1 corresponds to benign (non-cancerous) tumors.\n\nThis encoding follows scikit-learn’s convention for the Wisconsin Diagnostic Breast Cancer dataset. The dataset includes 569 samples with 30 numeric features derived from digitized images of fine needle aspirate biopsies.\nWhen fitting a logistic regression model, the predicted probabilities (y_pred_prob) represent the estimated probability that a tumor is benign (1). Consequently, high predicted probabilities correspond to benign cases, and low probabilities indicate malignant ones. The ROC and AUC computations use these probabilities as scores for the positive class (benign).\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nX = df[[\"mean radius\", \"mean texture\", \"mean area\"]]\ny = df[\"diagnosis\"].map({\"malignant\":0, \"benign\":1})\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nmodel = LogisticRegression(max_iter=500)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix\n\narray([[ 52,  11],\n       [  5, 103]])\n\n\nWe can compute accuracy, precision, recall, and F1-score to evaluate performance.\n\nprint(classification_report(y_test, y_pred,\n                                target_names=[\"malignant\",\"benign\"]))\n\n              precision    recall  f1-score   support\n\n   malignant       0.91      0.83      0.87        63\n      benign       0.90      0.95      0.93       108\n\n    accuracy                           0.91       171\n   macro avg       0.91      0.89      0.90       171\nweighted avg       0.91      0.91      0.91       171\n\n\n\n\nMalignant tumors: Precision of 0.91 means that 91% of tumors predicted malignant were truly malignant. Recall of 0.83 shows that the model correctly identified 83% of actual malignant tumors but missed 17% (false negatives). The F1-score of 0.87 balances these two aspects.\nBenign tumors: Precision of 0.90 means 90% of predicted benign tumors were correct. Recall of 0.95 shows the model caught 95% of actual benign tumors, misclassifying only 5% as malignant. The F1-score of 0.93 reflects this strong performance.\nOverall: The accuracy of 0.91 indicates that about 91% of tumors were classified correctly. The macro average (simple mean across classes) is slightly lower than the weighted average, reflecting the imbalance in sample sizes. Since benign cases are more common, the weighted average leans closer to their stronger performance.\n\nThe model seems quite accurate overall, but performs better at identifying benign tumors than malignant ones. The relatively lower recall for malignant cases means that some malignant tumors were misclassified as benign. In medical applications, such false negatives are especially serious and motivate the use of evaluation metrics beyond accuracy alone.\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, geom_abline, labs, theme_minimal\n\n# Assume model and data are from previous logistic regression example\n# y_test: true labels (0/1)\n# y_pred_prob: predicted probabilities for the positive class\ny_pred_prob = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc_value = roc_auc_score(y_test, y_pred_prob)\n\n# Create ROC DataFrame\nroc_df = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n})\n\n# Plot ROC curve\n(\n    ggplot(roc_df, aes(x='False Positive Rate', y='True Positive Rate')) +\n    geom_line(color='blue') +\n    geom_abline(linetype='dashed') +\n    labs(\n        title=f'ROC Curve (AUC = {auc_value:.2f})',\n        x='False Positive Rate (1 - Specificity)',\n        y='True Positive Rate (Sensitivity)'\n    ) +\n    theme_minimal()\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "09-classification.html#tuning-regularized-logistic-models",
    "href": "09-classification.html#tuning-regularized-logistic-models",
    "title": "9  Classification",
    "section": "9.3 Tuning Regularized Logistic Models",
    "text": "9.3 Tuning Regularized Logistic Models\nThe logistic regression model with an L1 (lasso) penalty requires a tuning parameter controlling the strength of regularization. In scikit-learn, this parameter is expressed as \\(C = 1 / \\lambda\\). Smaller \\(C\\) values correspond to stronger penalties, shrinking more coefficients toward zero. The goal is to select \\(C\\) that optimizes a performance metric such as F1 or AUC via cross-validation.\n\nStep 1: Data and Setup\n\nWe continue with the breast cancer dataset.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, labs, scale_x_log10, theme_minimal\n\n# Load the data\nX, y = load_breast_cancer(return_X_y=True)\n\n\nStep 2: Automatic Selection of Candidate C Values\n\nRather than manually picking a grid, LogisticRegressionCV determines a data- dependent range of \\(C\\) values by examining the scale of the coefficients. It constructs a grid of regularization strengths internally based on the variance of the features and outcome, ensuring coverage from under-regularized to over- regularized regimes.\n\n# Use LogisticRegressionCV to determine reasonable C values\nauto_lasso = LogisticRegressionCV(\n    Cs=20,\n    penalty=\"l1\",\n    solver=\"saga\",\n    cv=5,\n    scoring=\"roc_auc\",\n    max_iter=5000,\n    n_jobs=-1\n)\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", auto_lasso)\n])\n\npipeline.fit(X, y)\n\n## set tested Cs\nC_values = np.logspace(-2, 1, 20)\n## C_values = auto_lasso.Cs_        \n\n\nStep 3: Cross-Validation for F1 and AUC\n\nWhile LogisticRegressionCV provides built-in AUC-based selection, we can also evaluate each \\(C\\) value using different metrics such as F1-score.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Evaluate across automatically chosen C values\nf1_scores = []\nauc_scores = []\nfor C in C_values:\n    model = LogisticRegression(\n        penalty=\"l1\", solver=\"saga\", C=C, max_iter=5000\n    )\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", model)\n    ])\n    f1_scores.append(cross_val_score(pipe, X, y, cv=cv, scoring=\"f1\").mean())\n    auc_scores.append(cross_val_score(pipe, X, y, cv=cv, scoring=\"roc_auc\").mean())\n\ncv_results = pd.DataFrame({\"C\": C_values, \"F1\": f1_scores, \"AUC\": auc_scores})\n\n\nStep 4: Identify the Optimal Regularization Strength\n\n\nbest_f1 = cv_results.loc[cv_results[\"F1\"].idxmax()]\nbest_auc = cv_results.loc[cv_results[\"AUC\"].idxmax()]\n\nprint(\"Best by F1:\", best_f1)\nprint(\"Best by AUC:\", best_auc)\n\nBest by F1: C      1.623777\nF1     0.979323\nAUC    0.994523\nName: 14, dtype: float64\nBest by AUC: C      0.784760\nF1     0.976486\nAUC    0.995644\nName: 12, dtype: float64\n\n\n\nStep 5: Visualization\n\n\n(\n    ggplot(cv_results.melt(id_vars=\"C\", var_name=\"Metric\", value_name=\"Score\"),\n           aes(x=\"C\", y=\"Score\", color=\"Metric\"))\n    + geom_line()\n    + scale_x_log10()\n    + labs(\n        title=\"Cross-Validation for Lasso Logistic Regression\",\n        x=\"C (Inverse Regularization Strength)\",\n        y=\"Mean 5-Fold Score\"\n    )\n    + theme_minimal()\n)\n\n\n\n\n\n\n\n\nThis approach avoids arbitrary grids. The smallest \\(C\\) corresponds to the strongest regularization (the simplest model), while the largest \\(C\\) allows almost no penalty (the most flexible model). The optimal \\(C\\) often lies between these extremes. A higher AUC indicates better class separation, whereas a higher F1-score indicates balanced precision and recall. Depending on the application, one metric may be prioritized over the other.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "10-supervised.html",
    "href": "10-supervised.html",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1 Introduction\nMachine Learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed. At its core, machine learning algorithms aim to identify patterns in data and use those patterns to make decisions or predictions.\nMachine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type differs in the data it uses and the learning tasks it performs, addressing addresses different tasks and problems. Supervised learning aims to predict outcomes based on labeled data, unsupervised learning focuses on discovering hidden patterns within the data, and reinforcement learning centers around learning optimal actions through interaction with an environment.\nLet’s define some notations to introduce them:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#introduction",
    "href": "10-supervised.html#introduction",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "\\(X\\): A set of feature vectors representing the input data. Each element \\(X_i\\) corresponds to a set of features or attributes that describe an instance of data.\n\\(Y\\): A set of labels or rewards associated with outcomes. In supervised learning, \\(Y\\) is used to evaluate the correctness of the model’s predictions. In reinforcement learning, \\(Y\\) represents the rewards that guide the learning process.\n\\(A\\): A set of possible actions in a given context. In reinforcement learning, actions \\(A\\) represent choices that can be made in response to a given situation, with the goal of maximizing a reward.\n\n\n10.1.1 Supervised Learning\nSupervised learning is the most widely used type of machine learning. In supervised learning, we have both feature vectors \\(X\\) and their corresponding labels \\(Y\\). The objective is to train a model that can predict \\(Y\\) based on \\(X\\). This model is trained on labeled examples, where the correct outcome is known, and it adjusts its internal parameters to minimize the error in its predictions, which occurs as part of the cross-validation process.\nKey tasks in supervised learning include:\n\nClassification: Assigning data points to predefined categories or classes.\nRegression: Predicting a continuous value based on input data.\n\nIn supervised learning, the data consists of both feature vectors \\(X\\) and labels \\(Y\\), namely, \\((X, Y)\\).\n\n\n10.1.2 Unsupervised Learning\nUnsupervised learning involves learning patterns from data without any associated labels or outcomes. The objective is to explore and identify hidden structures in the feature vectors \\(X\\). Since there are no ground-truth labels \\(Y\\) to guide the learning process, the algorithm must discover patterns on its own. This is particularly useful when subject matter experts are unsure of common properties within a data set.\nCommon tasks in unsupervised learning include:\n\nClustering: Grouping similar data points together based on certain features.\nDimension Reduction: Simplifying the input data by reducing the number of features while preserving essential patterns.\n\nIn unsupervised learning, the data consists solely of feature vectors \\(X\\).\n\n\n10.1.3 Reinforcement Learning\nReinforcement learning involves learning how to make a sequence of decisions to maximize a cumulative reward. Unlike supervised learning, where the model learns from a static dataset of labeled examples, reinforcement learning involves an agent that interacts with an environment by taking actions \\(A\\), receiving feedback in the form of rewards \\(Y\\), and learning over time which actions lead to the highest cumulative reward.\nThe process in reinforcement learning involves:\n\nStates: The context or environment the agent is in, represented by feature vectors \\(X\\).\nActions: The set of possible choices the agent can make in response to the current state, denoted as \\(A\\).\nRewards: Feedback the agent receives after taking an action, which guides the learning process.\n\nIn reinforcement learning, the data consists of feature vectors \\(X\\), actions \\(A\\), and rewards \\(Y\\), namely, \\((X, A, Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#decision-trees",
    "href": "10-supervised.html#decision-trees",
    "title": "10  Supervised Learning",
    "section": "10.2 Decision Trees",
    "text": "10.2 Decision Trees\nDecision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed (Breiman et al., 1984). Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.\n\n10.2.1 Recursive Partition Algorithm\nThe core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node \\(m\\), the data subset is denoted as \\(Q_m\\) with a sample size of \\(n_m\\). The objective is to find a candidate split \\(\\theta\\), defined as a threshold for a given feature, that minimizes an impurity or loss measure \\(H\\).\nWhen a split is made at node \\(m\\), the data is divided into two subsets: \\(Q_{m,l}\\) (left node) with sample size \\(n_{m,l}\\), and \\(Q_{m,r}\\) (right node) with sample size \\(n_{m,r}\\). The split quality, measured by \\(G(Q_m, \\theta)\\), is given by:\n\\[\nG(Q_m, \\theta) = \\frac{n_{m,l}}{n_m} H(Q_{m,l}(\\theta)) +\n\\frac{n_{m,r}}{n_m} H(Q_{m,r}(\\theta)).\n\\]\nThe algorithm aims to identify the split that minimizes the impurity:\n\\[\n\\theta^* = \\arg\\min_{\\theta} G(Q_m, \\theta).\n\\]\nThis process is applied recursively at each child node until a stopping condition is met.\n\nStopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.\nPruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves generalization.\n\n\n\n10.2.2 Search Space for Possible Splits\nAt each node, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the feature values. For a given feature, the algorithm considers each of its unique value in the current node as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring effective partition.\nFormally, let the feature set be \\(\\{X_1, X_2, \\ldots, X_p\\}\\), where \\(p\\) is the total number of features, and let the unique values of feature \\(X_j\\) at node \\(m\\) be denoted by \\(\\{v_{m,j,1}, v_{m,j,2}, \\ldots, v_{m,j,k_{mj}}\\}\\). The search space at node \\(m\\) includes:\n\nFeature candidates: \\(\\{X_1, X_2, \\ldots, X_p\\}\\).\nThreshold candidates for \\(X_j\\): \\[\n\\left\\{ \\frac{v_{m,j,i} + v_{m,j,i+1}}{2} \\mid 1 \\leq i &lt; k_{mj} \\right\\}.\n\\]\n\nWhile the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.\n\n\n10.2.3 Metrics\n\n10.2.3.1 Classification\nIn classification, the split quality metric measures how pure the resulting nodes are after a split. A pure node contains observations that predominantly belong to a single class.\n\nGini Index: The Gini index measures node impurity by the probability that two observations randomly drawn from the node belong to different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node \\(m\\), the Gini index is \\[\nH(Q_m) = \\sum_{k=1}^{K} p_{mk} (1 - p_{mk})\n= 1 - \\sum_{k=1}^n p_{mk}^2,\n\\] where \\(p_{mk}\\) is the proportion of samples of class \\(k\\) at node \\(m\\); and \\(K\\) is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including sklearn.\nThe Gini index originates from the Gini coefficient, introduced by Corrado Gini in 1912 to quantify inequality in income distributions. In that context, the Gini coefficient measures how unevenly a quantity (such as wealth) is distributed across a population. Decision tree algorithms adapt this concept of inequality to measure the impurity of a node: instead of wealth, the distribution concerns class membership. A perfectly pure node, where all observations belong to the same class, represents complete equality and yields a Gini index of zero. As class proportions become more mixed, inequality in class membership increases, leading to higher impurity values. Thus, the Gini index used in decision trees can be viewed as a statistical measure of diversity or heterogeneity derived from Gini’s original work on inequality.\nEntropy (Information Gain): Derived from information theory, entropy quantifies the disorder of the data at a node. Lower entropy means higher purity. At node \\(m\\), it is defined as \\[\nH(Q_m) = - \\sum_{k=1}^{K} p_{mk} \\log p_{mk}.\n\\] Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.\nMisclassification Error: Misclassification error focuses on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node \\(m\\), it is defined as \\[\nH(Q_m) = 1 - \\max_k p_{mk},\n\\] where \\(\\max_k p_{mk}\\) is the largest proportion of samples belonging to any class \\(k\\).\n\n\n\n10.2.3.2 Regression Criteria\nIn regression, the goal is to minimize the spread or variance of the target variable within each node.\n\nMean Squared Error (MSE): MSE is the average squared difference between observed and predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} (y_i - \\bar{y}_m)^2,\n\\] where\n\n\\(y_i\\) is the actual value for sample \\(i\\);\n\\(\\bar{y}_m\\) is the mean value of the target at node \\(m\\);\n\\(n_m\\) is the number of samples at node \\(m\\).\n\nMSE works well when the target is continuous and normally distributed.\nHalf Poisson Deviance: Used for count target, the Poisson deviance measures the variance in the number of occurrences of an event. At node \\(m\\), it is \\[\nH(Q_m) = \\sum_{i=1}^{n_m} \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right),\n\\] where \\(\\hat{y}_i\\) is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.\nMean Absolute Error (MAE): MAE aims to minimize the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} |y_i - \\bar{y}_m|.\n\\] MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.\n\n\n\n\n10.2.4 Ames Housing Example\nThe Ames Housing data are used to illustrate a regression tree model for predicting log house price.\nAs before, we retrive the data from OpenML.\n\nimport openml\nimport pandas as pd\nimport numpy as np\n\n# Load Ames Housing dataset (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\ndf[\"LogPrice\"] = np.log(df[\"SalePrice\"])\n\nA decision tree partitions the feature space into regions where the average log price is relatively constant.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nimport plotnine as gg\nimport pandas as pd\n\nnumeric_features = [\n    \"OverallQual\", \"GrLivArea\", \"GarageCars\",\n    \"TotalBsmtSF\", \"YearBuilt\", \"FullBath\"\n]\ncategorical_features = [\"KitchenQual\"]\n\npreprocessor = ColumnTransformer([\n    (\"num\", \"passthrough\", numeric_features),\n    (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n])\n\nX = df[numeric_features + categorical_features]\ny = df[\"LogPrice\"]\n\ndepths = range(2, 11)\ncv_scores = [\n    cross_val_score(\n        Pipeline([\n            (\"pre\", preprocessor),\n            (\"model\", DecisionTreeRegressor(max_depth=d, random_state=0))\n        ]),\n        X, y, cv=5, scoring=\"r2\"\n    ).mean()\n    for d in depths\n]\n\nlist(zip(depths, cv_scores))\n\n[(2, 0.594134995704976),\n (3, 0.6712972027857058),\n (4, 0.7141792234890973),\n (5, 0.748794485599919),\n (6, 0.7587964739851765),\n (7, 0.7343953839481492),\n (8, 0.7186324525934304),\n (9, 0.7112790937242873),\n (10, 0.6938966980572193)]\n\n\nCross-validation identifies an appropriate tree depth that balances fit and generalization. A too-deep tree overfits, while a shallow tree misses structure.\n\ndt = Pipeline([\n    (\"pre\", preprocessor),\n    (\"model\", DecisionTreeRegressor(max_depth=4, random_state=0))\n])\n\ndt.fit(X, y)\ny_pred = dt.predict(X)\n\ndf_pred = pd.DataFrame({\"Observed\": y, \"Predicted\": y_pred})\n\n(gg.ggplot(df_pred, gg.aes(x=\"Observed\", y=\"Predicted\")) +\n gg.geom_point(alpha=0.5) +\n gg.geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n gg.labs(title=\"Decision Tree Regression on Ames Housing\",\n         x=\"Observed Log Price\", y=\"Predicted Log Price\"))\n\n\n\n\n\n\n\n\nThe plot shows predicted versus observed log prices. A well-fitted model has points close to the diagonal. The decision tree naturally captures nonlinear effects and interactions, though its predictions are piecewise constant, producing visible step patterns.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#gradient-boosted-models",
    "href": "10-supervised.html#gradient-boosted-models",
    "title": "10  Supervised Learning",
    "section": "10.3 Gradient-Boosted Models",
    "text": "10.3 Gradient-Boosted Models\nGradient boosting is a powerful ensemble technique in machine learning that combines multiple weak learners into a strong predictive model. Unlike bagging methods, which train models independently, gradient boosting fits models sequentially, with each new model correcting errors made by the previous ensemble (Friedman, 2001). While decision trees are commonly used as weak learners, gradient boosting can be generalized to other base models. This iterative method optimizes a specified loss function by repeatedly adding models designed to reduce residual errors.\n\n10.3.1 Introduction\nGradient boosting builds on the general concept of boosting, aiming to construct a strong predictor from an ensemble of sequentially trained weak learners. The weak learners are often shallow decision trees (stumps), linear models, or generalized additive models (Hastie et al., 2009). Each iteration adds a new learner focusing primarily on the data points poorly predicted by the existing ensemble, thereby progressively enhancing predictive accuracy.\nGradient boosting’s effectiveness stems from:\n\nError Correction: Each iteration specifically targets previous errors, refining predictive accuracy.\nWeighted Learning: Iteratively focuses more heavily on difficult-to-predict data points.\nFlexibility: Capable of handling diverse loss functions and various types of predictive tasks.\n\nThe effectiveness of gradient-boosted models has made them popular across diverse tasks, including classification, regression, and ranking. Gradient boosting forms the foundation for algorithms such as XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018), known for their high performance and scalability.\n\n\n10.3.2 Gradient Boosting Process\nGradient boosting builds an ensemble by iteratively minimizing the residual errors from previous models. This iterative approach optimizes a loss function, \\(L(y, F(x))\\), where \\(y\\) represents the observed target variable and \\(F(x)\\) the model’s prediction for a given feature vector \\(x\\).\nKey concepts:\n\nLoss Function: Guides model optimization, such as squared error for regression or logistic loss for classification.\nLearning Rate: Controls incremental updates, balancing training speed and generalization.\nRegularization: Reduces overfitting through tree depth limitation, subsampling, and L1/L2 penalties.\n\n\n10.3.2.1 Model Iteration\nThe gradient boosting algorithm proceeds as follows:\n\nInitialization: Define a base model \\(F_0(x)\\), typically the mean of the target variable for regression or the log-odds for classification.\nIterative Boosting: At each iteration \\(m\\):\n\nCompute pseudo-residuals representing the negative gradient of the loss function at the current predictions. For each observation \\(i\\): \\[\nr_i^{(m)} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F(x)=F_{m-1}(x)},\n\\] where \\(x_i\\) and \\(y_i\\) denote the feature vector and observed value for the \\(i\\)-th observation, respectively. The residuals represent the direction of steepest descent in function space, so fitting a learner to them approximates a gradient descent step minimizing \\(L(y, F(x))\\).\nFit a new weak learner \\(h_m(x)\\) to these residuals.\nUpdate the model: \\[\nF_m(x) = F_{m-1}(x) + \\eta \\, h_m(x),\n\\] where \\(\\eta\\) is a small positive learning rate (e.g., 0.01–0.1), controlling incremental improvement and reducing overfitting.\n\nIn some implementations, the update step includes an additional multiplier determined by a one-dimensional line search that minimizes the loss function at each iteration. Specifically, the optimal step length is defined as \\[\n\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n\nL\\bigl(y_i,\\, F_{m-1}(x_i) + \\gamma\\, h_m(x_i)\\bigr),\n\\] leading to an updated model of the form \\[\nF_m(x) = F_{m-1}(x) + \\eta\\, \\gamma_m\\, h_m(x),\n\\] where \\(\\eta\\) remains a shrinkage factor controlling the overall rate of learning, while \\(\\gamma_m\\) adjusts the step size adaptively at each iteration.\nFinal Model: After \\(M\\) iterations, the ensemble model is: \\[\n  F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta \\, h_m(x).\n  \\]\n\nStochastic gradient boosting is a variant that enhances gradient boosting by introducing randomness through subsampling at each iteration, selecting a random fraction of data points (typically 50%–80%) to fit the model . This randomness helps reduce correlation among trees, improve model robustness, and lower the risk of overfitting.\n\n\n\n10.3.3 Boosted Trees with Ames Housing\nBoosted trees apply the gradient boosting framework to decision trees. They build an ensemble of shallow trees, each trained to correct the residual errors of the preceding ones. By sequentially emphasizing observations that are difficult to predict, the model progressively improves its overall predictive accuracy. We now apply gradient boosting using the same preprocessed features. Boosting combines many shallow trees, each correcting the residual errors of its predecessors, to improve predictive accuracy.\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Define a range of tree counts for tuning\nn_estimators_list = [50, 100, 200, 400]\n\ncv_scores_gb = [\n    cross_val_score(\n        Pipeline([\n            (\"pre\", preprocessor),\n            (\"model\", GradientBoostingRegressor(\n                n_estimators=n,\n                learning_rate=0.05,\n                max_depth=3,\n                random_state=0))\n        ]),\n        X, y, cv=5, scoring=\"r2\"\n    ).mean()\n    for n in n_estimators_list\n]\n\nlist(zip(n_estimators_list, cv_scores_gb))\n\n[(50, 0.8199019428994425),\n (100, 0.8437789746745888),\n (200, 0.8451433195728157),\n (400, 0.8405078307788265)]\n\n\nCross-validation shows how increasing the number of boosting rounds initially improves performance but eventually risks overfitting when too many trees are added.\n\ngb = Pipeline([\n    (\"pre\", preprocessor),\n    (\"model\", GradientBoostingRegressor(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=0))\n])\n\ngb.fit(X, y)\ny_pred_gb = gb.predict(X)\n\ndf_pred_gb = pd.DataFrame({\"Observed\": y, \"Predicted\": y_pred_gb})\n\n(gg.ggplot(df_pred_gb, gg.aes(x=\"Observed\", y=\"Predicted\")) +\n gg.geom_point(alpha=0.5) +\n gg.geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n gg.labs(title=\"Gradient-Boosted Regression on Ames Housing\",\n         x=\"Observed Log Price\", y=\"Predicted Log Price\"))\n\n\n\n\n\n\n\n\nThe boosted model produces predictions that are generally closer to the 45-degree line than a single tree, reflecting improved accuracy and smoother response across the feature space.\nGradient-boosted trees introduce several parameters that govern model complexity, learning stability, and overfitting control:\n\nn_estimators: the number of trees (boosting rounds). More trees can reduce bias but increase computation and risk of overfitting. learning_rate — the shrinkage parameter \\(\\eta\\) controlling the contribution of each new tree. Smaller values (e.g., 0.05 or 0.01) require more trees but often yield better generalization.\nmax_depth: the maximum depth of each individual tree, limiting the model’s ability to overfit local noise. Shallow trees (depth 2–4) are typical weak learners.\nsubsample: the fraction of data used in each iteration. Values below 1.0 introduce randomness (stochastic boosting), improving robustness and reducing correlation among trees.\nmin_samples_split and min_samples_leaf: minimum numbers of observations required for splitting or forming leaves. These control tree granularity and help regularize the model.\n\nIn practice, moderate learning rates with a sufficiently large number of estimators and shallow trees often perform best, balancing bias, variance, and computational cost.\n\n\n10.3.4 XGBoost: Extreme Gradient Boosting\nXGBoost is a scalable and efficient implementation of gradient-boosted decision trees (Chen & Guestrin, 2016). It has become one of the most widely used machine learning methods for structured data due to its high predictive performance, regularization capabilities, and speed. XGBoost builds an ensemble of decision trees in a stage-wise fashion, minimizing a regularized objective that balances training loss and model complexity.\nThe core idea of XGBoost is to fit each new tree to the gradient of the loss function with respect to the model’s predictions. Unlike traditional boosting algorithms like AdaBoost, which use only first-order gradients, XGBoost optionally uses second-order derivatives (Hessians), enabling better convergence and stability (Friedman, 2001).\nXGBoost is widely used in data science competitions and real-world applications. It supports regularization (L1 and L2), handles missing values internally, and is designed for distributed computing.\nXGBoost builds upon the same foundational idea as gradient boosted machines—sequentially adding trees to improve the predictive model— but introduces a number of enhancements:\n\n\n\n\n\n\n\n\nAspect\nTraditional GBM\nXGBoost\n\n\n\n\nImplementation\nBasic gradient boosting\nOptimized, regularized boosting\n\n\nRegularization\nShrinkage only\nL1 and L2 regularization\n\n\nLoss Optimization\nFirst-order gradients\nFirst- and second-order\n\n\nMissing Data\nRequires manual imputation\nHandled automatically\n\n\nTree Construction\nDepth-wise\nLevel-wise (faster)\n\n\nParallelization\nLimited\nBuilt-in\n\n\nSparsity Handling\nNo\nYes\n\n\nObjective Functions\nFew options\nCustom supported\n\n\nCross-validation\nExternal via GridSearchCV\nBuilt-in xgb.cv\n\n\n\nXGBoost is therefore more suitable for large-scale problems and provides better generalization performance in many practical tasks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#random-forest",
    "href": "10-supervised.html#random-forest",
    "title": "10  Supervised Learning",
    "section": "10.4 Random Forest",
    "text": "10.4 Random Forest\nThis section was prepared by Alex Plotnikov, first-year master’s student in statistics.\n\n10.4.1 Random Forest Algorithm\nRandom forests are an ensemble method that was developed by Leo Breiman in 2001 to improve the performance of tree-based models. This model creates multiple trees in parallel. There are a few key differences in how trees in a random forest are constructed compared to the decision tree algorithm. The main differences are:\n\nSample of data used for each tree.\nNumber of features selected at each node.\nTrees are allowed to grow deep in random forests.\nPrediction is made based on aggregating results from all trees.\n\n\n\n\n\n\n\n\n\n\n\n10.4.1.1 Bagging (Bootstrap Aggregation)\nUnlike decision trees which are trained on the entire dataset, random forests use bagging to create a sample for each tree. Afterwards, the tree is trained on that specific sample.\nThe term bagging comes from bootstrap aggregation. This process involves creating a sample set with the same number of records as the original by sampling from the original dataset with replacement. This means that observations may appear multiple times in the sample dataset.\nFor one draw:\n\\[\nP(\\text{not selected}) = 1 - \\frac{1}{N}.\n\\]\nFor N draws (since we sample N times with replacement):\n\\[\nP(\\text{not selected in entire sample}) = \\left(1 - \\frac{1}{N}\\right)^N.\n\\]\nAs N approaches infinity:\n\\[\n\\left(1 - \\frac{1}{N}\\right)^N \\approx e^{-1} \\approx 0.3679.\n\\]\nSo, about 36.8% of the original observations are not included in a given bootstrap sample.\nTherefore, the remaining portion is:\n\\[\n1 - e^{-1} \\approx 0.632.\n\\]\nOn average, 63.2% of the values in each sample are non-repeated. Bagging helps reduce overfitting to noise and reducing variance of the outcome for a more stable model.\n\n\n10.4.1.2 Feature Sampling and Tree Depth\nRandom forests also calculate impurity on a subset of all features rather than all features as in a decision tree. At each node, the algorithm selects a random subset of features and calculates the impurity metric for these features. At the next node, the algorithm takes another sample of the entire feature set and calculates the metric on this new set of features.\nFor \\(p\\) features, the algorithm uses (\\(\\sqrt{p}\\)) features for classification and (\\(\\frac{p}{3}\\)) features for regression.\nTrees in random forests are also usually allowed to grow to max depth. The goal of having multiple trees is reducing variance of output and overfitting, and this is achieved through averaging of trees.\n\n\n10.4.1.3 Prediction\nRandom forests make predictions by aggregating results from all trees.\nFor classification, this is done by each tree predicting a label and then the forest combines predictions from all trees to output a label based on a majority vote.\nFor regression, the forest takes the numerical value predicted by each tree and then calculates the average for the output.\n\n\n10.4.1.4 Algorithm Comparison\n\n\n\n\n\n\n\n\n\nDecision Trees\nRandom Forests\n\n\n\n\nData Set\nEntire dataset\nBootstrap sample for each tree\n\n\nFeatures\nAll features\nRandom subset at each node\n\n\nTree Size\nBased on input\nCan be modified with input but usually until tree fully grows\n\n\nPrediction\nLeaf where new data lands\nMajority vote of all trees for classification, average for regression\n\n\n\n\n\n\n10.4.2 RandomForestClassifier Parameters\nThese concepts can be adjusted as parameters in the RandomForestClassifier class in Python. Not all parameters are shown here as they are similar to decision tree parameters:\n\nn_estimators: number of trees to train in forest.\nmax_depth: max depth of the tree in the forest, default value is set to None which allows the tree to grow to max depth.\nmax_features: number of features to use at each node. Default value is 'sqrt' and can also be set to 'log2' and None.\nbootstrap: whether or not to use bootstrap for each tree, if False then each tree will use the entire dataset.\nmax_samples: the maximum number of samples for each tree can be adjusted rather than the same size as the dataset.\n\n\n\n10.4.3 Random Forests in Python (Ames Data)\nThe below example shows a random forest regression on the Ames data. We construct a plot to show the observed log price against the predicted log price.\n\nimport openml\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier # For classification\nfrom sklearn.ensemble import RandomForestRegressor # For regression\nfrom sklearn.metrics import accuracy_score, mean_squared_error # For evaluation\nimport plotnine as gg\n\n\n# Load Ames Housing (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\n\ndf[\"LogPrice\"] = np.log(df[\"SalePrice\"])\n\nind_columns = df.columns.drop(['Id','SalePrice','LogPrice']).tolist()\ndf_encoded = pd.get_dummies(df[ind_columns], drop_first=True)\n\nX = df_encoded \ny = df['LogPrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                    test_size=0.2, random_state=42)\n\n# # For Classification\n# model = RandomForestClassifier(n_estimators=100, random_state=42)\n# model.fit(X_train, y_train)\n\n# For Regression (if 'target' was continuous)\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n\n# For graph\ndt = RandomForestRegressor(n_estimators=100, random_state=42)\n\ndt.fit(X, y)\ny_pred = dt.predict(X)\n\ndf_pred = pd.DataFrame({\"Observed\": y, \"Predicted\": y_pred})\n\n(gg.ggplot(df_pred, gg.aes(x=\"Observed\", y=\"Predicted\")) +\n gg.geom_point(alpha=0.5) +\n gg.geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n gg.labs(title=\"Random Forest Regression on Ames Housing\",\n         x=\"Observed Log Price\", y=\"Predicted Log Price\"))\n\n\n\n\n\n\n\n\n\n\n10.4.4 Further Reading\n\nClassification and Regression Trees. Brieman, Friedman, Olshen, Stone 1986.\nBagging Predictors. Brieman 1994.\nRandom Forests. Brieman 2001.\nRandom Forest Class - scikit",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#neural-networks",
    "href": "10-supervised.html#neural-networks",
    "title": "10  Supervised Learning",
    "section": "10.5 Neural Networks",
    "text": "10.5 Neural Networks\nNeural networks learn flexible, nonlinear relationships by composing simple computational units. Each unit (or neuron) applies an affine transformation to its inputs followed by a nonlinearity; stacking layers of such units yields a feed-forward mapping from features to predictions. A helpful anchor is logistic regression: it can be viewed as a one-layer neural network with a sigmoid activation. Hidden layers generalize this idea by transforming inputs into progressively more useful internal representations, enabling the model to capture complex patterns beyond linear decision boundaries.\nHistorically, the field traces back to the early formal model of a neuron by McCulloch & Pitts (1943), followed by Rosenblatt’s perceptron (Rosenblatt, 1958), which was the first trainable linear classifier. The limitations of single-layer perceptrons, notably their inability to model nonlinearly separable functions, were rigorously analyzed by Minsky & Papert (1969), leading to a temporary decline in interest. The introduction of the backpropagation algorithm by Rumelhart et al. (1986) revived the study of neural networks in the 1980s. With the growth of data, advances in hardware, and improved optimization methods, neural networks became the foundation for modern deep learning. Multilayer networks are universal function approximators in principle; in practice, their success depends on careful architectural design, efficient optimization, and appropriate regularization.\n\n\n\n\n\n\nDimensions and Shapes\n\n\n\nFor an input vector \\(x \\in \\mathbb{R}^d\\), a hidden layer with \\(m\\) neurons computes \\(h = \\sigma(Wx + b)\\) through an activation function \\(\\sigma\\) \\(W \\in \\mathbb{R}^{m\\times d}\\) and \\(b \\in \\mathbb{R}^m\\). The output layer applies another affine map (and possibly a final activation) to \\(h\\). Keeping track of these shapes prevents many implementation bugs.\n\n\nThe figure below shows an input layer, one hidden layer, and an output layer with directed connections. (We will formalize the mathematics in the next subsection.)\n\n\n\n\n\n\n\n\nFigure 10.1: A minimal feed-forward neural network with an input layer (3 nodes), one hidden layer (4 nodes), and an output layer (1 node). Edges indicate the direction of information flow.\n\n\n\n\n\n\n10.5.1 Structure of a Neural Network\nA neural network is composed of layers of interconnected processing units called neurons or nodes. Each neuron receives inputs, applies a linear transformation, and passes the result through a nonlinear activation function. The layers are arranged sequentially so that information flows from input features to intermediate representations and finally to the output.\n\n\n10.5.2 Mathematical Formulation\nLet the input vector be \\(x \\in \\mathbb{R}^{d}\\). A hidden layer with \\(m\\) neurons computes\n\\[\nh = \\sigma(Wx + b),\n\\]\nwhere \\(W \\in \\mathbb{R}^{m \\times d}\\) is the weight matrix, \\(b \\in \\mathbb{R}^{m}\\) is the bias vector, and \\(\\sigma(\\cdot)\\) is an activation function applied element-wise.\nSubsequent layers take \\(h\\) as input and repeat the same computation, producing successively transformed representations.\nIf the output layer contains \\(k\\) neurons with linear activations, the network computes\n\\[\n\\hat{y} = Vh + c,\n\\]\nwhere \\(V \\in \\mathbb{R}^{k \\times m}\\) and \\(c \\in \\mathbb{R}^{k}\\) are the output weights and biases.\nCollectively, these parameters define the model’s trainable structure.\n\n\n\n\n\n\nFeed-Forward Flow\n\n\n\nDuring a forward pass, inputs propagate layer by layer: \\(x \\rightarrow h^{(1)} \\rightarrow h^{(2)} \\rightarrow \\cdots\n\\rightarrow \\hat{y}\\).\nBackward propagation of gradients (discussed later) updates all weights based on prediction error.\n\n\n\n\n10.5.3 Network Diagram\nFigure 10.2 illustrates a feed-forward neural network with two hidden layers. Arrows indicate the direction of information flow from one layer to the next. Each connection carries a learnable weight, and nonlinear activations transform the signals as they propagate forward.\n\n\n\n\n\n\n\n\nFigure 10.2: A feed-forward neural network with two hidden layers. Directed arrows represent weighted connections; one example weight \\(w^{(1)}_{23}\\) is annotated to indicate its origin and destination neurons.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEach connection (edge) has an associated weight adjusted during training. The depth of a network refers to the number of hidden layers, and the width refers to the number of neurons per layer.\n\n\n\n\n10.5.4 Activation Functions\nActivation functions introduce nonlinearity into neural networks. Without them, the entire network would collapse into a single linear transformation, regardless of the number of layers. Nonlinear activations allow networks to approximate arbitrary complex functions, enabling them to model curved decision boundaries and hierarchical representations.\nLet \\(z\\) denote the input to a neuron before activation. The most common choices are:\n\n\n\n\n\n\n\n\n\nFunction\nFormula\nRange\nKey Property\n\n\n\n\nSigmoid\n\\(\\displaystyle \\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\\((0,1)\\)\nSmooth, bounded; used in early networks and binary outputs\n\n\nTanh\n\\(\\displaystyle \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)\n\\((-1,1)\\)\nZero-centered; stronger gradients than sigmoid\n\n\nReLU\n\\(\\displaystyle \\mathrm{ReLU}(z) = \\max(0, z)\\)\n\\([0, \\infty)\\)\nSparse activations; efficient to compute\n\n\nLeaky ReLU\n\\(\\displaystyle \\mathrm{LReLU}(z) = \\max(0.01z, z)\\)\n\\((-\\infty, \\infty)\\)\nAvoids “dead” neurons of ReLU\n\n\n\nFigure 10.3 shows the shape of these activation functions. Notice how ReLU sharply truncates negative values while sigmoid and tanh saturate for large magnitudes.\n\n\n\n\n\n\n\n\nFigure 10.3: Activation functions commonly used in neural networks. ReLU and its variants introduce sparsity, while sigmoid and tanh are smooth and saturating.\n\n\n\n\n\n\n\n10.5.5 Training Neural Networks\nTraining a neural network means adjusting its parameters so that its predictions \\(\\hat{y}\\) align closely with the true outputs \\(y\\). This is done by minimizing a loss function that quantifies prediction error. Optimization proceeds iteratively through forward and backward passes.\n\n10.5.5.1 Loss Functions\nThe choice of loss depends on the task:\n\n\n\n\n\n\n\n\nTask\nTypical Loss Function\nExpression\n\n\n\n\nRegression\nMean squared error (MSE)\n\\(\\displaystyle L = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\n\n\nBinary classification\nBinary cross-entropy\n\\(\\displaystyle L = -\\frac{1}{n}\\sum_{i=1}^{n} \\left[y_i \\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i)\\right]\\)\n\n\nMulticlass classification\nCategorical cross-entropy\n\\(\\displaystyle L = -\\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik}\\log \\hat{y}_{ik}\\)\n\n\n\nThe loss function \\(L(\\theta)\\) depends on all network parameters \\(\\theta = \\{W, b, V, c, \\ldots\\}\\) and guides the optimization.\n\n\n10.5.5.2 Gradient Descent\nNeural networks are trained by gradient descent, which updates parameters in the opposite direction of the gradient of the loss:\n\\[\n\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta^{(t)}),\n\\]\nwhere \\(\\eta &gt; 0\\) is the learning rate controlling the step size. Choosing \\(\\eta\\) too small leads to slow convergence; too large can cause divergence.\nVariants such as stochastic and mini-batch gradient descent compute gradients on subsets of data to speed learning and improve generalization.\nFigure 10.4 gives a two-dimensional analogy of gradient descent. In practice, a neural network may contain millions of parameters, meaning the loss function is defined over an extremely high-dimensional space. The contours shown here merely represent a projection of that space onto two dimensions for illustration.\nThe true optimization surface is much more complex—full of flat regions, sharp valleys, and numerous local minima—but the intuition of moving “downhill” along the loss gradient remains valid.\n\n\n\n\n\n\n\n\nFigure 10.4: Conceptual illustration of gradient descent. The optimizer iteratively moves parameters down the loss surface toward a local minimum.\n\n\n\n\n\n\n\n\n10.5.6 Regularization and Overfitting\nBecause neural networks contain large numbers of parameters, they can easily overfit training data—capturing noise or random fluctuations rather than generalizable patterns. A model that overfits performs well on the training set but poorly on new data. Regularization techniques introduce constraints or randomness that help the network generalize.\n\n10.5.6.1 The Bias–Variance Trade-Off\nAdding model complexity (more layers or neurons) reduces bias but increases variance. The goal of regularization is to find a balance between these forces. Figure 10.5 provides a schematic view: the unregularized model follows every data fluctuation, while a regularized model captures only the underlying trend.\n\n\n\n\n\n\n\n\nFigure 10.5: Illustration of overfitting and regularization. The unregularized model follows noise in the data, whereas the regularized model captures the broader pattern.\n\n\n\n\n\n\n\n10.5.6.2 Common Regularization Techniques\nAmong the simplest and most widely used techniques is L2 weight decay, which adds a penalty term to discourage large weights:\n\\[\nL_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_{l}\\sum_{i,j} (w_{ij}^{(l)})^2,\n\\]\nwhere \\(\\lambda\\) controls the strength of the constraint. Other general approaches include dropout, which randomly deactivates neurons during training to prevent reliance on any single pathway, and early stopping, which halts training once the validation loss stops improving. These ideas apply to networks of all depths and form the foundation for more advanced regularization strategies discussed later in the deep learning section.\n\n\n\n\n\n\nWhy Regularization Works\n\n\n\nRegularization methods restrict how freely the model parameters can adapt to training data. This constraint encourages smoother mappings, reduces sensitivity to noise, and improves generalization to unseen inputs.\n\n\n\n\n\n\n\n\nPractical Tips\n\n\n\n\nStart with small \\(\\lambda\\) for L2 weight decay.\n\nUse dropout (e.g., \\(p = 0.2\\)–\\(0.5\\)) between dense layers.\n\nAlways track training vs. validation loss curves to detect overfitting.\n\n\n\n\n\n\n\n10.5.7 Example: A Simple Feed-Forward Network\nTo illustrate how a neural network operates in practice, we train a simple multilayer perceptron (MLP) on a two-dimensional nonlinear dataset. The model learns a curved decision boundary that cannot be captured by linear classifiers.\nThe two-moons dataset consists of two interleaving half-circles, forming a pattern that resembles a pair of crescents. Each point represents an observation with two features \\((x_1, x_2)\\), and the color indicates its class. The two classes overlap slightly due to added random noise.\nA linear classifier such as logistic regression would draw a straight line through the plane, misclassifying many points. In contrast, a neural network can learn the nonlinear boundary that curves along the interface between the moons. This problem, although simple, captures the essence of nonlinear learning in higher dimensions.\nWe generate the data using make_moons() from scikit-learn and train a feed-forward neural network with two hidden layers. The model’s task is to assign each observation to one of the two moon shapes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\n# Generate synthetic data\nX, y = make_moons(n_samples=500, noise=0.25, random_state=1023)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Define and train a small neural network\nmlp = MLPClassifier(hidden_layer_sizes=(10, 5),\n                    activation='relu',\n                    solver='adam',\n                    alpha=0.001,\n                    max_iter=2000,\n                    random_state=1023)\nmlp.fit(X_train, y_train)\n\nMLPClassifier(alpha=0.001, hidden_layer_sizes=(10, 5), max_iter=2000,\n              random_state=1023)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifier?Documentation for MLPClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nhidden_layer_sizes \n(10, ...)\n\n\n\nactivation \n'relu'\n\n\n\nsolver \n'adam'\n\n\n\nalpha \n0.001\n\n\n\nbatch_size \n'auto'\n\n\n\nlearning_rate \n'constant'\n\n\n\nlearning_rate_init \n0.001\n\n\n\npower_t \n0.5\n\n\n\nmax_iter \n2000\n\n\n\nshuffle \nTrue\n\n\n\nrandom_state \n1023\n\n\n\ntol \n0.0001\n\n\n\nverbose \nFalse\n\n\n\nwarm_start \nFalse\n\n\n\nmomentum \n0.9\n\n\n\nnesterovs_momentum \nTrue\n\n\n\nearly_stopping \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nbeta_1 \n0.9\n\n\n\nbeta_2 \n0.999\n\n\n\nepsilon \n1e-08\n\n\n\nn_iter_no_change \n10\n\n\n\nmax_fun \n15000\n\n\n\n\n            \n        \n    \n\n\nAfer the model is trained, we evaluate its accuracy.\n\n# Evaluate accuracy\nacc_train = mlp.score(X_train, y_train)\nacc_test = mlp.score(X_test, y_test)\n\n# Prepare grid for decision boundary\nxx, yy = np.meshgrid(np.linspace(-2, 3, 300),\n                     np.linspace(-1.5, 2, 300))\nZ = mlp.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 5))\nax.contourf(xx, yy, Z, cmap=\"coolwarm\", alpha=0.3)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", edgecolor=\"k\", s=40)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_title(f\"Two-Moons Classification (Train Acc: {acc_train:.2f}, Test Acc: {acc_test:.2f})\")\nplt.show()\n\n\n\n\n\n\n\nFigure 10.6: Decision boundary learned by a simple multilayer perceptron on the two-moons dataset. The network captures the nonlinear separation between the classes.\n\n\n\n\n\nThe trained network achieves high accuracy on both the training and test sets, demonstrating good generalization. The contour regions in Figure Figure 10.6 reveal the curved boundary the network has learned. Each hidden layer transforms the input coordinates into a new representation where the two classes become more separable. After a few layers of nonlinear transformations, the output layer can classify the points with a simple linear threshold.\n\n\n\n\n\n\nNote\n\n\n\nEven this small network, with only a few dozen parameters, can learn a highly nonlinear decision surface. This illustrates the expressive power of multilayer neural networks, even in low-dimensional spaces.\n\n\n\n\n\n\n\n\nTry It Yourself\n\n\n\nIncrease the noise level in the data to test robustness. Change the hidden layer sizes (e.g., (3,), (20,10)), or the activation function (tanh, logistic). Adjust the regularization parameter alpha to see how it affects smoothness of the boundary.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#neural-networks-for-data-with-temporal-dependence",
    "href": "10-supervised.html#neural-networks-for-data-with-temporal-dependence",
    "title": "10  Supervised Learning",
    "section": "10.6 Neural Networks for Data with Temporal Dependence",
    "text": "10.6 Neural Networks for Data with Temporal Dependence\nMany real-world datasets are sequential, where earlier observations influence what happens later. Examples include electricity demand over hours, temperature across days, and stock prices through trading sessions. Such data exhibit temporal dependence, meaning that successive observations are not independent.\nTraditional supervised learning models, such as linear regression and feedforward neural networks, treat each observation as if it were independent. When applied directly to time-ordered data, they fail to capture how information evolves through time. A prediction for one step does not reflect patterns that unfolded earlier.\nTo learn from sequential patterns, we need models that can remember what has already occurred and use that information to improve predictions. Neural networks designed for temporal dependence achieve this by introducing internal states that are updated as the sequence unfolds. The simplest such model is the recurrent neural network (RNN), which forms the basis for more advanced architectures such as long short-term memory (LSTM) and gated recurrent unit (GRU) networks.\n\n10.6.1 Recurrent Neural Networks (RNNs)\nTo model data with temporal dependence, a neural network must be able to retain information about what has happened previously. A recurrent neural network (RNN) accomplishes this by maintaining an internal hidden state that evolves over time. The hidden state acts as a summary of all past inputs and is updated as new data arrive.\nAt each time step \\(t\\), an RNN receives an input vector \\(x_t\\) and produces a hidden state \\(h_t\\) according to\n\\[\nh_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h),\n\\]\nwhere \\(W_h\\) and \\(W_x\\) are weight matrices and \\(b_h\\) is a bias term. The output at the same step can be expressed as\n\\[\n\\hat{y}_t = \\sigma(W_y h_t + b_y),\n\\]\nwith \\(\\sigma(\\cdot)\\) representing an activation or link function. Because \\(h_t\\) depends on \\(h_{t-1}\\), the network can in principle capture relationships across time.\nThe initial hidden state \\(h_0\\) must be specified before the sequence starts. In most applications, \\(h_0\\) is set to a vector of zeros with the same dimension as \\(h_t\\), allowing the network to begin without prior memory. This default works well because the recurrent updates quickly overwrite the initial state as new inputs arrive. In some advanced or stateful applications, \\(h_0\\) can instead be learned during training or carried over from the final state of a previous sequence, enabling the model to preserve continuity across batches.\nBefore training can begin, an objective function must be defined to measure how well the network predicts the target sequence. For a series of observations \\(\\{(x_t, y_t)\\}_{t=1}^T\\), the total loss is typically the sum of stepwise prediction errors, \\[\n\\mathcal{L} = \\sum_{t=1}^T \\ell(y_t, \\hat{y}_t),\n\\] where \\(\\ell\\) is a suitable loss such as mean squared error for regression or cross-entropy for classification. The gradients of \\(\\mathcal{L}\\) with respect to the network parameters are then computed and used to update the weights through backpropagation through time.\n\n\n\n\n\n\n\n\nFigure 10.7: An unrolled RNN showing how the hidden state connects across time steps.\n\n\n\n\n\nFigure 10.7 illustrates how an RNN can be unrolled across time steps, showing that the same set of weights is reused at each step. The hidden state serves as a bridge between past and present inputs, allowing the network to accumulate information through time.\nTraining an RNN is done by backpropagation through time (BPTT), which unrolls the network over all time steps and applies gradient descent. However, when sequences are long, the repeated multiplication of gradients can lead to vanishing or exploding gradients. This makes it difficult for a standard RNN to learn long-term dependencies, limiting its ability to remember events far in the past.\nIn many applications, temporal dependence is only one part of the problem. Alongside the time-varying input \\(x_t\\), there may be additional covariates \\(z\\) that describe static or slowly changing characteristics, such as a station ID, region, or weather condition. These can be incorporated into an RNN by concatenating them with \\(x_t\\) at each time step or by feeding them into separate layers whose outputs are combined with the recurrent representation. In practice, the design depends on whether such covariates are constant across time or vary together with the sequence.\nTo address the limitations of standard RNNs, researchers developed architectures that explicitly control how information is remembered or forgotten. The most influential of these is the LSTM network, which introduces a structured memory cell and gating mechanisms to stabilize learning over longer sequences.\n\n\n10.6.2 Long Short-Term Memory (LSTM)\nThe main limitation of a standard RNN is its inability to retain information over long sequences. During backpropagation through time, gradients tend to either vanish or explode, preventing effective learning of long-term dependencies. The Long Short-Term Memory (LSTM) network, proposed by Hochreiter and Schmidhuber (1997), was designed to overcome this problem.\nAn LSTM introduces a separate cell state \\(C_t\\) that acts as a highway for information to flow across time steps, along with gating mechanisms that regulate what to remember and what to forget. The gates use sigmoid activations to produce values between 0 and 1, allowing the network to scale information rather than overwrite it.\nThe key update equations of an LSTM are\n\\[\n\\begin{aligned}\nf_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f), \\\\\ni_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i), \\\\\n\\tilde{C}_t &= \\tanh(W_C [h_{t-1}, x_t] + b_C), \\\\\nC_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t, \\\\\no_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o), \\\\\nh_t &= o_t \\odot \\tanh(C_t),\n\\end{aligned}\n\\]\nwhere \\(\\odot\\) denotes element-wise (Hadamard) multiplication and \\(\\sigma(\\cdot)\\) is the logistic sigmoid function. Each gate \\(f_t\\), \\(i_t\\), and \\(o_t\\) outputs values between 0 and 1 that determine how information flows through the cell.\nThe activation functions \\(\\tanh(\\cdot)\\) and \\(\\sigma(\\cdot)\\) play specific roles in the LSTM design. The sigmoid \\(\\sigma\\) compresses values to the range \\((0,1)\\), making it suitable for gate control because it behaves like a smooth on–off switch. The hyperbolic tangent \\(\\tanh\\) maps inputs to \\((-1,1)\\), allowing both positive and negative contributions to the cell state.\nOther activation functions can in principle replace \\(\\tanh\\), such as ReLU or Leaky ReLU, but this is uncommon in practice. ReLU may cause the cell state to grow without bound, and smooth symmetric activations like \\(\\tanh\\) are generally more stable for recurrent updates. Some modern variants, such as the Peephole LSTM and GRU, adjust or simplify these activations, but the original combination of \\(\\sigma\\) and \\(\\tanh\\) remains the standard choice.\n\n\n\n\n\n\n\n\nFigure 10.8: Structure of an LSTM cell showing the flow of information through the input, forget, and output gates.\n\n\n\n\n\nEach of the three gates in an LSTM serves a distinct role.\nThe forget gate \\(f_t\\) determines how much of the previous cell state \\(C_{t-1}\\) should be retained, effectively deciding what information to discard. The input gate \\(i_t\\) controls how much new information \\(\\tilde{C}_t\\) enters the cell state, allowing the network to incorporate relevant updates. The output gate \\(o_t\\) regulates how much of the cell state is exposed as the hidden state \\(h_t\\), influencing the network’s prediction at the current step. Together, these gates maintain a balance between remembering long-term patterns and adapting to new signals. Figure Figure 10.8 illustrates how the three gates interact with the cell state and hidden states to manage information flow through time.\n\n\n10.6.3 Gated Recurrent Unit (GRU)\nThe Gated Recurrent Unit (GRU), introduced by Cho et al. (2014), is a simplified variant of the LSTM that retains its ability to capture long-term dependencies while using fewer parameters. The GRU combines the roles of the input and forget gates into a single update gate and omits the separate cell state \\(C_t\\), relying only on the hidden state \\(h_t\\) to store information.\nThe GRU update equations are\n\\[\n\\begin{aligned}\nz_t &= \\sigma(W_z [h_{t-1}, x_t] + b_z), \\\\\nr_t &= \\sigma(W_r [h_{t-1}, x_t] + b_r), \\\\\n\\tilde{h}_t &= \\tanh(W_h [r_t \\odot h_{t-1}, x_t] + b_h), \\\\\nh_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t,\n\\end{aligned}\n\\]\nwhere \\(z_t\\) is the update gate and \\(r_t\\) is the reset gate. The update gate controls how much of the previous hidden state to keep, while the reset gate determines how strongly past information should influence the new candidate state \\(\\tilde{h}_t\\).\n\n\n\n\n\n\n\n\nFigure 10.9: Structure of a GRU cell showing the update and reset gates.\n\n\n\n\n\nThe structure of a GRU cell is illustrated in Figure 10.9. Compared with an LSTM, the GRU is computationally simpler because it has no separate cell state and fewer matrix operations. Despite this simplification, GRUs often perform as well as LSTMs, especially when datasets are smaller or sequence lengths are moderate.\n\n\n10.6.4 Example: Forecasting a Synthetic Sequential Signal (PyTorch)\nTo compare recurrent architectures in a reproducible way, we use a synthetic sine-wave signal with random noise. This allows us to train RNN, LSTM, and GRU models side-by-side without large datasets or external dependencies.\n\n10.6.4.1 Step 1. Generate the data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\ntime = np.arange(0, 200)\nsignal = np.sin(time / 6) + 0.3 * np.random.randn(200)\ndf = pd.DataFrame({\"time\": time, \"signal\": signal})\n\nplt.figure(figsize=(6, 3))\nplt.plot(df[\"time\"], df[\"signal\"])\nplt.xlabel(\"Time\")\nplt.ylabel(\"Signal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nSynthetic sine-wave signal with random noise.\n\n\n\n\n\n\n10.6.4.2 Step 2. Prepare input sequences\nEach training example uses the previous 20 observations to predict the next value.\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\n\nL = 20\nscaler = MinMaxScaler()\nscaled = scaler.fit_transform(df[[\"signal\"]])\n\nX, y = [], []\nfor t in range(L, len(scaled)):\n    X.append(scaled[t-L:t, 0])\n    y.append(scaled[t, 0])\nX, y = np.array(X), np.array(y)\nX = X.reshape(X.shape[0], L, 1)\n\n# split and convert to tensors\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=False)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\nX_test  = torch.tensor(X_test, dtype=torch.float32)\ny_test  = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n\n\n\n10.6.4.3 Step 3. Define recurrent models\n\nclass RecurrentModel(nn.Module):\n    def __init__(self, rnn_type=\"RNN\", hidden_size=50):\n        super().__init__()\n        if rnn_type == \"LSTM\":\n            self.rnn = nn.LSTM(1, hidden_size, batch_first=True)\n        elif rnn_type == \"GRU\":\n            self.rnn = nn.GRU(1, hidden_size, batch_first=True)\n        else:\n            self.rnn = nn.RNN(1, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc(out[:, -1, :])\n\n\n\n10.6.4.4 Step 4. Train and evaluate\n\ndef train_model(model, X, y, epochs=50, lr=0.01):\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    for _ in range(epochs):\n        optimizer.zero_grad()\n        loss = criterion(model(X), y)\n        loss.backward()\n        optimizer.step()\n    return loss.item()\n\ndef predict(model, X):\n    model.eval()\n    with torch.no_grad():\n        return model(X).numpy()\n\nmodels = {\n    \"RNN\":  RecurrentModel(\"RNN\"),\n    \"LSTM\": RecurrentModel(\"LSTM\"),\n    \"GRU\":  RecurrentModel(\"GRU\")\n}\n\nfor name, m in models.items():\n    final_loss = train_model(m, X_train, y_train)\n    print(f\"{name} final training loss: {final_loss:.5f}\")\n\ny_preds = {name: predict(m, X_test) for name, m in models.items()}\n\nRNN final training loss: 0.01107\nLSTM final training loss: 0.02883\nGRU final training loss: 0.01470\n\n\n\n\n10.6.4.5 Step 5. Compare RMSE and MAE\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ndef metrics(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    return rmse, mae\n\nfor name, y_hat in y_preds.items():\n    rmse, mae = metrics(y_test, y_hat)\n    print(f\"{name:5s} – RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nRNN   – RMSE: 0.0992, MAE: 0.0828\nLSTM  – RMSE: 0.1803, MAE: 0.1533\nGRU   – RMSE: 0.1121, MAE: 0.0928\n\n\n\n\n10.6.4.6 Step 6. Visual comparison\n\nplt.figure(figsize=(6.5, 4))\nplt.plot(y_test[:100], label=\"Observed\", color=\"black\")\nfor name, y_hat in y_preds.items():\n    plt.plot(y_hat[:100], label=name, alpha=0.7)\nplt.xlabel(\"Time index\")\nplt.ylabel(\"Scaled signal\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nObserved and predicted values using RNN, LSTM, and GRU (PyTorch).\n\n\n\n\n\n\n10.6.4.7 Discussion\nAll three networks capture the oscillatory pattern, but the vanilla RNN has difficulty preserving phase alignment when the sequence is long. Both the LSTM and GRU learn the dependency structure more reliably. The GRU reaches nearly the same accuracy as the LSTM while training faster, thanks to its simpler gating design.\n\n\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984). Classification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5), 1189–1232.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., & Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. Advances in Neural Information Processing Systems, 3146–3154.\n\n\nMcCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4), 115–133.\n\n\nMinsky, M., & Papert, S. A. (1969). Perceptrons: An introduction to computational geometry. MIT Press.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. Advances in Neural Information Processing Systems, 6638–6648.\n\n\nRosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386.\n\n\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533–536. https://doi.org/10.1038/323533a0",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11-unsupervised.html",
    "href": "11-unsupervised.html",
    "title": "11  Unsupervised Learning",
    "section": "",
    "text": "11.1 Principal Component Analysis\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset with potentially correlated features into a set of uncorrelated components. These components are ordered by the amount of variance each one captures, allowing PCA to summarize the data structure while retaining the most informative features. This approach is widely used in unsupervised learning, particularly for data compression and noise reduction.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "11-unsupervised.html#principal-component-analysis",
    "href": "11-unsupervised.html#principal-component-analysis",
    "title": "11  Unsupervised Learning",
    "section": "",
    "text": "11.1.1 Theory\nPCA works by identifying directions, or “principal components,” along which the variance of the data is maximized. Let \\(X\\) be a dataset with \\(n\\) observations and \\(p\\) features, represented as an \\(n \\times p\\) matrix. The principal components are derived from the eigenvectors of the data’s covariance matrix, representing directions of greatest variation.\n\nStandardization: To ensure each feature contributes equally, features in \\(X\\) are often standardized to have zero mean and unit variance. Without this step, variables with larger scales can dominate the resulting components.\nCovariance Matrix: Compute the covariance matrix \\(S\\) of the data as:\n\\[\nS = \\frac{1}{n-1} X_c^\\top X_c,\n\\]\nwhere \\(X_c\\) is the centered version of \\(X\\). This matrix measures how pairs of features vary together.\nEigenvalue Decomposition: The eigenvectors of \\(S\\) represent the principal components, and the associated eigenvalues quantify the variance each component captures.\nDimensionality Reduction: Select the top \\(k\\) eigenvectors with the largest eigenvalues and project \\(X\\) onto them: \\[\nX_{\\text{reduced}} = X W_k,\n\\] where \\(W_k\\) contains these eigenvectors as columns.\nThe resulting lower-dimensional data retains most of the variation in \\(X\\).\n\n\n\n11.1.2 Properties of PCA\nPCA has several important properties that make it valuable for unsupervised learning:\n\nVariance Maximization: The first principal component is the direction that maximizes variance in the data. Each subsequent component maximizes variance under the constraint of being orthogonal to previous components.\nOrthogonality: Principal components are orthogonal to each other, ensuring that each captures unique information. This property transforms the data into an uncorrelated space, simplifying further analysis.\nDimensionality Reduction: By selecting only components with the largest eigenvalues, PCA enables dimensionality reduction while preserving most of the data’s variability. This is especially useful for large datasets.\nReconstruction: If all components are retained, the original data can be perfectly reconstructed. When fewer components are used, the reconstruction is approximate but retains the essential structure of the data.\nSensitivity to Scaling: PCA is sensitive to the scale of input data, so standardization is often necessary to ensure that each feature contributes equally to the analysis.\n\n\n\n11.1.3 Interpreting PCA Results\nThe output of PCA provides several insights into the data:\n\nPrincipal Components: Each principal component represents a linear combination of the original features. The loadings (or weights) for each feature indicate the contribution of that feature to the component. Large weights (positive or negative) suggest that the corresponding feature strongly influences the principal component.\nExplained Variance: Each principal component captures a specific amount of variance in the data. The proportion of variance explained by each component helps determine how many components are needed to retain the key information in the data. For example, if the first two components explain 90% of the variance, then these two components are likely sufficient to represent the majority of the data’s structure.\nSelecting the Number of Components: The cumulative explained variance plot indicates the total variance captured as more components are included. A common approach is to choose the number of components such that the cumulative variance reaches an acceptable threshold (e.g., 95%). This helps in balancing dimensionality reduction with information retention.\nInterpretation of Component Scores: The transformed data points, or “scores,” in the principal component space represent each original observation as a combination of the selected principal components. Observations close together in this space have similar values on the selected components and may indicate similar patterns.\nIdentifying Patterns and Clusters: By visualizing the data in the reduced space, patterns and clusters may become more apparent, especially in cases where there are inherent groupings in the data. These patterns can provide insights into underlying relationships between observations.\n\nPCA thus offers a powerful tool for both reducing data complexity and enhancing interpretability by transforming data into a simplified structure, with minimal loss of information.\n\n\n11.1.4 Example: PCA on 8x8 Digit Data\nThe 8x8 digit dataset contains grayscale images of handwritten digits (0 through 9), each stored as an 8x8 grid of pixel intensities. Each pixel intensity serves as a feature, giving 64 features per image. The dataset is thus high-dimensional, yet its underlying structure is visually simple.\n\n11.1.4.1 Loading and Visualizing the Data\nWe begin by loading the data and displaying a few sample images to understand its structure.\n\n# Import required libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\n\n# Load the 8x8 digit dataset\ndigits = load_digits()\nX = digits.data  # feature matrix: 64 features (8x8 pixels)\ny = digits.target  # target labels (0-9 digit classes)\n\n# Display the shape of the data\nprint(\"Feature matrix shape:\", X.shape)\nprint(\"Target vector shape:\", y.shape)\n\n# Plot some sample images from the dataset\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X[i].reshape(8, 8), cmap='gray')\n    ax.set_title(f\"Digit: {y[i]}\")\n    ax.axis('off')\nplt.suptitle(\"Sample Images from 8x8 Digit Dataset\", fontsize=16)\nplt.show()\n\nFeature matrix shape: (1797, 64)\nTarget vector shape: (1797,)\n\n\n\n\n\n\n\n\nFigure 11.1: Sample 8×8 grayscale images from the handwritten digit dataset.\n\n\n\n\n\nAfter visualizing the data, we note:\n\nEach digit corresponds to an 8×8 grid of pixels, forming a 64-dimensional feature space.\nDespite the high dimensionality, many features (pixels) are correlated or redundant.\nPCA can therefore help summarize the data while retaining essential structure.\n\nBecause the dataset is high-dimensional, PCA can address several key questions:\n\nDimensionality Reduction: Can we reduce the dataset’s dimensionality while preserving the essential structure of each digit? This simplification may improve visualization and computational efficiency.\nVariance Explained: How many principal components are needed to capture most of the variance? Determining this shows how many features meaningfully distinguish digits.\nCluster Structure: Do distinct clusters appear in the reduced component space? Plotting the first few components may reveal natural groupings by digit class.\n\n\n\n11.1.4.2 Performing PCA and Plotting Variance Contribution\nWe now apply PCA to the digit data and examine how much variance each principal component explains. This analysis helps determine the number of components that provide a good balance between dimensionality reduction and information retention.\nOur goal is to identify how many components capture most of the variance. A cumulative explained variance plot will illustrate how the total variance increases as additional components are included.\n\n# Import the PCA module\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Initialize PCA without specifying the number of components\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Calculate the explained variance ratio for each component\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# Plot variance contributions\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n# Individual explained variance\naxes[0].plot(\n    np.arange(1, len(explained_variance) + 1),\n    explained_variance,\n    marker=\"o\"\n)\naxes[0].set_xlabel(\"Principal Component\")\naxes[0].set_ylabel(\"Explained Variance Ratio\")\naxes[0].set_title(\"Variance by Component\")\n\n\n# Cumulative explained variance\naxes[1].plot(\n    np.arange(1, len(cumulative_variance) + 1),\n    cumulative_variance,\n    marker=\"o\"\n)\naxes[1].set_xlabel(\"Number of Components\")\naxes[1].set_ylabel(\"Cumulative Explained Variance\")\naxes[1].set_title(\"Cumulative Variance\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 11.2: Variance contribution of each principal component and cumulative explained variance for the digit dataset.\n\n\n\n\n\nThe plots in Figure 11.2 show how variance is distributed across components.\n\nVariance by Component: The left panel displays the variance explained by each component. Components with larger contributions represent the most informative directions of variation.\nCumulative Variance: The right panel shows the cumulative variance as the number of components increases. The curve helps identify an efficient cutoff for dimension reduction.\n\nTo select the number of components:\n\nVariance Threshold: Select the smallest number of components that explain a desired proportion of variance, such as 90% or 95%.\nElbow Method: Choose the elbow point on the cumulative variance curve, balancing compactness and representational accuracy.\n\nIn this dataset, the first 10 components account for roughly 75% of the variance, while about 50 components are required to capture nearly all variance.\n\n\n11.1.4.3 PCA in Dimension Reduction\nPCA can also be used to visualize high-dimensional data in a lower- dimensional space. Here we project the digit data onto the first two and first three principal components to observe how well PCA captures the underlying structure and whether the digits form distinct clusters in reduced dimensions.\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Apply PCA for 2D and 3D projections\npca_2d = PCA(n_components=2)\nX_pca_2d = pca_2d.fit_transform(X)\n\npca_3d = PCA(n_components=3)\nX_pca_3d = pca_3d.fit_transform(X)\n\n# 2D projection\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap=\"tab10\",\n                      s=15, alpha=0.7)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"2D PCA Projection of Digit Data\")\nplt.colorbar(scatter, label=\"Digit Label\")\nplt.show()\n\n# 3D projection\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection=\"3d\")\nscatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],\n                     c=y, cmap=\"tab10\", s=15, alpha=0.7)\nax.set_xlabel(\"PC 1\")\nax.set_ylabel(\"PC 2\")\nax.set_zlabel(\"PC 3\")\nax.set_title(\"3D PCA Projection of Digit Data\")\nfig.colorbar(scatter, ax=ax, label=\"Digit Label\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(a) 2D and 3D PCA projections of the 8×8 digit data, showing clustering by digit class.\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 11.3\n\n\n\n\nThe 3D projection in Figure 11.3 shows each image’s position in the space defined by the first three principal components. Several observations emerge:\n\nCluster Formation: Distinct clusters of points represent different digits. Digits with similar shapes, such as “1” and “7” (both often vertical), may appear closer to each other in this reduced space. This clustering suggests that PCA effectively captures structural features, even when reducing dimensions.\nEffectiveness of Dimensionality Reduction: Despite reducing from 64 dimensions to only three, PCA retains essential variance, allowing for distinction between different digits. This demonstrates PCA’s utility in data compression, providing a simplified representation without losing significant information.\nExploring Further Dimensions: Additional components could capture more variance, if required. However, the first three components often capture most of the meaningful variance, balancing dimensionality reduction with information retention.\n\nThis PCA projection shows that the digit data has underlying patterns well-represented by the first few components. These findings highlight PCA’s usefulness in compressing high-dimensional data while preserving its structure, making it a valuable tool for visualization, noise reduction, and as a pre-processing step in machine learning tasks.\n\n\n11.1.4.4 PCA in Noise Filtering\nPCA can also be applied for denoising data by reconstructing it from a subset of principal components. Components associated with small variance often correspond to noise, so omitting them can yield a cleaner version of the data. We demonstrate this effect using the digit dataset through the following steps:\n\nAdd Random Noise: Add random noise to the original digit images.\nFit PCA to Noisy Data: Apply PCA to the noisy data, selecting enough components to retain 50% of the variance.\nReconstruct the Digits: Use PCA’s inverse transform to reconstruct the digits from the reduced components, effectively filtering out the noise.\nDisplay the Results: Show a side-by-side comparison of the original, perturbed, and reconstructed images for visual assessment.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_digits\n\ndef plot_digits(datasets, titles):\n    \"\"\"\n    Display 2×5 grids of digit images for each dataset in `datasets`.\n\n    Parameters\n    ----------\n    datasets : list of np.ndarray\n        Each array has shape (n_samples, 64), representing different \n        versions of the digit data (e.g., original, noisy, reconstructed).\n    titles : list of str\n        Titles corresponding to each dataset (e.g., [\"Original\", \"Noisy\", \"Reconstructed\"]).\n    \"\"\"\n    fig, axes = plt.subplots(len(datasets), 10, figsize=(8, 5),\n                             subplot_kw={\"xticks\": [], \"yticks\": []},\n                             gridspec_kw=dict(hspace=0.2, wspace=0.1))\n    for row, (data, title) in enumerate(zip(datasets, titles)):\n        for i, ax in enumerate(axes[row]):\n            ax.imshow(data[i].reshape(8, 8), cmap=\"binary\", interpolation=\"nearest\", clim=(0, 16))\n        axes[row, 0].set_ylabel(title, rotation=0, labelpad=25,\n                                fontsize=11, ha=\"right\")\n\n    plt.suptitle(\"PCA Noise Filtering: Original, Noisy, and Reconstructed Digits\",\n                 fontsize=14)\n    plt.show()\n\n# Load the digit dataset\ndigits = load_digits()\nX = digits.data\n\n# Add Gaussian noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 4, X.shape)\nX_noisy = X + noise\n\n# Fit PCA to retain 50% of total variance\npca_50 = PCA(0.50)\nX_pca_50 = pca_50.fit_transform(X_noisy)\nX_reconstructed_50 = pca_50.inverse_transform(X_pca_50)\n\n# Display results\nplot_digits([X, X_noisy, X_reconstructed_50],\n            [\"Original\", \"Noisy\", \"Reconstructed\"])\n\n\n\n\n\n\n\nFigure 11.4: Noise filtering using PCA: original, noisy, and reconstructed digit images.\n\n\n\n\n\nThe visualization in Figure 11.4 highlights PCA’s ability to filter out random noise:\n\nOriginal vs. Noisy Images: The second row shows the effect of added random noise, making the digits less recognizable.\nReconstructed Images: In the third row, PCA has filtered out much of the random noise, reconstructing cleaner versions of the digits while preserving important structural features. This illustrates PCA’s effectiveness in noise reduction by retaining only the principal components that capture meaningful variance.\n\nThis example illustrates PCA’s denoising mechanism: by keeping only the components with the largest variance, it suppresses random noise and retains the dominant patterns in the data. This property makes PCA useful for preprocessing, image restoration, and general noise reduction tasks.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "11-unsupervised.html#stochastic-neighbor-embedding",
    "href": "11-unsupervised.html#stochastic-neighbor-embedding",
    "title": "11  Unsupervised Learning",
    "section": "11.2 Stochastic Neighbor Embedding",
    "text": "11.2 Stochastic Neighbor Embedding\nStochastic Neighbor Embedding (SNE) is a dimensionality reduction technique used to project high-dimensional data into a lower-dimensional space (often 2D or 3D) while preserving local neighborhoods of points. It is particularly popular for visualization tasks, helping to reveal clusters or groupings among similar points. Key characteristics include:\n\nUnsupervised: It does not require labels, relying on similarity or distance metrics among data points.\nProbabilistic framework: Pairwise distances in the original space are interpreted as conditional probabilities, which SNE attempts to replicate in the lower-dimensional space.\nCommon for exploratory data analysis: Especially useful for high-dimensional datasets such as images, text embeddings, or genetic data.\n\n\n11.2.1 Statistical Rationale\nSNE aims to construct a low-dimensional representation that preserves the local neighborhood relationships observed in the original high-dimensional data. The method achieves this by matching probabilistic similarities between points across the two spaces.\n\nFor each point \\(x_i\\) in the high-dimensional space, SNE defines a conditional probability \\(p_{j|i}\\) that measures how likely \\(x_j\\) would be chosen as a neighbor of \\(x_i\\). The probability is modeled as\n\\[\np_{j|i} =\n\\frac{\\exp\\!\\left(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2\\right)}\n     {\\sum_{k \\neq i}\\exp\\!\\left(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2\\right)},\n\\]\nwhere the bandwidth \\(\\sigma_i\\) controls the neighborhood size and is typically determined to match a specified perplexity.\nEach high-dimensional point \\(x_i\\) is mapped to a low-dimensional coordinate \\(y_i\\), from which a corresponding similarity distribution is defined:\n\\[\nq_{j|i} =\n\\frac{\\exp\\!\\left(-\\|y_i - y_j\\|^2\\right)}\n     {\\sum_{k \\neq i}\\exp\\!\\left(-\\|y_i - y_k\\|^2\\right)}.\n\\]\nDenote by \\(P_i = \\{p_{j|i}\\}_{j \\neq i}\\) and \\(Q_i = \\{q_{j|i}\\}_{j \\neq i}\\) the conditional probability distributions for point \\(i\\) in the high- and low-dimensional spaces, respectively. The optimal embedding \\(\\{y_i\\}\\) minimizes the Kullback–Leibler (KL) divergence between the two conditional distributions:\n\\[\nC = \\sum_i \\text{KL}(P_i \\| Q_i)\n  = \\sum_i \\sum_j p_{j|i} \\log\\!\\frac{p_{j|i}}{q_{j|i}}.\n\\]\nThis optimization encourages points that are close in the high-dimensional space to remain close in the lower-dimensional map, thus preserving local structure.\n\nThe asymmetry of the KL divergence in SNE plays a central role in how local structure is preserved. Because the divergence \\(\\text{KL}(P_i \\| Q_i)\\) penalizes situations where a true neighbor \\(x_j\\) (with high \\(p_{j|i}\\)) is assigned a low similarity \\(q_{j|i}\\) in the low-dimensional map, the optimization strongly discourages breaking apart neighborhoods that exist in the original space. In contrast, pairs of distant points with small \\(p_{j|i}\\) values contribute little to the objective even if they are mapped close together. This asymmetry biases the optimization toward accurately reproducing local relationships rather than global geometry, emphasizing cluster fidelity over large-scale distances. While this focus on local preservation is desirable for visualizing complex data manifolds, it can also cause points from different neighborhoods to become compressed together, a phenomenon known as the crowding problem.\nThe later t-SNE algorithm addresses this issue by symmetrizing the joint probabilities and replacing the Gaussian kernel in the low-dimensional space with a Student-\\(t\\) distribution to allow for heavier tails and better separation of clusters.\n\n\n11.2.2 t-SNE Variation\nThe t-distributed Stochastic Neighbor Embedding (t-SNE) modifies SNE to overcome two major limitations: the crowding problem and the asymmetry of conditional probabilities. These refinements improve both optimization stability and visual interpretability of the resulting low-dimensional map.\nThe crowding problem arises because, in high-dimensional spaces, pairwise distances are more evenly distributed than can be represented in two or three dimensions. As a result, points that are moderately distant in the original space may become crowded together in the low-dimensional map. To mitigate this issue, t-SNE replaces the Gaussian kernel in the low-dimensional space with a Student-\\(t\\) distribution with one degree of freedom, which has heavier tails and allows distant points to remain farther apart.\nTo simplify computation and ensure that similarity is mutual, t-SNE also defines symmetric joint probabilities as \\(p_{ij} = (p_{j|i} + p_{i|j}) / (2N)\\), producing a symmetric similarity matrix.\nThe corresponding similarity in the low-dimensional space is defined as\n\\[\nq_{ij} =\n\\frac{\\bigl(1 + \\|y_i - y_j\\|^2\\bigr)^{-1}}\n     {\\sum_{k \\neq l}\\bigl(1 + \\|y_k - y_l\\|^2\\bigr)^{-1}}.\n\\]\nt-SNE minimizes the Kullback–Leibler divergence between the joint probabilities \\(\\{p_{ij}\\}\\) and \\(\\{q_{ij}\\}\\) with respect to the embedding coordinates \\(\\{y_i\\}\\):\n\\[\nC = \\sum_{i}\\sum_{j} p_{ij}\\log\\!\\frac{p_{ij}}{q_{ij}}.\n\\]\nBy combining symmetric similarities and heavy-tailed distributions, t-SNE preserves local structure while providing enough flexibility to separate distant clusters in the low-dimensional representation.\n\n\n11.2.3 Supervised Variation\nWhile SNE and t-SNE are fundamentally unsupervised methods, they can be extended to incorporate available label information. In supervised or semi-supervised variants, the similarity structure is adjusted so that points sharing the same label are drawn closer together, whereas points from different classes are pushed apart through modified distance weights or additional penalty terms in the loss function. These extensions preserve the local manifold structure of the data while simultaneously promoting class separation in the low-dimensional embedding. Such hybrid formulations are particularly useful when partial label information is available and the goal is to combine supervised guidance with unsupervised neighborhood discovery.\n\n\n11.2.4 An Example with the NIST Digits Data\nBelow is a brief example using t-SNE on a small subset of the MNIST digits, which is itself a curated subset of the original NIST handwritten digits data. This example illustrates how t-SNE projects high-dimensional image data—each digit represented by a 784-dimensional vector—onto a two-dimensional space. The resulting visualization reveals clusters corresponding to different digits, providing an intuitive view of how similar handwriting patterns group together in the embedding space.\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX = mnist.data[:2000]\ny = mnist.target[:2000].astype(int)\n\ntsne = TSNE(\n    n_components=2,\n    perplexity=30,\n    learning_rate='auto',\n    init='random',\n    random_state=42\n)\nX_embedded = tsne.fit_transform(X)\n\nplt.figure()\ndigits = np.unique(y)\nfor digit in digits:\n    idx = (y == digit)\n    plt.scatter(\n        X_embedded[idx, 0],\n        X_embedded[idx, 1],\n        label=f\"Digit {digit}\",\n        alpha=0.5\n    )\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\nFigure 11.5: t-SNE visualization of a subset of MNIST digits. Points belonging to the same digit cluster together, while visually similar digits such as 3 and 5 show partial overlap in the 2D embedding.\n\n\n\n\n\nAs shown in Figure (ref?)(fig:tsne-mnist), digits with similar handwriting styles tend to cluster together, forming visually distinct regions in the embedding space. Overlaps occur where digits share structural features, such as 3 and 5 or 4 and 9, reflecting the inherent ambiguity in handwritten data. A few scattered points often correspond to atypically written digits that t-SNE places between clusters. Overall, the visualization provides an interpretable two-dimensional summary of the high-dimensional digit images, illustrating how t-SNE preserves local similarity while maintaining global diversity in the data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "90-advanced.html",
    "href": "90-advanced.html",
    "title": "12  Advanced Topics",
    "section": "",
    "text": "12.1 Filling Missing Data With Multiple Imputation\nThis section was prepared by Jacob Schlessel, a junior double majoring in Statistical Data Science and Economics.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "90-advanced.html#filling-missing-data-with-multiple-imputation",
    "href": "90-advanced.html#filling-missing-data-with-multiple-imputation",
    "title": "12  Advanced Topics",
    "section": "",
    "text": "12.1.1 Introduction\nIt is rare to work with a dataset that does not have missing values. This section will introduce multiple imputation as a method of filling these values and justify its use over other methods of imputation. The following topics will be covered:\n\nAssessing Missingness\nBenefits of Imputation\nLimitations of Imputation\nMethods of Single Imputation\nMultiple Imputation\n\n\n\n12.1.2 Assessing Missingness\nBefore filling in missing values, we must determine whether imputation makes sense given the context of the data. As a starting point, we can classify the missingness of the data into one of three different types. To illustrate the differences between these cases, we will use the following example:\nA lab technician conducts a study on the relationship between age and blood pressure. Measurements are taken from a randomly selected group of 1000 volunteers, with all age groups being included in the study.\n\nMissing completely at random (MCAR): the ideal case - missingness is independent of both observed and unobserved data\n\nA lab technician accidentally forgets to record 10 patients’ blood pressure values because of a computer glitch\nMissingness is random noise - estimates are not biased\n\nMissing at random (MAR): the second best case - missingness depends only observed variables\n\nOlder patients are more likely to skip the blood pressure test because the cuff feels uncomfortable\nResults are biased if you ignore the missing values\n\nMissing not at random (MNAR): the missing values depend on the unobserved(missing) value itself\n\nPatients with very high blood pressure intentionally refuse the test for fear of bad news\nImputation is not possible - requires advanced techniques like sensitivity analyses or pattern-mixture modes\n\n\nOnce we understand what type of missingness exists in the data, we have two options:\n\nComplete Case Analysis: Drop all rows with missing data and only include complete rows in analysis\nImputation: Replace missing values with specific values, include filled values in analysis\n\n\n12.1.2.1 Application in Python:\nWe wll use the airquality dataset from statsmodels as an example of working with missing data.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Load the dataset\ndf = sm.datasets.get_rdataset(\"airquality\").data\n\n# Summarize missing data\nmissing_count = df.isna().sum()\n\nmissing_percent = (df.isna().mean() * 100).round(2)\n\nmissing_summary = pd.DataFrame({\n    'Missing Count': missing_count,\n    'Missing %': missing_percent\n}).sort_values('Missing %', ascending=False)\n\nprint(missing_summary)\n\n         Missing Count  Missing %\nOzone               37      24.18\nSolar.R              7       4.58\nWind                 0       0.00\nTemp                 0       0.00\nMonth                0       0.00\nDay                  0       0.00\n\n\nWe cannot perform imputation if the data is MNAR. Therefore, to disprove that a dataset is MNAR, a good starting point is to look for evidence for MCAR or MAR, as a dataset cannot exhibit two different types of missingness. We do not actually need to know which of these two cases apply since imputation works with both, we just need to be able to confidently claim that the missing values are not associated with unobserved values and thus will not affect inference.\nTo find evidence of MCAR or MAR, we can use the following framework to analyze patterns of missingness:\n\nDefine the missingness target (the variable with missing values)\nPerform statistical tests (t-tests for numeric, chi-squared tests for categorical) - note: this is not always conclusive\nFind correlations between the target and other variables. If the correlation is above 0.2, this could be evidence of MAR and warrants further investigation\nUse domain knowledge to perform in-depth analysis to rule out the possibility of MNAR missingness. Note: it is impossible to definitively prove or disprove MNAR based on the data because by definition, we do not have data to analyze.\n\nNull hypothesis: The means for each variable are the same for the in the missing/nonmissing Ozone groups\nAlternative hypothesis: The means for each variable are different for the in the missing/nonmissing Ozone groups\n\nfrom scipy.stats import ttest_ind\n\n# Create indicator: 1 if Ozone is missing, else 0\ndf['Ozone_missing'] = df['Ozone'].isna().astype(int)\n\nfor col in ['Solar.R', 'Wind', 'Temp']:\n    group1 = df.loc[df['Ozone_missing'] == 0, col].dropna() \n    group2 = df.loc[df['Ozone_missing'] == 1, col].dropna()  \n    stat, p = ttest_ind(group1, group2, equal_var=False)\n    \n    mean1 = group1.mean()\n    mean2 = group2.mean()\n    \n    print(f\"{col}:\")\n    print(f\"  Mean (Ozone observed) = {mean1:.2f}\")\n    print(f\"  Mean (Ozone missing)  = {mean2:.2f}\")\n    print(f\"  p-value = {p:.4f}\\n\")\n\nSolar.R:\n  Mean (Ozone observed) = 184.80\n  Mean (Ozone missing)  = 189.51\n  p-value = 0.7846\n\nWind:\n  Mean (Ozone observed) = 9.86\n  Mean (Ozone missing)  = 10.26\n  p-value = 0.5446\n\nTemp:\n  Mean (Ozone observed) = 77.87\n  Mean (Ozone missing)  = 77.92\n  p-value = 0.9787\n\n\n\nThe pvalues are all large, meaning the means are the same for each variable regardless of whether ozone is missing.\nSince Month is a categorical variable, we can perform a chi-squared test to check if the number of missing values depend on the month:\nNull hypothesis: The number of missing Ozone values does not depend on the month Alternative hypothesis: The number of missing Ozone values does depend on the month in some way\n\nfrom scipy.stats import chi2_contingency\n\n# Create contingency table: Month × Missingness\ncontingency = pd.crosstab(df[\"Month\"], df[\"Ozone_missing\"])\nprint(\"Contingency Table (Month vs. Ozone Missingness):\\n\")\nprint(contingency)\n\n# Perform Chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(contingency)\n\nprint(\"\\nChi-square test results:\")\nprint(f\"Chi-square statistic = {chi2:.3f}\")\nprint(f\"Degrees of freedom    = {dof}\")\nprint(f\"p-value               = {p:.4f}\")\n\nContingency Table (Month vs. Ozone Missingness):\n\nOzone_missing   0   1\nMonth                \n5              26   5\n6               9  21\n7              26   5\n8              26   5\n9              29   1\n\nChi-square test results:\nChi-square statistic = 44.751\nDegrees of freedom    = 4\np-value               = 0.0000\n\n\nJune has noticeably more missing Ozone values than the other months, so we confirm that this difference is statistically significant by observing a pvalue of 0. We can confirm this visually as well:\n\nimport matplotlib.pyplot as plt\n\n# Plot number of missing ozone observations by month\nmissing_counts = (\n    df[df[\"Ozone_missing\"] == 1][\"Month\"]\n    .value_counts()\n    .sort_index()\n)\nmissing_counts = (\n    missing_counts.reindex([5, 6, 7, 8, 9], fill_value=0)\n)\nplt.figure(figsize=(6, 4))\nplt.bar(\n    missing_counts.index.astype(str),\n    missing_counts.values,\n    color=\"lightsteelblue\",\n    edgecolor=\"black\"\n)\nplt.xlabel(\"Month (May = 5, ..., September = 9)\")\nplt.ylabel(\"Number of Missing Ozone Values\")\nplt.title(\"Missing Ozone Values by Month\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSince the number of missing values of Ozone depends on observed data, we have determined that the missingness is MAR and we can continue with imputation.\nWe can follow the same process for analyzing the missingness of Solar.R.\n\ndf['Solar_missing'] = df['Solar.R'].isna().astype(int) #1 = Missing\n\n#t test for numeric variables\nfor col in ['Ozone', 'Wind', 'Temp']:\n    g1 = df.loc[df['Solar_missing']==0, col].dropna()\n    g2 = df.loc[df['Solar_missing']==1, col].dropna()\n    stat, p = ttest_ind(g1, g2, equal_var=False)\n    print(col)\n    print(f\"Mean (Solar.R observed) = {g1.mean():.2f}\")\n    print(f\"Mean: (Solar.R missing) = {g2.mean():.2f}\")\n    print(f\"p-value = {p:.4f}\\n\")\n\n#chi squared test for categorical variable (month)\ncont = pd.crosstab(df['Month'], df['Solar_missing'])\nchi2, p, dof, exp = chi2_contingency(cont)\nprint(f\"Month vs Solar.R missingness: chi2={chi2:.2f}, p={p:.4f}\")\n\n\n# Bar chart of counts by month\nmissing_counts = (\n    df[df[\"Solar_missing\"] == 1][\"Month\"]\n    .value_counts()\n    .sort_index()\n)\nmissing_counts = (\n    missing_counts.reindex([5, 6, 7, 8, 9], fill_value=0)\n)\nplt.figure(figsize=(6, 4))\nplt.bar(\n    missing_counts.index.astype(str),\n    missing_counts.values,\n    color=\"lightsteelblue\",\n    edgecolor=\"black\"\n)\nplt.xlabel(\"Month (May = 5, ..., September = 9)\")\nplt.ylabel(\"Number of Missing Solar.R Values\")\nplt.title(\"Missing Solar.R Values by Month\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()\n\nOzone\nMean (Solar.R observed) = 42.10\nMean: (Solar.R missing) = 42.80\np-value = 0.9602\n\nWind\nMean (Solar.R observed) = 10.00\nMean: (Solar.R missing) = 9.00\np-value = 0.5343\n\nTemp\nMean (Solar.R observed) = 78.12\nMean: (Solar.R missing) = 73.00\np-value = 0.3602\n\nMonth vs Solar.R missingness: chi2=11.14, p=0.0251\n\n\n\n\n\n\n\n\n\nSimilarly to Ozone, it appears that Solar.R is MAR since the missing values are related to the month during which the data was measured.\n\n\n12.1.2.2 Important Notes\n\nConsider MNAR if domain knowledge suggests it\nMake sure to evaluate case-specific context to ensure imputation is appropriate. For example, using statistical imputation methods for the missing zip codes in the NYC Crash Data would not have made sense\nProportion of missing data alone should not dictate whether imputation is used or not, need to use best judgement\n\n\n\n\n12.1.3 Benefits of Imputation\nImpututation replaces missing values with estimated ones to create a complete dataset. This process offers a number of benefits for modeling and analysis.\n\n12.1.3.1 Preserving information\nIf we were to perform complete case analysis, we would lose the valuable information provided by the rows that may be missing values for one or two variables. By making an educated guess as to what the value would have been, we can\n\nMaintain real-world relationships observed among variables\nPreserve sample size to build models with higher precision\nConduct robust analysis with messy data\nEnables use of statistical methods and machine learning models\n\n\n\n12.1.3.2 Reducing bias\nIf the data are MAR, deletion will bias the results of any statistical modeling because the missing cases are systematically different from observed cases. To ensure that our data is representative of what we are studying, we can use fill missing values using estimates based on relationships that exist in our observed data. Imputation preserves covariances and joint distributions, so the result is lower bias in model coefficients.\nNote: not all imputation methods have the same effect on reducing bias. Multiple imputation tends to minimize bias more than single imputation methods when data is MAR.\n\n\n\n12.1.4 Limitations of Imputation\nImputation does not completely solve issues caused by missing data, it only alleviates them. It is important to note the following limitations of imputation to better understand when it is appropriate to use:\n\nSimple imputation methods distort distributions and can impact visualizations\nNeed to acknowledge that imputing missing data adds some uncertainty\nCannot correct cases where data are MNAR\nCan be very computationally expensive depending on method of imputation and complexity of data\n\n\n\n12.1.5 Methods of Single Imputation\nSingle Imputation: Replacing each missing value with a single, fixed estimate\nTypes of single imputation:\n\nMean Imputation: Replacing missing values with the mean of the observed data\nMedian Imputation: Replacing missing values with the median of observed data\nMode Imputation: Replace missing values with the mode (for filling categorical data)\nHot-Deck Imputation: Replace a missing value with an observed value (a “donor” from a similar case)\n\nFor each method, we will fit a regression model with Temp as the response variable and Ozone, Solar.R, Wind, and Month as predictors. We will find the value of the coefficients and their standard errors after applying each method of imputation to compare the results.\n\n12.1.5.1 Mean Imputation\nWe will replace all missing Ozone and Solar.R values with the respective means from the observed data:\n\n# Encode Month as categorical and create dummy variables\ndf[\"Month\"] = df[\"Month\"].astype(\"category\")\ndf= pd.get_dummies(df, columns=[\"Month\"], drop_first=True)\ndf = df.astype(float)\n\n# Assign predictor and response variables\nx_var = [\n    \"Ozone\", \"Solar.R\", \"Wind\", \"Month_6\", \"Month_7\", \"Month_8\", \"Month_9\"]\n\n# Apply mean imputation and fit the regression model\ndf_mean = df.copy()\n\nfor col in [\"Ozone\", \"Solar.R\"]:\n    df_mean[col] = df_mean[col].fillna(df_mean[col].mean())\n\ny = df_mean[\"Temp\"]\nX = sm.add_constant(df_mean[x_var])\nmodel = sm.OLS(y, X).fit()\nresults = pd.DataFrame({\n    \"Coefficient\": model.params.round(4),\n    \"Std_Error\": model.bse.round(4)\n})\n\nprint(results)\n\n         Coefficient  Std_Error\nconst        64.6504     2.3474\nOzone         0.0953     0.0207\nSolar.R       0.0153     0.0054\nWind         -0.3807     0.1512\nMonth_6      11.7914     1.4211\nMonth_7      13.9666     1.4895\nMonth_8      14.5699     1.5075\nMonth_9      10.5281     1.4189\n\n\nBenefits:\n\nIs sufficient if less than 5% of a particular variable is missing\n\nLimitations:\n\nUnderestimates variances -&gt; does not add any spread to the data, standard errors and confidence intervals are too small\nReduces natural variability\nPerforms poorly if data is not MCAR\nShrinks covariance -&gt; coefficients are biased towards zero\n\n\n\n12.1.5.2 Median Imputation\nWe can fit the same model, this time using median imputation:\n\ndf_median = df.copy()\n\n# Apply median imputation and fit the model\nfor col in [\"Ozone\", \"Solar.R\"]:\n    df_median[col] = df_median[col].fillna(df_median[col].median())\n\n# 4) Define predictors and response\ny = df_median[\"Temp\"]\nX = sm.add_constant(df_median[x_var])\n\n# 5) Fit OLS regression\nmodel = sm.OLS(y, X).fit()\n\n# 6) Display coefficients and standard errors only\nresults = pd.DataFrame({\n    \"Coefficient\": model.params.round(4),\n    \"Std_Error\": model.bse.round(4)\n})\n\nprint(results)\n\n         Coefficient  Std_Error\nconst        64.6857     2.3324\nOzone         0.0965     0.0205\nSolar.R       0.0150     0.0054\nWind         -0.3708     0.1509\nMonth_6      12.3827     1.4122\nMonth_7      14.0042     1.4848\nMonth_8      14.5675     1.5032\nMonth_9      10.4375     1.4178\n\n\nBenefits:\n\nMore robust to outliers than mean imputation\n\nLimitations:\n\nUnderestimates uncertainty\nFlattens the distribution by clustering values at the center\n\n\n\n12.1.5.3 Hot-Deck Imputation\nHot-Deck imputation chooses the value from a similar observation in the current dataset to fill the missing values with. The “donor” can be chosen randomly or conditionally. Here. we will fill missing values with similar observations from the same month, since earlier we found that Month was related to the number of missing values of both Ozone and Solar.R.\n\nimport numpy as np\n\ndf_hot = df.copy()\nrng = np.random.default_rng(42)\n\ndef hot_deck_impute(df, target_col, month_dummies):\n    # Reconstruct Month number from dummies (5–9)\n    df = df.copy()\n    df[\"Month_num\"] = 5\n    for i, col in enumerate(month_dummies, start=6):\n        df.loc[df[col] == 1, \"Month_num\"] = i\n\n    # Impute missing values within same month\n    for i in df[df[target_col].isna()].index:\n        month = df.at[i, \"Month_num\"]\n        donor_pool = df.loc[\n            (df[\"Month_num\"] == month) & (~df[target_col].isna()), target_col\n        ]\n        if donor_pool.empty:\n            donor_pool = df.loc[~df[target_col].isna(), target_col]\n        df.at[i, target_col] = rng.choice(donor_pool.values)\n    return df.drop(columns=\"Month_num\")\n\n# Apply to missing targets and fit model\nmonth_dummies = [\"Month_6\", \"Month_7\", \"Month_8\", \"Month_9\"]\nfor col in [\"Ozone\", \"Solar.R\"]:\n    df_hot = hot_deck_impute(df_hot, col, month_dummies)\n\ny = df_hot[\"Temp\"]\nX = sm.add_constant(df_hot[x_var])\nmodel = sm.OLS(y, X).fit()\nresults = pd.DataFrame({\n    \"Coefficient\": model.params.round(4),\n    \"Std_Error\": model.bse.round(4)\n})\nprint(results)\n\n         Coefficient  Std_Error\nconst        66.0892     2.2664\nOzone         0.0929     0.0194\nSolar.R       0.0128     0.0053\nWind         -0.4199     0.1475\nMonth_6      12.5071     1.4301\nMonth_7      13.6540     1.5196\nMonth_8      13.9432     1.5414\nMonth_9      10.0670     1.4339\n\n\nBenefits:\n\nPreserves real data values -&gt; borrowed from observed “donors”\nRetains covariance structure, keeping relationships realistic\nWorks well in MAR cases\n\nLimitations:\n\nRequires enough donor candidates to avoid highly biased results\nCan propagate outliers\n\n\n\n\n12.1.6 Multiple Imputation\nInstead of replacing the missing cases with a single guess, multiple imputation creates multiple guesses to incorporate some randomness that mimics real world uncertainty.\nMultiple Imputation by Chained Equations (MICE): the most widely used implementation of multiple imputation\n\nDefine one target variable and start with simple guesses (mean/median imputation) for missing cases of the non-target variable\nFit a regression model for the target using all other variables as predictors\nImpute the missing values of the target variable with the estimate generated by the regression\nRepeat for all variables that have missing data, using your newly found imputations as training data\nRepeat the process m times using different random draws to create m complete datasets. For each cycle, the coefficients for the regression models are sampled from the posterior distribution - essentially, the coefficients are randomly selected from their respective distributions of their plausible values\nFit the final model m times using the complete datasets and pool results using Rubin’s Rules to combine estimates and obtain valid inference\n\nRubin’s Rules: formulas for combining m regression results into one final set of estimates, correctly accounts for within-imputation variance and between-imputation variance.\nPooled Estimate (Mean of Coefficients)\n\\[\n\\bar{Q} = \\frac{1}{m} \\sum_{i=1}^{m} Q_i\n\\]\n\n\\(Q_i\\): estimate (regression coefficient) from imputed dataset i\n\n\\(\\bar{Q}\\): overall pooled estimate (average of all estimates)\n\n\nWithin-Imputation Variance: captures the uncertainty within each model\n\\[\n\\bar{U} = \\frac{1}{m} \\sum_{i=1}^{m} U_i\n\\]\n\n\\(U_i\\): estimated variance from dataset i\n\n\nBetween-Imputation Variance: captures uncertainty across imputations\n\\[\nB = \\frac{1}{m - 1} \\sum_{i=1}^{m} (Q_i - \\bar{Q})^2\n\\]\n\nTotal Variance (Rubin’s Combined Uncertainty): combines both within- and\nbetween-imputation uncertainty\n\\[\nT = \\bar{U} + \\left(1 + \\frac{1}{m}\\right) B\n\\]\n\nPooled Standard Error: the final standard error that accounts for both\nmodel uncertainty and missing-data uncertainty, useful for inference\n\\[\nSE(\\bar{Q}) = \\sqrt{T}\n\\]\n\n12.1.6.1 Summary Table\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nInterpretation\n\n\n\n\n\\(Q_i\\)\nEstimate from imputation i\ne.g., regression coefficient\n\n\n\\(U_i\\)\nVariance from imputation i\n\\(SE^2\\) from that model\n\n\n\\(\\bar{Q}\\)\nMean of all \\(Q_i\\)\nFinal pooled estimate\n\n\n\\(\\bar{U}\\)\nMean of all \\(U_i\\)\nWithin-imputation variance\n\n\n\\(B\\)\nBetween-imputation variance\nVariability across datasets\n\n\n\\(T\\)\nTotal variance\nCombined uncertainty\n\n\n\\(SE(\\bar{Q})\\)\nPooled standard error\n\\(\\sqrt{T}\\), used in inference\n\n\n\nScikit-learn’s IterativeImputer class simplifies this process.\nExample using the airquality data:\n\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\n\n# Variables for imputation regression models\nmonth_dummies = [c for c in df.columns if c.startswith(\"Month_\")]\nnum_cols = [\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\"] + month_dummies\n\n# Variables for final regression omdel\nx_vars = [\"Ozone\", \"Solar.R\", \"Wind\"] + month_dummies\ny_var = \"Temp\"\n\n\n# Number of imputations\nM = 10\n\n# To store results\ncoef_list, var_list = [], []\n\n# Perform MICE 10 times\nfor m in range(M):\n    imp = IterativeImputer(  # Performs chained equations \n        random_state=42 + m, # Different random seed every time\n        sample_posterior=True, # Adds random noise\n        max_iter=20 # Iterates thru models 20 times or until convergence\n    )\n    imputed = imp.fit_transform(df[num_cols])\n\n    df_imp = df.copy()\n    df_imp[num_cols] = imputed\n\n    X = sm.add_constant(df_imp[x_vars])\n    y = df_imp[y_var]\n    model = sm.OLS(y, X).fit()\n\n    coef_list.append(model.params)\n    var_list.append(model.bse ** 2)\n\n# Rubin’s Rules\ncoefs_df = pd.DataFrame(coef_list)\nvars_df  = pd.DataFrame(var_list)\n\nQ_bar = coefs_df.mean()\nU_bar = vars_df.mean()\nB     = coefs_df.var(ddof=1)\nT_var = U_bar + (1 + 1/len(coef_list)) * B\nSE    = np.sqrt(T_var)\n\npooled = pd.DataFrame({\"MI Coefficient\": Q_bar.round(4),\n                       \"MI Std_Error\": SE.round(4)})\nprint(pooled)\n\n         MI Coefficient  MI Std_Error\nconst           64.9348        2.2675\nOzone            0.0946        0.0211\nSolar.R          0.0138        0.0054\nWind            -0.3213        0.1545\nMonth_6         11.2000        1.5068\nMonth_7         13.2032        1.5408\nMonth_8         13.8689        1.5867\nMonth_9          9.9381        1.4006\n\n\nNotice that the standard errors for this method are larger than those from the single imputation methods. This is good -&gt; we are correctly accounting for the increased uncertainty caused by missing data\n\n\n\n12.1.7 Conclusion\n\nImputation is an excellent tool for producing valid inference from datasets with missing data\nBefore performing any type of imputation, you should get very familiar with your data and assess what type of missingness it exhibits\nThe model results generated from using MICE have less bias but increased variance\n\n\n\n12.1.8 Further Readings\n\nOverview of Multiple Imputation\nMultiple Imputation With Varying Proportions of Missing Data\nWorkflow of Performing Imputation",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "90-advanced.html#web-scraping",
    "href": "90-advanced.html#web-scraping",
    "title": "12  Advanced Topics",
    "section": "12.2 Web Scraping",
    "text": "12.2 Web Scraping\nThis section was prepared by Sahil Patel, an undergraduate junior majoring in Computer Science, Statistical Data Science, and Economics.\n\n12.2.1 An Introduction to Web Scraping\n\n12.2.1.1 What is Web Scraping\nWeb scraping is a way to automatically gather information from websites. Instead of spending hours copying and pasting data, you can use scripts to collect large amounts of structured information quickly and consistently. Once you have the data, it can be analyzed, stored, or used for research and decision making.\nWeb scraping is particularly useful because most web data is created for humans to read, not for machines to process. By scraping, we can turn that information into a format that computers can work with, allowing us to explore trends, track changes, or gain insights that would be really hard to collect manually.\n\n\n12.2.1.2 Use Cases of Web Scraping\nHere are some of the use cases of web scarping:\n\nMarket Analysis\n\nCompanies use web scraping to track products, prices, and customer reviews, helping them understand market trends and stay competitive.\n\nFinance and Investment\n\nAnalysts gather stock prices, financial news, and reports to make informed investment decisions or to analyze market sentiment.\n\nAcademic Research\n\nResearchers collect articles, datasets, and public records to support studies and uncover insights across a variety of fields.\n\nSocial Media and Marketing\n\nMarketers and analysts monitor trends, hashtags, and audience engagement to understand consumer behavior and improve campaigns.\n\n\n\n\n12.2.1.3 Static versus Dynamic Web Pages\nBefore scraping, it is important to understand the kind of web pages you are working with:\n\nStatic Web Pages\n\nThe content is already in the HTML when the page loads. Everything you need is there, so extracting it is relatively straightforward.\n\nDynamic Web Pages\n\nThe content is generated by JavaScript or updated after the page loads. Fetching the HTML alone may not give you what you want. You may need to interact with the page to see the full content.\n\n\n\n\n12.2.1.4 Legal and Ethical Considerations\nEven though web scraping can be extremely powerful, it comes with important responsibilities and best practices that should not be ignored.\n\nRespect robots.txt: Before scraping a site, check its robots.txt file to understand which pages or sections the website owner allows you to access, and make sure to follow these guidelines.\nAvoid overloading servers: Sending too many requests too quickly can strain a website’s server. Introduce pauses between requests, use random delays, and keep your scraping activity at a reasonable pace to avoid causing problems for the website.\nDo not collect sensitive information: Avoid scraping personal, confidential, or protected information unless you have explicit permission to do so, as this could violate privacy laws or ethical standards.\nCheck copyright and licensing: Just because you can access the data does not mean you have the right to use or redistribute it freely. Always review the site’s terms of use and any applicable copyright rules.\nBe transparent: When using scraped data in reports, analyses, or projects, clearly cite your sources and acknowledge where the information came from. Transparency helps maintain credibility and respect for the original content creators.\n\n\n\n\n12.2.2 Basics of Web Requests\nBefore you can extract any data from a website, you need to first fetch the content of the page. In Python, this is typically done using the requests library, which allows you to send HTTP requests and receive responses. Understanding how these requests work is key to effective web scraping.\nIf you haven’t done so already, you will need install requests to your terminal.\npip install requests\n\n12.2.2.1 HTML Basics\nTo scrape web pages effectively, it is important to understand the basic structure of HTML, which is the language used to create web pages. Web pages are made up of elements enclosed in tags. Some common tags include:\n\n&lt;body&gt;: this tag identifies the main body of the website, which contains the content that is visible to users.\n&lt;table&gt;: This tag identifies a table element, which is used to organize data in rows and columns.\n&lt;tbody&gt;: This tag identifies the body of the table, which contains all the rows of the table.\n&lt;tr&gt;: This tag identifies a table row, which contains individual cells (&lt;td&gt;) with data.\n\n\n\n12.2.2.2 Response Status Codes\nEvery HTTP request returns a status code that indicates whether it was successful.\n\n200 means the request was successful, and the server returned the requested content.\n404 means the page was not found, which usually indicates the URL is incorrect.\n403 means access is forbidden, which often happens when a server blocks automated requests.\n500 indicates a server error on the website.\n\nChecking these codes allows you to handle errors gracefully and avoid scraping pages that are unavailable or blocked.\n\n\n12.2.2.3 HTTP Methods\nThe two most common HTTP methods used in web scraping are GET and POST.\n\nGET requests are used to retrieve data from a web server. This is the method you will use most often because you are usually trying to download the content of a page.\nPOST requests are used to send data to a server, often when submitting forms or interacting with a website. Some websites require POST requests to access certain content or search results.\n\n\n\n12.2.2.4 Using Headers and User-Agent Strings\nWeb servers sometimes treat automated requests differently from requests made by real browsers. By adding headers, you can make your requests look more like they are coming from a normal user.\nA common header to include is the User-Agent, which identifies the browser and device making the request. Here is an example of what a header can look like:\n\nimport requests\n\nurl = 'https://uconn.edu'\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n                  '(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n}\n\nresponse = requests.get(url, headers=headers)\nprint(response.status_code)  # prints the HTTP status code\nprint(response.text[:500])   # prints the first 500 characters of the HTML\n\n200\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\" class=\"no-js\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt;\n    &lt;link rel=\"profile\" href=\"http://gmpg.org/xfn/11\" /&gt;\n    &lt;link rel=\"pingback\" href=\"https://uconn.edu/xmlrpc.php\" /&gt;\n    &lt;title&gt;University of Connecticut&lt;/title&gt;\n        &lt;script type=\"application/ld+json\"&gt;\n        {\n            \"@context\": \"http://schema.org\",\n            \"@type\n\n\n\n\n\n12.2.3 An Introduction to BeautifulSoup\nBeautifulSoup is one of the most popular Python libraries for web scraping. It helps you collect and organize information from web pages written in HTML or XML. When you look at a web page’s source code, it often looks like a confusing block of text. BeautifulSoup turns that chaos into a clean, organized structure that you can easily search and navigate. It is a reliable tool for collecting data from static websites such as news articles, course lists, or research archives.\n\n12.2.3.1 Installing and Importing BeautifulSoup\nTo use BeautifulSoup, you first need to install it.\npip install beautifulsoup4\nOnce installed, import it into your Python script:\n\nfrom bs4 import BeautifulSoup\n\nBeautifulSoup works well alongside the requests library, creating a simple and efficient way to download and process web pages.\n\n\n12.2.3.2 Loading HTML into BeautifulSoup\nOnce you have downloaded the HTML content of a web page, the next step is to load it into BeautifulSoup.\n\nurl = \"https://decisiondrivers.com/nyc/zip-code/\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n                  \"(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nresponse = requests.get(url, headers=headers)\nprint(\"Status code:\", response.status_code)\n\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\nStatus code: 200\n\n\nThe requests.get() function fetches the page, and BeautifulSoup() parses it into a structure that can be easily searched. The \"html.parser\" argument tells BeautifulSoup how to interpret the HTML. The resulting soup object becomes your workspace for exploring and extracting information. You can now search for tags like &lt;table&gt;, &lt;tr&gt;, or &lt;td&gt; to extract the data you need.\nSince the status code prints 200, that means the request worked successfully and the page’s content is ready to scrape.\n\n\n12.2.3.3 Navigating and Searching the Parse Tree\nBeautifulSoup organizes the HTML into a tree like structure that you can move through. Each tag, paragraph, or link becomes something you can directly access.\nThe most common ways to search are find(), find_all(), and select().\n\nfind() – finds the first instance of a tag\nfind_all() – finds all instances of a tag\nselect() – finds elements using CSS selectors\n\n\n# Find the first heading on the page\nheading = soup.find('h1')\n\n# Find all links on the page\nlinks = soup.find_all('a')\n\n# Use a CSS selector to find specific items\nnav_links = soup.select('nav a')\n\n\n\n12.2.3.4 Extracting Data\nBefore writing any scraping code, it’s helpful to understand where the data actually sits within the webpage. You can right-click anywhere on the site and choose Inspect to open your browser’s developer tools. This reveals the HTML structure of the page, where information is organized using tags such as &lt;table&gt;, &lt;tr&gt;, and &lt;td&gt;. These tags define how the data is arranged, helping you identify exactly which elements you’ll need to target with BeautifulSoup.\nFor example, on the [Decision Drivers NYC ZIP Codes] (https://decisiondrivers.com/nyc/zip-code/) page, the ZIP code information is not stored inside a table but listed under each borough heading. Every borough name appears as a heading tag, such as &lt;h2&gt;, followed by a list of ZIP codes written in plain text or inside &lt;ul&gt; and &lt;li&gt; tags. Instead of looping through table rows, you can search for each heading, record the borough name, and then capture the ZIP codes that appear beneath it. This structure works well with BeautifulSoup because it allows you to identify the borough from the heading and pair it with its corresponding ZIP codes by reading the text that follows.\n\nimport pandas as pd\n\nboroughs = []\nzips = []\n\n# Split the entire text into lines, then look for boroughs and ZIPs\nlines = soup.get_text(\"\\n\", strip=True).splitlines()\ncurrent_borough = None\n\nfor line in lines:\n    text = line.strip()\n    # If the line is a borough name, update the current borough\n    if text.upper() in [\"MANHATTAN\", \"BROOKLYN\", \"QUEENS\", \"BRONX\", \n    \"STATEN ISLAND\"]:\n        current_borough = text.title()\n        continue\n\n    # If the line is a 5-digit number, it’s a ZIP code\n    if current_borough and text.isdigit() and len(text) == 5:\n        boroughs.append(current_borough)\n        zips.append(text)\n\n# Create a clean DataFrame\ndf = pd.DataFrame({\"Borough\": boroughs, \"ZIP\": zips})\ndf = df.drop_duplicates().sort_values([\"Borough\", \"ZIP\"]).reset_index(drop=True)\n\n# Display the first few rows\nprint(df.head(10))\nprint(\"\\nTotal ZIP codes found:\", len(df))\n\n  Borough    ZIP\n0   Bronx  10451\n1   Bronx  10452\n2   Bronx  10453\n3   Bronx  10454\n4   Bronx  10455\n5   Bronx  10456\n6   Bronx  10457\n7   Bronx  10458\n8   Bronx  10459\n9   Bronx  10460\n\nTotal ZIP codes found: 179\n\n\n\n\n12.2.3.5 Extended Example\nThis is an example that will be useful for our midterm assignment. Here, we scrape the addresses of all the precincts in NYC.\n\nurl = \"https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n                  \"(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nresponse = requests.get(url, headers=headers)\nprint(\"Status code:\", response.status_code)\n\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n# Step 1: Locate the table containing the precinct information\ntable = soup.find(\"table\", class_=\"rt\")\n\n# Step 2: If the table is found, extract all precinct rows\nif table:\n    rows = table.find_all(\"tr\")\nelse:\n    print(\"No table found on this page.\")\n    rows = []\n\n# Step 3: Prepare a list to store precinct data as dictionaries\nprecinct_data = []\n\n# Step 4: Loop through each row and collect precinct name and address\nfor row in rows:\n    precinct_cell = row.find(\"td\", attrs={\"data-label\": \"Precinct\"})\n    address_cell = row.find(\"td\", attrs={\"data-label\": \"Address\"})\n\n    if precinct_cell and address_cell:\n        precinct_name = precinct_cell.text.strip()\n        address = address_cell.text.strip()\n        precinct_data.append({\n            \"Precinct\": precinct_name,\n            \"Address\": address\n        })\n\nprecincts_df = pd.DataFrame(precinct_data)\n\n# Step 6: Display a quick summary\nprint(f\"Extracted {len(precincts_df)} precincts successfully.\")\nprint(precincts_df.head())\n\nStatus code: 200\nExtracted 78 precincts successfully.\n       Precinct              Address\n0  1st Precinct    16 Ericsson Place\n1  5th Precinct  19 Elizabeth Street\n2  6th Precinct   233 West 10 Street\n3  7th Precinct   19 1/2 Pitt Street\n4  9th Precinct    321 East 5 Street\n\n\n\n\n\n12.2.4 An Introduction to Selenium\nWhile BeautifulSoup works best for static websites, not all web pages are created equal. Many modern sites use JavaScript to load content after the initial HTML has already been sent to the browser. This means that if you use requests and BeautifulSoup, you might get an empty page or partial data. This is where you use Selenium.\nSelenium is a powerful browser automation tool that lets you control an actual web browser, like Chrome or Firefox, through Python. It can click buttons, fill out forms, scroll through pages, and load JavaScript rendered content before scraping it. You can then pass the fully loaded page to BeautifulSoup for parsing.\n\n12.2.4.1 Installing and Setting Up Selenium\nTo use Selenium, you first need to install it.\npip install selenium\nSelenium also requires a WebDriver, which acts as a bridge between Python and your browser. If you are using Google Chrome, download ChromeDriver and make sure it is installed correctly. You can verify this by typing the following command in your terminal:\nchromedriver --version\nIf you are on macOS and see the error “chromedriver cannot be opened because the developer cannot be verified,” you can fix it by running this command:\nxattr -d com.apple.quarantine $(which chromedriver)\nAfter this, your WebDriver should work normally.\nNow we can work on the code itself. This is how we set up Selenium:\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nimport os\n\nBefore launching Chrome, it’s common to set a few options that control how it behaves. For scraping, “headless” mode is useful because it runs Chrome without opening a window. It is faster and less distracting.\n\n# Create Chrome options to control browser behavior\noptions = Options()\n\n# Run Chrome invisibly (no window)\noptions.add_argument(\"--headless\")\n\n# Disable GPU acceleration for smoother headless operation\noptions.add_argument(\"--disable-gpu\")\n\n# Helps avoid permission issues in some environments\noptions.add_argument(\"--no-sandbox\")\n\nInstead of typing out the full path manually, this function looks through your system’s PATH to find where chromedriver is installed.\n\ndef find_chromedriver():\n\n    # Loop through all directories listed in the system PATH\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n\n        # Build the potential path to chromedriver\n        driver_path = os.path.join(path, \"chromedriver\")\n\n        # Check if it exists and is executable\n        if os.path.isfile(driver_path) and os.access(driver_path, os.X_OK):\n\n            return driver_path  # Return the path if found\n\n    return None  # Return None if not found\n\n# Run the function to locate chromedriver\nchromedriver_path = find_chromedriver()\n\n# If found, print its path\nif chromedriver_path:\n    print(\"Found ChromeDriver at:\", chromedriver_path)\n\n# If not found, raise an error with instructions\nelse:\n    raise FileNotFoundError(\"ChromeDriver not found.\")\n\nFound ChromeDriver at: /usr/local/bin/chromedriver\n\n\nOnce you have the driver path and your browser settings, you can start Chrome.\n\n# Import the Service class and connect the driver\nservice = Service(chromedriver_path)\n\n# Start the Chrome WebDriver with the defined options\ndriver = webdriver.Chrome(service=service, options=options)\n\nAt this point, Selenium is ready. You can visit a webpage by calling:\n\ndriver.get(\"https://example.com\")\n\n\n\n12.2.4.2 Using Selenium to Load Dynamic Pages\nHere is a simple example that shows how Selenium can open a website, wait for it to load, and then pass the content to BeautifulSoup.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n\ndef scrape_schedule(url, league):  # league = \"NFL\" or \"MLB\"\n    league = league.upper()\n    # Identify the correct team link pattern based on the league\n    team_href = \"/nfl/team/_/name\" if league == \"NFL\" else \"/mlb/team/_/name\"\n\n    # Configure and launch Chrome\n    options = Options()\n    options.add_argument(\"--headless=new\")\n    options.add_argument(\"--window-size=1920,1080\")\n    driver = webdriver.Chrome(options=options)\n\n    # Load the ESPN schedule page\n    driver.get(url)\n    wait = WebDriverWait(driver, 25)\n\n    # Wait until key elements are visible (date headers and team links)\n    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n    \"div.Table__Title\")))\n    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n    f\"a.AnchorLink[href*='{team_href}']\")))\n\n    rows_out = []\n\n    # Extract data by date section\n    # Each schedule day starts with a &lt;div class=\"Table__Title\"&gt; element\n    for title in driver.find_elements(By.CSS_SELECTOR, \"div.Table__Title\"):\n        date_text = title.text.strip()\n        if not date_text:\n            continue  # Skip if the date header is empty\n\n        # The table directly after the title contains the matchups\n        try:\n            table = title.find_element(By.XPATH, \"following::table[1]\")\n        except:\n            continue  # Skip if structure differs\n\n        # Extract teams for each game\n        for row in table.find_elements(By.XPATH, \n        \".//tr[contains(@class,'Table__TR')]\"):\n            teams = [\n                a.text.strip()\n                for a in row.find_elements(By.CSS_SELECTOR, \n                f\"td a.AnchorLink[href*='{team_href}']\")\n                if a.text.strip()\n            ]\n            if len(teams) &gt;= 2:\n                # Format as \"Away @ Home\"\n                rows_out.append({\"Date\": date_text, \"Matchup\": f\"{teams[0]} @ {\n                    teams[1]}\"})\n\n    # --- Step 5: Clean up and close browser ---\n    driver.quit()\n\n    # Build a clean DataFrame\n    df = pd.DataFrame(rows_out, columns=[\"Date\", \"Matchup\"])\n    df[\"League\"] = league\n    return df\n\n\n# --- Step 6: Run for both leagues ---\nnfl_df = scrape_schedule(\"https://www.espn.com/nfl/schedule\", \"NFL\")\nmlb_df = scrape_schedule(\"https://www.espn.com/mlb/schedule\", \"MLB\")\n\nprint(\"===== NFL =====\")\nprint(nfl_df)\nprint(\"\\n===== MLB =====\")\nprint(mlb_df)\n\n===== NFL =====\n                           Date                     Matchup League\n0   Thursday, November 20, 2025           Buffalo @ Houston    NFL\n1     Sunday, November 23, 2025        Pittsburgh @ Chicago    NFL\n2     Sunday, November 23, 2025    New England @ Cincinnati    NFL\n3     Sunday, November 23, 2025          New York @ Detroit    NFL\n4     Sunday, November 23, 2025       Minnesota @ Green Bay    NFL\n5     Sunday, November 23, 2025         Seattle @ Tennessee    NFL\n6     Sunday, November 23, 2025  Indianapolis @ Kansas City    NFL\n7     Sunday, November 23, 2025        New York @ Baltimore    NFL\n8     Sunday, November 23, 2025       Cleveland @ Las Vegas    NFL\n9     Sunday, November 23, 2025      Jacksonville @ Arizona    NFL\n10    Sunday, November 23, 2025       Philadelphia @ Dallas    NFL\n11    Sunday, November 23, 2025       Atlanta @ New Orleans    NFL\n12    Sunday, November 23, 2025     Tampa Bay @ Los Angeles    NFL\n13    Monday, November 24, 2025    Carolina @ San Francisco    NFL\n\n===== MLB =====\n                         Date                Matchup League\n0  Saturday, November 1, 2025  Los Angeles @ Toronto    MLB\n\n\nHere is how the code works:\n\nSetting Up Selenium\n\n\nThe first part initializes the Chrome browser in headless mode so it runs quietly in the background.\n\n\nWaiting for the Page to Load\n\n\nSelenium doesn’t automatically know when a website’s content is ready. Using WebDriverWait and expected_conditions, the script pauses until:\n\nA date header (div.Table__Title) appears\nA team link with a specific pattern (/nfl/team/_/name or /mlb/team/_/name) exists\n\n\n\nLooping Through Each Day’s Games\n\n\nEach date on the ESPN page appears as a header (Table__Title), followed by a table of games. The script finds each table using the XPath rule \"following::table[1]\", meaning the first table that comes after the header.\n\n\nExtracting Teams from Each Row\n\n\nInside each table, every row (tr) represents one game. The team names are stored in &lt;a&gt; tags with URLs containing the league’s team name pattern. Selenium grabs these links and combines them into \"Away @ Home\" format.\n\n\nCreating a DataFrame\n\n\nOnce all data is collected, it’s turned into a Pandas DataFrame with columns:\n\nDate: the game date from the header\nMatchup: formatted team pairing\nLeague: either NFL or MLB\n\n\n\nClosing the Browser\n\n\nAlways close the browser using driver.quit() to free up system resources.\n\n\n\n\n12.2.5 Further Reading\n\nPython Web Scraping Tutorials\nPython Web Scraping: Full Tutorial With Examples\nUltimate Guide to Web Scraping with Python\nBeautifulSoup Documentation\nWebscraping With Selenium",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "90-advanced.html#uniform-manifold-approximation-and-projection",
    "href": "90-advanced.html#uniform-manifold-approximation-and-projection",
    "title": "12  Advanced Topics",
    "section": "12.3 Uniform Manifold Approximation and Projection",
    "text": "12.3 Uniform Manifold Approximation and Projection\nThis section was prepared by Mark Zhang, an undergraduate at UCONN.\n\n12.3.1 Introduction\nUniform Manifold Approximation and Projection (UMAP) was introduced by Leland McInnes, John Healy, and James Melville in 2018 (mcinnes2020umapuniformmanifoldapproximation?). UMAP is a powerful dimensionality reduction technique designed to visualize high-dimensional data in a low-dimensional space, such as 2D or 3D. Unlike linear methods like PCA, UMAP captures nonlinear structures, preserving local clusters while also maintaining the overall global patterns of the data. UMAP is utilized for clustering, data visualization, and data exploration.\n\n\n12.3.2 Other Dimensionality Reduction Techniques\n\n12.3.2.1 Principle Component Analysis (PCA)\n\nlinear technique; projects data along directions of maximum variance.\nPreserves global structure fairly well but may lose local neighborhood. details.\ngood for visualization of 2-3 PCs but cannot capture nonlinear structures.\n\n\n\n12.3.2.2 t-distributed Stochastic Neighbor Embedding (t-SNE)\n\nfinds distances between pairs of points, maps onto t dist.\nnonlinear technique; preserves local structure very well.\ncan distort global relationships.\noften slower than UMAP for large datasets.\n\n\n\n\n12.3.3 Algorithm Approach and Intuition\nUMAP is a graph-based dimensionality reduction algorithm rooted in algebraic topology. It represents data as a fuzzy graph and finds a low-dimensional embedding that preserves both local neighborhoods and global structure.\n\n12.3.3.1 Mathematical Foundations\n\nRiemannian Manifold - UMAP assumes the data lies on a Riemannian manifold, a smoothly curved surface embedded in high-dimensional space.\nSimplicial Sets - From algebraic topology, a simplex is the simplest geometric building block (a point, line, triangle, etc.), and a simplicial complex describes how these pieces connect to form a space. Geometrically a k-simplex is a very simple way to build a k-dimensional object.\n\n\n\n12.3.3.2 Learning the manifold structure\nUMAP builds a graph as a discrete approximation of the manifold’s structure. To aid in visualizing the concepts, I will show how they apply to a noisy sin wave data set.\n\n\n\n\n\nTest data set of a noisy sin wave\n\n\n\n\nHow do we represent this data as a manifold? UMAP uses the concept of simplicial sets and topology and adapts them to fuzzy simplicial sets and metric space.\n\n12.3.3.2.1 Finding nearest neighbors\nFor each point we find the nearest neighbors. The nneighbors hyperparameter controls how local or global we want our structure to be. A large amount of neighbors represent the manifold more accurately as a whole and a smaller number would capture fine details of the structure.\n\n\n\n\n\nFuzzy circles to 4th nearest neighbor\n\n\n\n\n\n\n12.3.3.2.2 Constructing a neighborhood graph\nUMAP represents high-dimensional data as a fuzzy simplicial set, capturing the local connectivity of points. For each point, a fuzzy membership weight is assigned to its neighbors: \\[\nw_{ij} = \\exp\\Bigg(-\\frac{d_{ij} - \\rho_i}{\\sigma_i}\\Bigg),\n\\]\nwhere:\n\n\\(d_{ij}\\) is the distance between points \\(i\\) and \\(j\\),\n\\(\\rho_i\\) is the distance to the closest neighbor of \\(i\\),\n\\(\\sigma_i\\) is a local scaling factor.\n\nIn other words, each point adjusts its influence based on the local density of its neighborhood. Dense regions, where points are very close together, have smaller \\(\\sigma_i\\), which reduces the weight of individual connections. Sparse regions, with points farther apart, have larger \\(\\sigma_i\\). , increasing the weight of each connection.\nSince \\(w_{i \\to j}\\) and \\(w_{j \\to i}\\) can differ, UMAP combines them using a probabilistic union:\n\\[\nw_{ij}^{(final)} = w_{i \\to j} + w_{j \\to i} - w_{i \\to j} \\cdot w_{j \\to i}.\n\\]\nThis produces a symmetric weight between 0 and 1 that preserves local topology, balances contributions across dense and sparse regions, and yields a uniform manifold representation.\n\n\n\n\n\nFuzzy weighted graph with UMAP-style combined weights\n\n\n\n\n\n\n\n12.3.3.3 Finding a low-dimensional representation\nAfter building the fuzzy simplicial set (the weighted neighborhood graph), UMAP tries to embed the points in a low-dimensional space (e.g., 2D or 3D) while preserving the relationships encoded in the graph. This is done by optimizing a cost function.\n\n12.3.3.3.1 Minimum Distance\nThe min_dist parameter controls how close points can be in the low-dimensional embedding:\n\nSmall min_dist - points can be closer together, resulting in dense clusters that preserve fine local structure.\n\nLarge min_dist - points are forced farther apart, producing spread-out clusters that highlight global relationships.\n\nIn other words, min_dist adjusts the tightness of clusters in the embedding without changing the underlying topology of the data.\n\n\n12.3.3.3.2 Minimizing the cost function (Cross-Entropy)\nAfter constructing the fuzzy simplicial set and initializing the low-dimensional embedding, UMAP positions the points by minimizing a cross-entropy-like cost function:\nTo compute the similarity between points in the low-dimensional embedding, UMAP defines a corresponding low-dimensional weight for each pair of points:\n\\[\nw_{ij}^{(\\text{low})} = \\frac{1}{1 + a \\|y_i - y_j\\|^{2b}},\n\\]\nwhere:\n\n\\(\\|y_i - y_j\\|\\) is the Euclidean distance between points \\(y_i\\) and \\(y_j\\) in the low-dimensional space (2D or 3D),\n\\(a\\) and \\(b\\) are hyperparameters that control the shape of the attraction/repulsion curve.\n\nThe cross-entropy cost function is then defined as: \\[\nC = \\sum_{i \\neq j} \\Big[ w_{ij} \\log \\frac{1}{w_{ij}^{(\\text{low})}} +\n(1 - w_{ij}) \\log \\frac{1}{1 - w_{ij}^{(\\text{low})}} \\Big],\n\\] where: - \\(w_{ij}\\) = high-dimensional fuzzy weight between points \\(i\\) and \\(j\\), - \\(w_{ij}^{(\\text{low})}\\) = corresponding low-dimensional embedding weight.\nIntuition:\n\nIf two points are strongly connected in high dimensions (large \\(w_{ij}\\)), the embedding tries to place them close together.\n\nIf two points are weakly connected (small \\(w_{ij}\\)), the embedding tries to place them far apart.\n\nUMAP uses stochastic gradient descent to minimize this cost function, adjusting the coordinates of points in low dimensions so that the embedding preserves both local neighborhoods and global manifold structure.\n\n\n\n\n12.3.4 Practical Example: UMAP on 28x28 digit data\nFirst, install the necessary dependency:\npip install umap-learn\nBefore applying UMAP, let’s look at the raw data. The MNIST dataset from scikit-learn contains 70,000 images of handwritten digits, each represented as a 28×28 grayscale image (784 features per sample).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\n# Load MNIST\nmnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\nX = mnist.data[:2000]\ny = mnist.target[:2000].astype(int)\n\n# Reshape to 28×28 images\nimages = X.reshape(-1, 28, 28)\n\n# Show first 12 digits in a 3×4 grid\nfig, axes = plt.subplots(3, 4, figsize=(6, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.set_title(y[i])\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nHandwritten digit images (28×28) from MNIST dataset\n\n\n\n\nWe now project the 784-dimensional data into a 2D embedding using UMAP. The parameters below control the embedding:\n\nn_neighbors: how many nearby points are used to estimate local structure.\nmin_dist: how tightly UMAP packs points in the low-dimensional space.\n\n\nfrom umap.umap_ import UMAP\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\n# Load MNIST\nmnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\nX = mnist.data[:2000]\ny = mnist.target[:2000].astype(int)\n\n# 2D UMAP projection\nreducer = UMAP(\n    n_components=2,\n    n_neighbors=15,\n    min_dist=0.1,\n    random_state=42\n)\nX_umap = reducer.fit_transform(X)\n\n# 2D scatter plot\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(\n    X_umap[:, 0], X_umap[:, 1],\n    c=y, cmap='tab10', s=5\n)\n\nplt.title(\"2D UMAP projection of MNIST (28×28)\")\nplt.xlabel(\"UMAP 1\")\nplt.ylabel(\"UMAP 2\")\nplt.colorbar(scatter, label='Digit')\nplt.show()\n\n\n\n\n2D UMAP of MNIST dataset\n\n\n\n\nIn the 2D UMAP embedding of the digits dataset, each point represents a digit image, colored by its label. The plot shows that UMAP successfully captures the structure of high-dimensional data: points of the same digit cluster tightly, while visually similar digits appear near each other, preserving meaningful relationships.\n\nSame digits - form tight clusters, showing that UMAP preserves local neighborhood structure and keeps similar points together.\n\nSimilar digits (e.g., 3 & 8) - positioned near each other, reflecting their visual similarity in shape and style.\n\nDifferent clusters - mostly separate, indicating clear class separation and that UMAP maintains distinctions between digit types.\n\nGlobal layout - reflects the relative positions of all clusters , preserving overall structure of the dataset.\n\n\nTakeaway: UMAP reveals both local and global patterns, making high-dimensional data interpretable in 2D.\n\n\n\n12.3.5 Conclusion\nUMAP is a nonlinear dimensionality reduction technique that effectively captures both local and global structure in high-dimensional data. By constructing a fuzzy neighborhood graph and optimizing a low-dimensional embedding, it preserves cluster relationships, highlights visually similar points, and maintains overall manifold structure. This makes UMAP a powerful, interpretable tool for visualization and exploratory data analysis.\n\n\n12.3.6 Further Readings\n\nUMAP Paper",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "90-advanced.html#how-to-call-r-from-python",
    "href": "90-advanced.html#how-to-call-r-from-python",
    "title": "12  Advanced Topics",
    "section": "12.4 How to call R from Python",
    "text": "12.4 How to call R from Python\nThis section was prepared by Wilson Tang, a undergraduate junior as of fall 2025 pursuing a single degree in statistical data science.\n\n12.4.1 Motivations\nR and Python are both languages we have been exposed to the most at UConn. I’ve had moments using one where I think about how the other might be useful, and so one might want to incorporate both languages. I want to show how we can properly incorporate R and Python together and when each language may be better.\nData science workflow:\nUnderstanding the data science workflow will help us decide if we want to use R and Python together, or just individually.\n\nIdentify problem\n\nGather data\n\nClean & preprocess data\n\nData exploration\n\nFeature engineering / external data\nStatistical testing\n\nModeling & machine learning\n\nEvaluation\n\nVisualization\n\n\n\n12.4.2 Corresponding packages\nFor most tasks, we can find corresponding packages in both Python and R that can generally accomplish the same tasks. Of course, do mind exceptions can occur for very specific tasks and you should make decisions based on the scope of your work.\n\nCleaning / Preprocessing Data\n\nR: dplyr\nPython: pandas\n\nExploratory Data Analysis\n\nR: built-in functions, dplyr, ggplot2\nPython: pandas, matplotlib, seaborn, plotnine\n\nFeature Engineering\n\nR: recipes (tidymodels), forcats, lubridate\nPython: sklearn, pandas\n\nEvaluation\n\nR: yardstick, caret\nPython: sklearn, yellowbrick\n\nVisualization\n\nR: ggplot2\nPython: plotnine, matplotlib, seaborn\n\n\n\n\n12.4.3 R / Python advantages\nR:\n\nStatistical testing: t-test, ANOVA, chi-squared, Shapiro-wilk are common tests built into R. Durbin Test and a fisher’s test that can work for any r x c contingency table.\nEconometrics, time-series and biostatistics.\n\nPython:\n\nBeing able to handle bigger data\nMachine learning and AI\n\nsklearn for machine learning\nTensor flow / Keras / Pytorch for deep learning\nXGBoost / LightGBM / CatBoost for gradient boosting\nPipelines\n\n\nVersatile data handling\n\nAPIs - Better for large scale / complex integration (‘requests’, ‘httpx’, ‘aiohttp’)\nWeb Scraping - Better for large scale / dynamic web scraping (‘Beautiful’, ‘Scrapy’, ‘Selenium’)\nDatabases - SQL, NoSQL and big platforms\nCloud data - R has limited options for this and sometimes rely on Python libraries\nReal-time data - R can’t really do this\n\n\n\n\n12.4.4 Setup both R and Python\nMake sure you have your own python:\nWindows comes with python, however we want to install our own because it will prevent errors in the future steps. (Microsoft sucks)\nInstall python\nDownload R and Install R extension:\nInstall R\nFind the Extensions tab on the left toolbar, search R language and install the R extension for VS code. And you can use Ctrl+Shift+P and type R: Create R terminal.\nCalling R in Python:\nWe will use the package rpy2 to call R from Python, there are also other packages that can call R, check Additional resources.\nCalling R from Python is a Python first method, and is generally recommended if the user is going to be using more python. However, one can also call Python from R as well. Check Additional resources for packages in R that allows us to call Python from R.\nReproducibility:\nSince we are trying to use 2 languages at once, there will be packages from both R and Python. Of course, we would never want to compromise on reproducibility so what should we do? The best method would be to use Conda.\nA single Conda environment can specify both versions of Python as well as R, while also allowing us to keep the packages of both in one place.\n\n\n12.4.5 Example\nPrerequisites:\nIn order to render these classnotes you will need to have a Python virtual environment with packages from requirements.txt and the package gamlss needs to be installed into your R global environment.\nGAMLSS or Generalized Additive Models for Location, Scale, and Shape. It allows all parameters of the response distribution to depend on explanatory variables and thus each parameter can be modeled as a function of predictors. This model is useful when variability changes with predictors, skewness / kurtosis change with predictors.\nLoad Python libraries:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nLoad rpy2 to call R interface:\n\nfrom rpy2 import robjects\nfrom rpy2.robjects import pandas2ri, Formula\nimport rpy2.robjects.packages as rpackages\nfrom rpy2.robjects.conversion import localconverter\n\nError importing in API mode: ImportError(\"dlopen(/Users/junyan/work/teaching/ids-f25/ids-f25/.ids-f25-py312/lib/python3.12/site-packages/_rinterface_cffi_api.abi3.so, 0x0002): symbol not found in flat namespace '_R_BaseEnv'\")\nTrying to import in ABI mode.\n\n\nLoading custom .RprofileLoading custom .Rprofile\n\n\nCall gamlss from rpy2, rpy2 will look for the R global environment is the default location:\n\ngamlss = rpackages.importr('gamlss')\n\nLoad in penguins from seaborn and prepare Data:\n\npenguins = sns.load_dataset(\"penguins\").dropna()\n\npenguins = penguins.rename(columns={\n    \"body_mass_g\": \"mass\",\n    \"bill_length_mm\": \"bill\",\n    \"flipper_length_mm\": \"flipper\"\n})\n\npenguins = penguins[[\"mass\", \"bill\", \"flipper\", \"species\", \"sex\"]]\n\nSend data frame from pandas to R and have it be converted to a R data frame using rpy2:\n\nwith localconverter(robjects.default_converter + pandas2ri.converter):\n    r_df = robjects.conversion.py2rpy(penguins)\n\nHere we are saying that mass depends on flipper, bill, species, sex and the variability of mass depends on flipper. Run gamlss from the R data frame to extract whatever we need from gamlss, in this case fitted \\(\\sigma\\):\n\nformula_mu = Formula(\"mass ~ flipper + bill + species + sex\")\nformula_sigma = Formula(\"~ flipper\")\n\nmodel = gamlss.gamlss(formula_mu, sigma_formula=formula_sigma,\n                      data=r_df, family=\"NO\")\n\nfitted_sigma = robjects.r(\"fitted.values\")(model, \"sigma\")\n\nGAMLSS-RS iteration 1: Global Deviance = 4718.449 \nGAMLSS-RS iteration 2: Global Deviance = 4718.415 \nGAMLSS-RS iteration 3: Global Deviance = 4718.415 \n\n\nSend fitted values or anything extracted from the previous part:\n\nwith localconverter(robjects.default_converter + pandas2ri.converter):\n    penguins[\"fitted_sigma\"] = robjects.conversion.rpy2py(fitted_sigma)\n\nUse extracted values for anything of interest in Python, in this case let’s say for some reason we wanted to use matplotlib and seaborn to plot our fitted \\(\\sigma\\):\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(x=\"flipper\", y=\"fitted_sigma\", hue=\"species\", data=penguins, \n                alpha=0.6)\nsns.lineplot(x=\"flipper\", y=\"fitted_sigma\", hue=\"species\", data=penguins, lw=2)\nplt.title(\"GAMLSS: Predicted Variability vs Flipper Length\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Predicted Body Mass Variability (grams)\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nWe see here that as flipper length increases, predicted body mass variability tends to decrease.\n\n\n12.4.6 Additional resources\n\nMore packages to call R in Python and vice versa\nMore about Python / R strengths and workflow with conda",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "13  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Positron, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a HTML.\nPrint the HTML file to a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhy is the command line interface if preferred among the professionals?\nWhat are the advantages of Linux over Windows?\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\nWhy do I require at least 10 commits for each assignment?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\nExplain in words which strategy is preferred. Use analytic results to help if you could derive them.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\nTest your function with \\((1, 3, 9, 10)\\), \\((4, 4, 10, 10)\\), \\((1, 5, 5, 5)\\), \\((3, 3, 7, 7)\\), \\((3, 3, 8, 8)\\), and $(1, 4, 5, 6).\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. The data of the Labor Day week, 2025, was downloaded by filtering with CRASH Date in between 11:59:59 PM August 30 and 12:00:00 AM September 7, available in data/nyc_crashes_lbdwk_2025.csv.\n\nRead the data into a Pandas data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data curator, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration In the cleaned crash data in feather format (thank Wilson Tang for his work, we have a variable zip_filled which is true if, in the raw data, a crash has nonmissing geocode but missing zip code, which was filled during the cleaning process. This variable could be viewed as a data quality metric.\n\nConstruct a contigency table for zip_filled and borough. Is the pattern of fillable zips the same across boroughs? Formulate a hypothesis and test it.\nConstruct an hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash Severity Modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Noise complaints in NYC The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with requests to NYPD on noise complaints that are created from 12:00:00 AM 06/29/2025 to 12:00:00 PM 07/05/2025. The subset is available in CSV format as data/nypd311w062925noise_by100625.csv. Read the data dictionary online to understand the meaning of the variables. For a clean workflow, this data is not to be uploaded to your GitHub repo. You will lose style points if you do. Instead, it should be placed in an untracked local folder data, so your analysis is reproducible as long as I have the same folder on my computer.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Justify and clean them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second; action_update_date after closed_date.\nSummarize your suggestions to the data curator in several bullet points.\n\nData exploration.\n\nIf we suspect that response time may depend on the time of day when a complaint is made, we can compare the response times for complaints submitted during nighttime and daytime. To do this, we can visualize the comparison by complaint type, borough, and weekday (vs weekend/holiday).\nPerform a formal hypothesis test to confirm the observations from your visualization. Formally state your hypotheses and summarize your conclusions in plain English.\nCreate a binary variable over2h to indicate that a service request took two hours or longer to close.\nDoes over2h depend on the complaint type, borough, or weekday (vs weekend/holiday)? State your hypotheses and summarize your conclusions in plain English.\n\nData analysis.\n\nThe addresses of NYC police precincts are stored in data/nypd_precincts.csv. Use geocoding tools to find their geocode (longitude and latitude) from the addresses.\nCreate a variable dist2pp which represent the distance from each request incidence to the nearest police precinct.\nAdd zip code level variables by merging with data from US Census.\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over2h for the noise complaints with the training data, using all the variables you can engineer from the available data. If you have tuning parameters, justify how they were selected.\nAssess the performance of your model in terms of commonly used metrics. Summarize your results to a New Yorker who is not data science savvy.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\nAmerican Statistical Association (ASA). (2018). Ethical guidelines\nfor statistical practice.\n\n\nBansal, S. (2018). Python data visualisation made easy with\nplotnine: A how-to guide. https://medium.com/@suraj_bansal/python-data-visualisation-made-easy-with-plotnine-a-how-to-guide-f71e321bdef1;\nMedium.\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984).\nClassification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting\nsystem. Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nDe Cock, D. (2009). Ames, Iowa: Alternative to the\nBoston housing data as an end of semester regression\nproject. Journal of Statistics Education, 17(3), 1–13.\nhttps://doi.org/10.1080/10691898.2009.11889627\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient\nboosting machine. The Annals of Statistics, 29(5),\n1189–1232.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference, and prediction.\nSpringer.\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &\nLiu, T.-Y. (2017). LightGBM: A highly efficient gradient\nboosting decision tree. Advances in Neural Information Processing\nSystems, 3146–3154.\n\n\nMcCulloch, W. S., & Pitts, W. (1943). A logical calculus of the\nideas immanent in nervous activity. The Bulletin of Mathematical\nBiophysics, 5(4), 115–133.\n\n\nMinsky, M., & Papert, S. A. (1969). Perceptrons: An introduction\nto computational geometry. MIT Press.\n\n\nNelder, J. A., & Wedderburn, R. W. M. (1972). Generalized linear\nmodels. Journal of the Royal Statistical Society Series A:\nStatistics in Society, 135(3), 370–384.\n\n\nPace, R. K., & Barry, R. (1997). Quick computation of regressions\nwith a spatially autoregressive dependent variable. Geographical\nAnalysis, 29(3), 232–247.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin,\nA. (2018). CatBoost: Unbiased boosting with categorical features.\nAdvances in Neural Information Processing Systems, 6638–6648.\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nRosenblatt, F. (1958). The perceptron: A probabilistic model for\ninformation storage and organization in the brain. Psychological\nReview, 65(6), 386.\n\n\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning\nrepresentations by back-propagating errors. Nature,\n323(6088), 533–536. https://doi.org/10.1038/323533a0\n\n\nSarkar, D. (DJ). (2018). A comprehensive guide to the grammar of\ngraphics for effective visualization of multi-dimensional data. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149/;\nMedium, Towards Data Science.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nWilkinson, L. (2012). The grammar of graphics. Springer.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The\npractice of responsible data analysis and decision making. MIT\nPress.",
    "crumbs": [
      "References"
    ]
  }
]