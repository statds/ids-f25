[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book is free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-f25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Spring 2025, Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of useful data challenges:\n\nASA Data Challenge Expo: big data in 2025\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2026.\nA good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAgostino, Michael Angelo\n\nFurther understanding of github and improve my workflow.\nBuild a project in order to gain real world data science experience.\n\nBlake, Roger Cooley\n\nGain experience using git and github\nComplete a project that transforms real-world data into valuable insights\nMaster control over my computer and its file system\n\nBone, Dylan Jason\nCao, Enshuo\nChen, Irene\nChen, Jingang Calvin\nFazzina, Sophia Carmen\nFu, Wenwei\nHaerter, Alejandro Erik\nLawrence, Claire Elise\nLevine, Hannah Maya\nLucey, Sonia Niamh\n\nLearn practical applications of Data Science and Economics\nLearn how to use Git/GitHub\nImprove Python/coding skills\n\nMayer-Costa, Jaden Paulo\nMilun, Lewis Aaron\nMontalvo, Victor Samuel\nPatel, Sahil Sanjay\nPatel, Tulsi Manankumar\nPerkins, Jack Thomas\nSaltus, Quinn Lloyd Turner\nSaxena, Aanya\nSchlessel, Jacob E\nSgro, Owen Bentley\nTang, Wilson Chen\n\nI want to be very comfortable with using GitHub and GitBash\nI want to learn how to have a clear style\nI want to use programming tools in a professional way\n\nTran, Justin\nWhite, Abigail Lynn\nWishneski, Emma Irene\nYoon, Jessica Nari\nZhang, Mark Justin\n\nGet comfortable with command line and Git\nLearn to make my own data science projects\nlearn theory and application of ML algorithms\n\n\n\n\nStudents in 5255\n\nAnzalone, Matthew James\nGomez-Haibach, Konrad\nPlotnikov, Alexander",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(2819 + 4075 + 6227 + 5139 + 4768 + 109)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Chen,Jingang Calvin',\n 'Montalvo,Victor Samuel',\n 'Cao,Enshuo',\n 'Chen,Irene',\n 'Haerter,Alejandro Erik',\n 'Saxena,Aanya',\n 'Gomez-Haibach,Konrad',\n 'Sgro,Owen Bentley',\n 'Lucey,Sonia Niamh',\n 'Yoon,Jessica Nari',\n 'Patel,Sahil Sanjay',\n 'Plotnikov,Alexander',\n 'Milun,Lewis Aaron',\n 'Patel,Tulsi Manankumar',\n 'Perkins,Jack Thomas',\n 'Fazzina,Sophia Carmen',\n 'Tang,Wilson Chen',\n 'Wishneski,Emma Irene',\n 'Lawrence,Claire Elise',\n 'White,Abigail Lynn',\n 'Mayer-Costa,Jaden Paulo',\n 'Anzalone,Matthew James',\n 'Zhang,Mark Justin',\n 'Saltus,Quinn Lloyd Turner',\n 'Tran,Justin',\n 'Levine,Hannah Maya',\n 'Blake,Roger Cooley',\n 'Schlessel,Jacob E',\n 'Agostino,Michael Angelo']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proportions and rates\nDatabase operation with Structured query language (SQL)\nGrammar of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map\nAnimation\nSupport vector machine\nRandom forest\nGradient boosting machine\nNaive Bayes\nNeural networks basics\nMLP/ANN/CNN/RNN/LSTM\nUniform manifold approximation and projection\nAutomatic differentiation\nDeep learning\nTensorFlow\nAutoencoders\nK-means clustering\nPrincipal component analysis\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\nMaking presentations with Quarto\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/10\nLi, Shiyi\nA Primer of Markdown\n\n\n02/12\nJun, Joann\nMaking Presentations with Quarto\n\n\n02/17\nRoy, Luke William\nGrammar of Graphics with Plotnine\n\n\n02/19\nLang, Lang\nData manipulation with Pandas\n\n\n02/24\nAckerman, John\nPerforming Statistical Tests (SciPy)\n\n\n02/26\nHorn, Alyssa Noelle\nDatabase operation with Structured query language (SQL)\n\n\n03/03\nEl Zein, Amer Hani\nEffective Communication in Data Science\n\n\n03/05\nSchittina, Thomas\nSpatial Data With Geopandas & Google Maps\n\n\n03/10\nKline, Daniel Esteban\n\n\n\n03/12\nEdo, Mezmur Wossenu\nPrincipal Component Analysis (PCA)\n\n\n03/26\nAlsadadi, Ammar Shaker\nSentiment Analysis with Python\n\n\n03/26\nFables, Xavier Milan\nVariable Importance Metrics in Supervised Learning\n\n\n03/31\nTamhane, Shubhan\nSupport Vector Machine\n\n\n03/31\nNasejje, Ruth Nicole\nPersonal webpage on GitHub\n\n\n04/02\nLagutin, Vladislav\nGoogle Maps visualizations using Folium library\n\n\n04/02\nZhang, Gaofei\nRandom forest\n\n\n04/07\nLong, Ethan Kenneth\nSynthetic Minority Oversampling Technique (SMOTE)\n\n\n04/07\nXu, Peiwen\nDeveloping a Python package\n\n\n04/09\nKravette, Noah\nWorking with NetCDF Data\n\n\n04/09\nSymula, Sebastian\nImputation Methods for Missing Data\n\n\n04/09\nTomaino, Mario Anthony\nK-Prototypes Clustering\n\n\n04/14\nReed, Kyle Daniel\nAutoencoders\n\n\n04/14\nChen, Yifei\nExplaining XGBoost Predictions with SHAP\n\n\n04/14\nMundiwala, Mohammad Moiz\nMath animations with manim\n\n\n04/16\nLin, Selena\nNaive Bayes\n\n\n04/16\nPfeifer, Nicholas Theodore\nChoosing the number of clusters in clustering\n\n\n04/16\nVellore, Ajeeth Krishna\nNeural Network Basics\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is available under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\n5\n\n\n11/20\n5\n\n\n12/02\n5\n\n\n12/04\n5\n\n\n12/??\n6\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file _mysection.qmd as an example, which is includeed in index.qmd. See also how _ethics.qmd is included into intro.qmd for example.\nTips on making contributions:\n\nNo plagiarism.\nAvoid external graphics.\nUse simulated data.\nUse data from homework assignments.\nCite article/book references (learn how from our sources).\nInclude a subsection of Further Readings.\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance for feedbacks.\n\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?  \nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nIf you need a reference letter, how could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denote computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have that users previously faced before WSL was introduced.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump-start, here is a tutorial: Ubuntu Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding style against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McKinney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data-science-ethics",
    "href": "intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) ((ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-buildnotes",
    "href": "quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ 5.3190296 ,  2.29060701,  7.67724541,  3.24270303,  1.81220849,\n       -0.96964699, -2.83190463, -4.77919688, -2.45771875, -0.65100311])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.07068785, 0.0994727 , 0.03642663, 0.09503666, 0.09962572,\n       0.07571186, 0.04808254, 0.02372075, 0.05359982, 0.08007015])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n16.7 μs ± 6.91 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n3.25 μs ± 1.05 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n108 μs ± 18.9 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(n_doors, n_trials):\n    doors = np.arange(1, n_doors + 1)\n    prize = np.random.choice(doors, size=n_trials)\n    player = np.random.choice(doors, size=n_trials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(n_trials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(n_trials)])\n    return {'noswitch': np.sum(prize == player),\n               'switch': np.sum(prize == player2)}\n\nTest it out with 3 doors.\n\nmontyhall(3, 1000)\n\n{'noswitch': np.int64(314), 'switch': np.int64(686)}\n\n\nThen with 4 doors\n\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(246), 'switch': np.int64(362)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.\n\n4.4.1.1 Faster version\nThis one avoid loops.\n\nimport numpy as np\nfrom typing import Dict, Optional\n\ndef montyhall_fast(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Run a Monty Hall simulation with `n_doors` doors and `n_trials` repetitions.\n    The host always opens exactly one empty door that is neither the prize door\n    nor the player's initial choice.\n\n    Parameters\n    ----------\n    n_doors : int\n        Total number of doors in the game (must be &gt;= 3).\n    n_trials : int\n        Number of independent trials to simulate.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    Dict[str, int]\n        A dictionary with counts of wins under two strategies:\n        - 'noswitch': staying with the initial choice\n        - 'switch'  : switching after the host reveals one empty door\n\n    Examples\n    --------\n    &gt;&gt;&gt; results = montyhall_fast(3, 100000, seed=42)\n    &gt;&gt;&gt; results\n    {'noswitch': 33302, 'switch': 66698}\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initial random assignments\n    prize = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n    player = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n\n    host = np.empty(n_trials, dtype=np.int32)\n\n    # Case 1: player == prize → host excludes only 'player'\n    same = prize == player\n    if np.any(same):\n        k = rng.integers(n_doors - 1, size=np.sum(same), dtype=np.int32)\n        p = player[same]\n        host[same] = k + (k &gt;= p)\n\n    # Case 2: player != prize → host excludes 'player' and 'prize'\n    diff = ~same\n    if np.any(diff):\n        a = np.minimum(player[diff], prize[diff])\n        b = np.maximum(player[diff], prize[diff])\n        k = rng.integers(n_doors - 2, size=np.sum(diff), dtype=np.int32)\n        host[diff] = k + (k &gt;= a) + (k &gt;= b)\n\n    # Player switches: exclude 'player' and 'host'\n    a2 = np.minimum(player, host)\n    b2 = np.maximum(player, host)\n    k2 = rng.integers(n_doors - 2, size=n_trials, dtype=np.int32)\n    player2 = k2 + (k2 &gt;= a2) + (k2 &gt;= b2)\n\n    return {\n        \"noswitch\": int((prize == player).sum()),\n        \"switch\": int((prize == player2).sum()),\n    }\n\nAnother faster version uses Numba just-in-time (JIT) compiler Python, which translates a subset of Python and Numpy into fast machine code via LLVM at runtime. A decorator is added to the numeric functions and, after a one-time compile on the first call (“warm-up”), later calls run much faster. One can toggle parallel=True for multi-core speedups.\n\nfrom typing import Dict, Optional\nimport numpy as np\nfrom numba import njit, prange\n\n# --- internal helpers (compiled) ---\n\n@njit\ndef _seed_numba(seed: int) -&gt; None:\n    # Seed the RNG used inside Numba-compiled code\n    np.random.seed(seed)\n\n@njit\ndef _trial_once(n_doors: int) -&gt; (int, int):\n    \"\"\"\n    Run one Monty Hall trial (host opens exactly one empty door).\n    Returns (noswitch_win, switch_win) as 0/1 ints.\n    \"\"\"\n    prize  = np.random.randint(0, n_doors)\n    player = np.random.randint(0, n_doors)\n\n    # host picks an empty door not equal to prize or player\n    if player == prize:\n        # exclude only 'player' (size n_doors-1)\n        k = np.random.randint(0, n_doors - 1)\n        host = k + (1 if k &gt;= player else 0)\n    else:\n        # exclude 'player' and 'prize' (size n_doors-2)\n        a = player if player &lt; prize else prize\n        b = prize if prize &gt; player else player\n        k = np.random.randint(0, n_doors - 2)\n        host = k + (1 if k &gt;= a else 0) + (1 if k &gt;= b else 0)\n\n    # player switches: choose uniformly from doors != player and != host\n    a2 = player if player &lt; host else host\n    b2 = host if host &gt; player else player\n    k2 = np.random.randint(0, n_doors - 2)\n    player2 = k2 + (1 if k2 &gt;= a2 else 0) + (1 if k2 &gt;= b2 else 0)\n\n    noswitch_win = 1 if prize == player  else 0\n    switch_win   = 1 if prize == player2 else 0\n    return noswitch_win, switch_win\n\n@njit(parallel=True)\ndef _run_parallel(n_doors: int, n_trials: int) -&gt; (int, int):\n    ns = np.zeros(n_trials, dtype=np.int64)\n    sw = np.zeros(n_trials, dtype=np.int64)\n    for i in prange(n_trials):\n        a, b = _trial_once(n_doors)\n        ns[i] = a\n        sw[i] = b\n    return int(ns.sum()), int(sw.sum())\n\n@njit\ndef _run_serial(n_doors: int, n_trials: int) -&gt; (int, int):\n    noswitch = 0\n    switch   = 0\n    for _ in range(n_trials):\n        a, b = _trial_once(n_doors)\n        noswitch += a\n        switch   += b\n    return noswitch, switch\n\n# --- public API ---\n\ndef montyhall_numba(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None,\n    parallel: bool = True,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Monty Hall (host opens one empty door) using Numba-compiled loops.\n\n    Parameters\n    ----------\n    n_doors : int\n        Number of doors (&gt;= 3).\n    n_trials : int\n        Number of trials to simulate.\n    seed : int, optional\n        Seed for reproducibility.\n    parallel : bool, default True\n        Use a parallel loop over trials (multi-core).\n\n    Returns\n    -------\n    Dict[str, int]\n        {'noswitch': ..., 'switch': ...}\n    \"\"\"\n    if n_doors &lt; 3:\n        raise ValueError(\"n_doors must be &gt;= 3\")\n\n    if seed is not None:\n        _seed_numba(int(seed))  # seed the compiled RNG\n\n    ns, sw = (_run_parallel(n_doors, n_trials)\n              if parallel else\n              _run_serial(n_doors, n_trials))\n    return {\"noswitch\": ns, \"switch\": sw}\n\nIt wins when n_trials is very large (e.g., 10–100M) where loop overhead is amortized and you give it many threads, or the design avoids massive temporaries (not much in this example).\nLet’s see their time comparison.\n\nimport timeit\n\n# --- Timing function ---\ndef benchmark(n_doors: int = 3, n_trials: int = 1_000_00, seed: int = 42) -&gt; None:\n    print(f\"n_doors={n_doors}, n_trials={n_trials}\")\n    # Ensure deterministic where possible\n    np.random.seed(seed)\n\n    # Time original\n    t_orig = min(timeit.repeat(lambda: montyhall(n_doors, n_trials),\n                               repeat=3, number=1))\n    print(f\"Original (lists): {t_orig:.4f}s\")\n\n    # Time vectorized\n    t_vec = min(timeit.repeat(lambda: montyhall_fast(n_doors, n_trials, seed),\n                              repeat=3, number=1))\n    print(f\"Vectorized NumPy: {t_vec:.4f}s\")\n\n    # Time Numba serial (compile excluded by earlier warm-up)\n    t_numba_ser = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=False), repeat=3, number=1))\n    print(f\"Numba serial:     {t_numba_ser:.4f}s\")\n\n    # Time Numba parallel\n    t_numba_par = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=True), repeat=3, number=1))\n    print(f\"Numba parallel:   {t_numba_par:.4f}s\")\n\n\nbenchmark(n_doors = 4, n_trials = 1000_000)\n\nn_doors=4, n_trials=1000000\nOriginal (lists): 40.8824s\nVectorized NumPy: 0.0825s\nNumba serial:     0.0631s\n\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nNumba parallel:   0.0304s",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4608990784\n4608990784\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4457466640\n4545710960\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#sec-python-venv",
    "href": "python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "5  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Positron, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a HTML.\nPrint the HTML file to a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhy is the command line interface if preferred among the professionals?\nWhat are the advantages of Linux over Windows?\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\nWhy do I require at least 10 commits for each assignment?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\nExplain in words which strategy is preferred. Use analytic results to help if you could derive them.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\nTest your function with \\((1, 3, 9, 10)\\), \\((4, 4, 10, 10)\\), \\((1, 5, 5, 5)\\), \\((3, 3, 7, 7)\\), \\((3, 3, 8, 8)\\), and $(1, 4, 5, 6).\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Street flood in NYC The class presentation at the 2025 NYC Open Data Weeik is scheduled for Monday, March 24, 2:00-3:00 pm.\nThe NYC Open Data of 311 Service Requests contains all service requests from 2010 to the present. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets. SF complaints are typically filed when residents observe standing water or flooding, whereas CB complaints report clogged basins, defective grates, or other drainage problems. The dataset is available in CSV format as data/nycflood2024.csv. Refer to the online data dictionary for a detailed explanation of variable meanings. Try to tell a story in your report while going through the questions.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Can some of the missing values be filled? Fill them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second.\nSummarize your suggestions to the data curator in several bullet points.\n\nExploratory analysis.\n\nVisualize the locations of complaints on a NYC map, with different symbols for different descriptors.\nCreate a variable response_time, which is the duration from created_date to closed_date.\nVisualize the comparison of response time by complaint descriptor and borough. The original may not be the best given the long tail or outlers.\nIs there significant difference in response time between SF and CB complaints? Across different boroughs? Does the difference between SF and CB depend on borough? State your hypothesis, justify your test, and summarize your results in plain English.\nCreate a binary variable over3d to indicate that a service request took three days or longer to close.\nDoes over3d depend on the complaint descriptor, borough, or weekday (vs weekend/holiday)? State your hypotheses, justify your test, and summarize your results.\n\nModeling the occurrence of overly long response time.\n\nCreate a data set which contains the outcome variable over3d and variables that might be useful in predicting it. Consider including time-of-day effects (e.g., rush hour vs. late-night), seasonal trends, and neighborhood-level demographics. Zip code level information could be useful too, such as the zip code area and the ACS 2023 variables (data/nyc_zip_areas.feather and data/acs2023.feather).\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over3d for the complaints with the training data. If you have tuning parameters, justify how they were selected.\nConstruct the confusion matrix from your prediction with a threshold of 1/2 on both training and testing data. Explain your accuracy, recall, precision, and F1 score to a New Yorker.\nConstruct the ROC curve of your fitted logistic model and obtain the AUROC for both training and testing data. Explain your results to a New Yorker.\nIdentify the most important predictors of over3d. Use model coefficients or feature importance (e.g., odds ratios, standardized coefficients, or SHAP values).\nSummarize your results to a New Yorker who is not data science savvy in several bullet points.\n\nModeling the count of SF complains by zip code.\n\nCreate a data set by aggregate the count of SF and SB complains by day for each zipcode.\nMerge the NYC precipitation (data/rainfall_CP.csv), by day to this data set.\nMerge the NYC zip code level landscape variables (data/nyc_zip_lands.csv) and ACS 2023 variables into the data set.\nFor each day, create two variables representing 1-day lag of the precipitation and the number of CB complaints.\nFilter data from March 1 to November 30, excluding winter months when flooding is less frequent. November 30.\nCompare a Poisson regression with a Negative Binomial regression to account for overdispersion. Which model fits better? Explain the results to a New Yorker.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical\npractice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]