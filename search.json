[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book is free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-f25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Spring 2025, Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of useful data challenges:\n\nASA Data Challenge Expo: big data in 2025\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2026.\nA good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAgostino, Michael Angelo\n\nFurther understanding of github and improve my workflow.\nBuild a project in order to gain real world data science experience.\n\nBlake, Roger Cooley\n\nGain experience using git and github\nComplete a project that transforms real-world data into valuable insights\nMaster control over my computer and its file system\n\nCao, Enshuo\nChen, Irene\n\nBecome proficient in using Git and Python\nDeepen machine learning and data science skills\n\nChen, Jingang Calvin\n\ngain exposure and hands on experience to machine learning models\nbecome proficient in Git and Github\napply statistical and data analysis skills to a data science project\n\nFazzina, Sophia Carmen\n\nGet comfortable using Git and GitHub\nWork on a data science project from start to finish\nGet a good grade in this class\n\nHaerter, Alejandro Erik\n\nUnderstand proper workflow principles via GIt\nFind an intersection between data science and economics\nLearn efficient use of AI for trivial tasks\n\nLawrence, Claire Elise\nLevine, Hannah Maya\n\nBecome more comfortable using Git/GitHub\nImprove Python/style skills\nGain experience with real world projects\n\nLucey, Sonia Niamh\n\nLearn practical applications of Data Science and Economics\nLearn how to use Git/GitHub\nImprove Python/coding skills\n\nMayer-Costa, Jaden Paulo\n\nGain hands on experience working with real-world data\nBecome proficient in using Git and Github.\nContinue adding to python skills and using code editing software.\n\nMilun, Lewis Aaron\n\nBecome proficient in Git\nLearn more about the processes involved in Data Science\n\nMontalvo, Victor Samuel\nPatel, Sahil Sanjay\n\nI want learn more about time series forecasting\nI want to be more comfortable using git and GitHub\nI want to learn about the applications of Data Science in Finance\n\nPatel, Tulsi Manankumar\nPerkins, Jack Thomas\n\nBe able to incorporate git into my own workflow.\nUse python to boost efficiency in data problems.\nBecome more comfortable with python for data science.\n\nSaltus, Quinn Lloyd Turner\n\nGain proficiency in data visualization with Python\nBuild experience using version control to expand the scope of my projects\nLearn when and how to apply libraries (such as numpy & pytorch) to improve my code’s performance\nBecome familiar with machine learning tools and techniques\n\nSaxena, Aanya\nSchlessel, Jacob E\n\nLearn about different classification algorithms\nPractice using python for analyzing data\nBecome comfortable with Git and Github\n\nSgro, Owen Bentley\n\nI want to become better acquainted with command line interface and using the terminal.\nI want more experience with creating my own projects and finding my own way of creating things.\nI want to further my experience with python (and any other languages we may use) and be able to apply that to other classes.\n\nTang, Wilson Chen\n\nI want to be very comfortable with using GitHub and GitBash\nI want to learn how to have a clear style\nI want to use programming tools in a professional way\n\nTran, Justin\n\nTo become proficient in the knowledge of Git.\nTo adopt command line knowledge into the workforce.\nFostering good practices with commits.\n\nWhite, Abigail Lynn\n\nBecome more comfortable working with GitHub.\nLearn and memorize more commands used in the Terminal.\nAdvance my statistical skills through a data science project.\n\nWishneski, Emma Irene\n\nGain a better understanding of data science, and real life applications\nGet comfortable using github, python, and other applications\nBecome familiar with useful techniques for machine learning\n\nYoon, Jessica Nari\n\nBecome comfortable with Git and version control.\n\nApply skills from this classes to other courses.\nUnderstand my computer better.\n\nZhang, Mark Justin\n\nGet comfortable with command line and Git\nLearn to make my own data science projects\nlearn theory and application of ML algorithms\n\n\n\n\nStudents in 5255\n\nAnzalone, Matthew James\n\nBuild professional workflow habits for a data-science career\n\nIncrease my Python knowledge enough that I could eventually create usable packages\n\nImprove my data-driven thinking outside of the bounds of economics\n\nGomez-Haibach, Konrad\nPlotnikov, Alexander\n\nGain working knowledge of Git and project management.\nLearn AI and Machine Learning algorithms.\nApply skillset by working on different projects.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(2819 + 4075 + 6227 + 5139 + 4768 + 109)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Chen,Jingang Calvin',\n 'Montalvo,Victor Samuel',\n 'Cao,Enshuo',\n 'Chen,Irene',\n 'Haerter,Alejandro Erik',\n 'Saxena,Aanya',\n 'Gomez-Haibach,Konrad',\n 'Sgro,Owen Bentley',\n 'Lucey,Sonia Niamh',\n 'Yoon,Jessica Nari',\n 'Patel,Sahil Sanjay',\n 'Plotnikov,Alexander',\n 'Milun,Lewis Aaron',\n 'Patel,Tulsi Manankumar',\n 'Perkins,Jack Thomas',\n 'Fazzina,Sophia Carmen',\n 'Tang,Wilson Chen',\n 'Wishneski,Emma Irene',\n 'Lawrence,Claire Elise',\n 'White,Abigail Lynn',\n 'Mayer-Costa,Jaden Paulo',\n 'Anzalone,Matthew James',\n 'Zhang,Mark Justin',\n 'Saltus,Quinn Lloyd Turner',\n 'Tran,Justin',\n 'Levine,Hannah Maya',\n 'Blake,Roger Cooley',\n 'Schlessel,Jacob E',\n 'Agostino,Michael Angelo']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proportions and rates\nDatabase operation with Structured query language (SQL)\nGrammar of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map with gmplot\nAnimation\nSupport vector machine\nRandom forest\nGradient boosting machine\nNaive Bayes\nNeural networks basics\nMLP/ANN/CNN/RNN/LSTM\nUniform manifold approximation and projection\nAutomatic differentiation\nDeep learning\nTensorFlow\nAutoencoders\nK-means clustering\nPrincipal component analysis\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\nMaking presentations with Quarto\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/18\nChen, Jingang Calvin\nSyntax of Markdown\n\n\n09/23\nMontalvo, Victor Samuel\n\n\n\n09/25\nCao, Enshuo\n\n\n\n09/30\nLucey, Sonia Niamh\nGrammar of Graphics with Plotnine\n\n\n10/02\nHaerter, Alejandro Erik\nGeospatial Data with GeoPandas\n\n\n10/02\nSaxena, Aanya\n\n\n\n10/07\nGomez-Haibach, Konrad\n\n\n\n10/07\nSgro, Owen Bentley\nData Manipulation with Pandas\n\n\n10/09\nChen, Irene\nPresentations with Quarto\n\n\n10/09\nYoon, Jessica Nari\nRandom Forest\n\n\n10/14\nPatel, Sahil Sanjay\n\n\n\n10/14\nPlotnikov, Alexander\nRandom Forests\n\n\n10/16\nMilun, Lewis Aaron\nDatabase operation with Structured query language (SQL)\n\n\n10/16\nPatel, Tulsi Manankumar\n\n\n\n10/23\nPerkins, Jack Thomas\n\n\n\n10/23\nFazzina, Sophia Carmen\n\n\n\n10/28\nTang, Wilson Chen\nHow to call R in Python\n\n\n10/28\nWishneski, Emma Irene\n\n\n\n10/30\nLawrence, Claire Elise\n\n\n\n11/04\nWhite, Abigail Lynn\nEffective data science communication\n\n\n11/04\nMayer-Costa, Jaden Paulo\n\n\n\n11/06\nAnzalone, Matthew James\n\n\n\n11/06\nZhang, Mark Justin\n\n\n\n11/11\nSaltus, Quinn Lloyd Turner\nSpatial Statistical Methods\n\n\n11/11\nTran, Justin\n\n\n\n11/11\nLevine, Hannah Maya\n\n\n\n11/13\nBlake, Roger Cooley\n\n\n\n11/13\nSchlessel, Jacob E\n\n\n\n11/13\nAgostino, Michael Angelo\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is available under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nChen, Jingang Calvin; Montalvo, Victor Samuel; Cao, Enshuo; Lucey, Sonia Niamh; Haerter, Alejandro Erik\n\n\n11/20\nSaxena, Aanya; Sgro, Owen Bentley; ; Chen, Irene; Yoon, Jessica Nari; Patel, Sahil Sanjay\n\n\n12/02\nMilun, Lewis Aaron; Patel, Tulsi Manankumar; Perkins, Jack Thomas; Fazzina, Sophia Carmen; Tang, Wilson Chen\n\n\n12/04\nWishneski, Emma Irene; Lawrence, Claire Elise; White, Abigail Lynn; Mayer-Costa, Jaden Paulo; Zhang, Mark Justin\n\n\n12/???\nSaltus, Quinn Lloyd Turner; Tran, Justin; Levine, Hannah Maya; Blake, Roger Cooley; Schlessel, Jacob E; Agostino, Michael Angelo\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file _mysection.qmd as an example, which is includeed in index.qmd. See also how _ethics.qmd is included into intro.qmd for example.\nTips on making contributions:\n\nNo plagiarism.\nAvoid external graphics.\nUse simulated data or data in Python packages.\nUse data from homework assignments.\nCite article/book references (learn how from our sources).\nInclude a subsection of Further Readings.\nWatch for header levels.\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance for feedbacks.\n\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?  \nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nIf you need a reference letter, how could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-is-data-science",
    "href": "01-intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denote computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#expectations-from-this-course",
    "href": "01-intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#computing-environment",
    "href": "01-intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have that users previously faced before WSL was introduced.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump-start, here is a tutorial: Ubuntu Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding style against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McKinney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-ethics",
    "href": "01-intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) (American Statistical Association (ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n\nAmerican Statistical Association (ASA). (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-git.html",
    "href": "02-git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#set-up-gitgithub",
    "href": "02-git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#most-frequently-used-git-commands",
    "href": "02-git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#tips-on-using-git",
    "href": "02-git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#pull-request",
    "href": "02-git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "03-quarto.html",
    "href": "03-quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#introduction-to-quarto",
    "href": "03-quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#sec-buildnotes",
    "href": "03-quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#the-data-science-life-cycle",
    "href": "03-quarto.html#the-data-science-life-cycle",
    "title": "3  Reproducible Data Science",
    "section": "3.3 The Data Science Life Cycle",
    "text": "3.3 The Data Science Life Cycle\nThis section summarizes Chapter 2 of Veridical Data Science (Yu & Barter, 2024), which introduces the data science life cycle (DSLC). The DSLC provides a structured way to think about the progression of data science projects. It consists of six stages, each with a distinct purpose:\n\nStage 1: Problem formulation and data collection\nCollaborate with domain experts to refine vague questions into ones that can realistically be answered with data. Identify what data already exists or design new collection protocols. Understanding the collection process is crucial for assessing how data relates to reality.\nStage 2: Data cleaning, preprocessing, and exploratory data analysis\nClean data to make it tidy, unambiguous, and correctly formatted. Preprocess it to meet the requirements of specific algorithms, such as handling missing values or scaling variables. Exploratory data analysis (EDA) summarizes patterns using tables, statistics, and plots, while explanatory data analysis polishes visuals for communication.\nStage 3: Exploring intrinsic data structures (optional)\nTechniques such as dimensionality reduction simplify data into lower-dimensional forms, while clustering identifies natural groupings among observations. Even if not central to the project, these methods often enhance understanding.\nStage 4: Predictive and/or inferential analysis (optional)\nMany projects are cast as prediction tasks, training algorithms like regression or random forests to forecast outcomes. Inference focuses on estimating population parameters and quantifying uncertainty. This book emphasizes prediction while acknowledging inference as important in many domains.\nStage 5: Evaluation of results\nFindings should be evaluated both qualitatively, through critical thinking, and quantitatively, through the PCS framework. PCS stands for predictability, computability, and stability:\n\nPredictability asks whether findings hold up in relevant future data.\n\nComputability asks whether methods are feasible with available computational resources.\n\nStability asks whether conclusions remain consistent under reasonable changes in data, methods, or judgment calls.\nTogether, PCS provides a foundation for assessing the reliability of data-driven results.\n\nStage 6: Communication of results\nResults must be conveyed clearly to intended audiences, whether through reports, presentations, visualizations, or deployable tools. Communication should be tailored so findings can inform real-world decisions.\n\nThe DSLC is not a linear pipeline—analysts often loop back to refine earlier steps. The chapter also cautions against data snooping, where patterns discovered during exploration are mistaken for reliable truths. Applying PCS ensures that results are not only technically sound but also trustworthy and interpretable across the life cycle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#a-primer-of-markdown",
    "href": "03-quarto.html#a-primer-of-markdown",
    "title": "3  Reproducible Data Science",
    "section": "3.4 A Primer of Markdown",
    "text": "3.4 A Primer of Markdown\nThis section was prepared by Jingang Chen, an undergraduate junior pursuing a dual degree in computer science and statistical data science.\n\n3.4.1 Introduction\nThis section will focus on the syntax of Markdown, which is a lightweight markup language that allows to user to write content in plain text format, which can be rendered to various formats like HTML and PDF, and is widely used in open-source documentation.\n\n\n3.4.2 Headers\nIn markdown, creating heading levels for sections and subsections are denoted by # (atx-style) in the beginning of the line. The number of hashtags denote the heading level, with more hashtags indicating smaller heading levels. There are a total of 6 heading levels using the hashtags.\n# Header 1 (Main)\n## Header 2 (Subheading)\n### Header 3 (Subheading)\n#### Header 4\n##### Header 5\n###### Header 6\n\nA space is required after the hashtags to denote that it is a heading\n\nHeadings can also be denoted by using underlined = and - signs, though this will only work for the first and second level headers respectively.\nHeader 1\n===\nHeader 2\n----\nAny amount of = and - signs will work to create those top two headings.\n\n\n3.4.3 Paragraph and Line Break Convention\nTo seperate text into paragraphs, make sure there is at least one blank space between the blocks of texts.\nThis is the first pargraph which can contain multiple lines, and as long there\nis no blank line in this, then this block of text is one paragraph.\n\nThis is an example of a second paragraph. This one is seperated by a blank line\nfrom the paragraph above to denote a different paragraph.\nOutput:\nThis is the first pargraph which can contain multiple lines, and as long there is no blank line in this, then this block of text is one paragraph.\nThis is an example of a second paragraph. This one is seperated by a blank line from the paragraph above to denote a different paragraph.\nTo create a line break within a paragraph, end whatever line you’re on with two spaces, then press enter/return to start a new line. The break tag &lt;br&gt; is also sufficient.\nThis is one line.  \nThis is a line that is seperated from the line above using two spaces. &lt;br&gt;\nThis is a third line seperated using the `&lt;br&gt;` break tag.\nOutput:\nThis is one line.\nThis is a line that is seperated from the line above using two spaces.  This is a third line seperated using the &lt;br&gt; break tag.\n\n&lt;&gt; in Markdown is a HTML tag, which is another way to structure the document\n\n\n\n3.4.4 Horizontal Rules\nHorizontal rules visually separate document sections, which can be done in Markdown by adding 3 or more of one of 3 characters: ***, ---, or ___.\n\nparagraph 1.\n\n******\n\nparagraph 2.\n\n------\n\nparagraph 3.\n\n______\n\nparagraph 4.\nOutput:\nparagraph 1.\n\nparagraph 2.\n\nparagraph 3.\n\nparagraph 4.\n\nMake sure when using the horizontal rulers, each of them is one blank line above and below them. There shouldn’t be any text above or below them adjacently.\n\n\n\n3.4.5 Text Formatting\n\n3.4.5.1 Bolding and Italics\nItalic text, uses single asterisks (*) or underscores (_) around the text, while bold text uses double asterisks or underscores. To make the text both bolded and italicized, put three asterisks or underscores around the text.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\n*Italicized*  _Italicized_\nItalicized\n\n\n**Bolded**  __Bolded__\nBolded\n\n\n***bold & italics***  ___bold & italics___\nbolded & italics\n\n\n\nEmphasis can also be placed within a word as well, but only * can be used, not _.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\ns*uperfragalis*t ex**pialidociou**s\nsuperfragalist expialidocious\n\n\n\n\n\n3.4.5.2 Strikethrough\nStrikethrough uses ~~ around the text to cross it out.\n\n\n\nSyntax\nOutput\n\n\n\n\n~~strikethrough text~~\nstrikethrough text\n\n\n\n\n\n3.4.5.3 Superscript and Subscript\nTo superscript, use ^ around the desired text.\nFor subscript, use ~ around the desired text.\n\n\n\nSyntax\nOutput\n\n\n\n\nsuperscript^2^\nsuperscript2\n\n\nsubscript~2~\nsubscript2\n\n\n\n\n\n3.4.5.4 Underlining and Highlighting\nTo underline, bracket the desired text with [], and then follow that using {.underline}.\nTo highlight, bracket the desired text with [], and follow that using {.mark}. Alternatively you can start the text with &lt;mark&gt; and end it with &lt;/mark&gt;.\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\n[underlined text]{.underline}\nunderlined text\n\n\n[highlighted]{.mark}  &lt;mark&gt;higlighted&lt;/mark&gt;\nhighlighted\n\n\n\n\n\n3.4.5.5 Escape Characters\nIf you want to display the Markdown syntax characters, it can be done by putting \\ before and after the text.\n\\# Not a heading\\\n\\**not bolded**\\\n\\[Not a link]\\\nOutput:\n# Not a heading\n**not bolded**\n[Not a link]\n\n\n\n\n3.4.6 Blockquotes\nBlockquotes is a way to highlight quoted content or important information in the document, which is denoted by &gt; in the beginning of the line.\n&gt; This is a block quote\n&gt; \n&gt; second block quote\n&gt;\n&gt; third block quote\nOutput:\n\nThis is a blockquote\nsecond blockquote\nthird blockquote\n\nTo make sure that the blockquotes are seperated, make sure that each quote is seperated by &gt;’s with no text in that line. Putting blockquotes adjacent to each other will result in all the text being in the same paragraph.\n&gt; These blockquotes\n&gt; are not seperated and\n&gt; are all in one line\nOutput:\n\nThese blockquotes are not seperated and are all in one line\n\nBlockquotes can also be nested by using multiple &gt; in one line.\n&gt; This is a blockquote\n&gt;\n&gt; &gt; This is a nested blockquote\n&gt; &gt;\n&gt; &gt; &gt; Third level blockquote\nOutput:\n\nThis is a blockquote\n\nThis is a nested blockquote\n\nThird level blockquote\n\n\n\n\n\n3.4.7 Lists\nLists and nested lists can be structured in Markdown either unordered or ordered. For nested lists, make sure to include 4 spaces to properly indent the nested list (applies to both ordered and unordered).\n\n3.4.7.1 Unordered Lists\nFor unordered lists, *, +, or - can be used to make a list.\n* Item 1\n    * Subitem\n        * Another subitem\n* Item 2\n* Item 3\nand\n+ Item 1\n   + Subitem\n       + Another subitem\n+ Item 2\n+ Item 3\n- Item 1\n   - Subitem\n       - Another subitem\n- Item 2\n- Item 3\nas well a mix of all three:\n* Item 1\n   + Subitem\n       - Another subitem\n* Item 2\n* Item 3\nall yield the same output:\n\nItem 1\n\nSubitem\n\nAnother subitem\n\n\nItem 2\nItem 3\n\n\n\n3.4.7.2 Ordered Lists\nOrdered lists use numbers with periods.\n1. Item 1\n2. Item 2\n    1. Sub item\n3. Item 3\nThe numbers don’t necessarily have to be ordered, and they can be duplicated as well. Whatever number the list starts on will be the one that it will count from no matter the numbers that come after it.\n1. Item 1\n1. Item 2\n    1. Sub Item\n1. Item 3\n1. Item 1\n4. Item 2\n    2. Sub Item\n7. Item 3\nAll three of these lists will yield the same result:\n\nItem 1\nItem 2\n\nSub Item\n\nItem 3\n\nHowever, if the list were to start on 2, it would start counting from 2 no matter the order that follows.\n2. Item 1\n2. Item 2\n5. Item 3\nOutput:\n\nItem 1\nItem 2\nItem 3\n\n\n\n3.4.7.3 Task List\nTo denote the lists as a series of tasks, use - [ ], which is unchecked, and [x], which is checked, at the beginning of the line.\n- [ ] Task 1\n- [x] Task 2\nOutput:\n\nTask 1\nTask 2\n\n\n\n3.4.7.4 Definition lists\nDefintion lists can be created with the following convention:\nterm\n: defintion\n\nterm2\n: definition2\nOutput:\n\nterm\n\ndefintion\n\nterm2\n\ndefinition2\n\n\n\n\n3.4.7.5 Some Additional Notes About Lists\nThere are some additional features that can be done with the lists mentioned.\nA list can continue after a break in between. For ordered lists, the numbering still follows through after an interruption.\n1. Item 1\n\ninterruption text\n\n2. Item 2\nOutput:\n\nItem 1\n\ninterruption text\n\nItem 2\n\nText than isn’t numbered or in bullet points can also be added below list using four spaces for indenting. Code chunks can be added in this case as well.\n1. ordered list\n2. item 2\n    continued after indenting 4 spaces\n    ```python\n    print(\"Hello, World!\")\n    ```\n    A. sub-sub-item 1\nOutput:\n\nordered list\nitem 2\ncontinued after indenting 4 spaces\nprint(\"Hello, World!\")\n\nsub-sub-item 1 ### Code\n\n\n\n\n3.4.7.6 Inline Code\nMarkdown allows for inline code and code blocks.\nTo insert inline code, use the backticks ` around the text\nThis is an example of `inline code` in a text.\nOutput: This is an example of inline code in a text.\nTo display the ` as part of an inline code, surround the character with `` backticks and spacing them apart from `.\n\n\n3.4.7.7 Code Blocks\nTo create code blocks, ``` can be used.\n    ```\n    some code\n    ```\nOutput:\nsome code\nAlternatively, code blocks can be indented by identing four or more spaces prior to the code.\nThis is code using the indentation of four spaces\n    This line has more than four spaces\nA language can also be added to specify the language of the code blocks if ``` is used.\n```python\nprint('some python code')\n```\nOutput:\nprint('some python code')\nTo make the code executable, put {} around the syntax language being used in the code blocks.\n    ```{python}\n    print(\"Some python code\")\n    ```\nOutput:\n\nprint('some python code')\n\nsome python code\n\n\n\n\n\n3.4.8 Formulas and Equations\nMarkdown supports LaTeX-style expressions. Mathematical expressions can either be done inline (enclosing using $) or for display math (enclosed by $$).\n\n\n\n\n\n\n\nSyntax\nOutput\n\n\n\n\nInline: $x^2 + y^2 = z^2$\nInline: \\(x^2 + y^2 = z^2\\)\n\n\nDisplay:  $$x^2 + y^2 = z^2$$\nDisplay:  \\[x^2 + y^2 = z^2 \\tag{3.1}\\]\n\n\n\nFor mor information on how to use LaTeX expressions, visit https://www.overleaf.com/learn and look under the “Mathematics” section.\n\n\n3.4.9 Link Embedding\nLinks in Markdown can either be added inline or as a reference.\nTo add an inline link, use [] around the text that leads to the link, followed by actually inputting the link in ().\nthis is an example link that will lead to the \n[Markdown Guide](https://www.markdownguide.org/)\nOutput: : This is an example link that will lead to the Markdown Guide.\nAlternatively, for inline links, the URL can be directly added without linking it to text by just using &lt;&gt; around the URL.\nLink to &lt;https://www.markdownguide.org/&gt;.\nOutput: Link to https://www.markdownguide.org/.\nFor reference-style links, you can having text linked to the URL and a seperate number or text that points to the link with []\nExample of reference-style link leading to the [Markdown Guide][link].\n\n[link]: https://www.markdownguide.org\nOutput:\nExample of reference-style link leading to the Markdown Guide.\n\n\n3.4.10 Images\nTo insert an image into markdown, the convention is to first add !, followed by a caption to the image wrapped in [], and finally the file path or URL to the image enclosed with ().\nIn addition, the image can be embedded with a link by first wrapping the 3 parameters mentioned above in [], followed by the link wrapped in (). The link can either be a local file path stored on your computer or a direct link of the image found online. For reproducibility, this example uses a direct URL pointing to an online image.\n[![UConn Husky Logo](https://lofrev.net/wp-content/photos/2016/06/uconn_huskies_logo.jpg)](https://uconn.edu/)\nOutput:\n\n\n\n\n\n\nFigure 3.1: UConn Husky Logo\n\n\n\n\n\n3.4.11 Tables\nTo create tables, the | character is used to seperate the table into columns, while the - is used to seperate the headers of the table from the rest of the data. After the table, a header can be included two lines below the table starting with :.\n| Col 1 | Col 2 | Col 3 | Col 4 |\n|------|-----|----------|-------|\n|   a  |  b  |    c     |  d    |\n|  e   |  f  |  g       |  h    |\n|   i  |   j |    k     |    l  |\n\n: Sample Table 1\nOutput:\n\n\n\nTable 3.1: Sample Table 1\n\n\n\n\n\nCol 1\nCol 2\nCol 3\nCol 4\n\n\n\n\na\nb\nc\nd\n\n\ne\nf\ng\nh\n\n\ni\nj\nk\nl\n\n\n\n\n\n\nColumns can also be aligned to the left, right or center by add a colong : to the left, right or on both sides of the -’s of the table.\n| Right Col | Left Col | Center | Default |\n|----------:|:---------|:------:|---------|\n|        a  |       b  |    c   |    d    |\n|  e        |       f  |   g    |    h    |\n|   i       |        j |  k     |      l  |\n\n: Sample Table 2\nOutput:\n\nSample Table 2\n\n\nRight Col\nLeft Col\nCenter\nDefault\n\n\n\n\na\nb\nc\nd\n\n\ne\nf\ng\nh\n\n\ni\nj\nk\nl\n\n\n\n\n\n3.4.12 Cross Referencing\nPrior to cross referencing a section, there must be a label attached to the section that is being referenced. This is done by using {} after the section and giving it a label inside of it starting with #sec- followed by anything else after it.\n### Tables {#sec-tables}\nThen, to reference the section, use the @ followed by the label specified to create a direct link to the section. Optionally, you can wrap it around [] and add any additional text to the link.\nRefer back to the Tables section in [section @sec-tables].\nOutput:\nRefer back to the Tables section in section 3.4.11.\nTo refer to figures and images, make sure to input the label after the URL. Make sure in the label to start with #fig- to specify that it is a figure that is being referenced.\n[![UConn Husky Logo](https://lofrev.net/wp-content/photos/2016/06/uconn_huskies_logo.jpg){#fig-sample}](https://uconn.edu/)\nNow it can be referred back with the same convention as referencing a section:\nThis refers @fig-sample in the Images section.\nOutput:\nThis refers Figure 3.1 in the Images section.\nTo cross reference tables, include the label on the line where the header is specified. Make sure to start the label with #tbl- to specify that it’s a table that is being referenced.\n| Col 1 | Col 2 | Col 3 | Col 4 |\n|------|-----|----------|-------|\n|   a  |  b  |    c     |  d    |\n|  e   |  f  |  g       |  h    |\n|   i  |   j |    k     |    l  |\n\n: Sample Table 1 {#tbl-sample1}\nNow to reference the table back:\n@tbl-sample1 refers back to the first table in the Tables section.\nOutput:\nTable 3.1 refers back to the first table in the Tables section.\nFinally, to reference an equation, make sure to include the label after the equation, outside of the $$ starting with #eq-.\n$$x^2 + y^2 = z^2$$ {#eq-sample}\nNow to reference the equation from the Equations section back:\n@eq-sample is the Pythagorean theorem referenced from the Equations section.\nOutput:\nEquation 3.1 is the Pythagorean theorem referenced from the Equations section.\n\n\n3.4.13 Footnotes\nFootnotes, often denoted by superscripts, are placed at the bottom of a page in a document, which helps provide additional information and references related to a specific part of the text. To insert a footnote in a text, it is done in the [], where the first character inside of the brackets is ^, followed by the desired name of the footnote. After that is specified, reference that footnote in a newline and put whatever note that is needed.\nThis is where you can place a footnote,[^1] sometimes multiple can be placed in \none sentence.[^longnote]\n\n[^1]: This is a footnote.\n\n[^longnote]: This is a long footnote, which can have paragraphs.\n   \n    Make sure to use the four spaces to inent so that the subsequent paragraphs \n    belong to the same footnote.\n\n    ```\n    {code can also be inserted in here}\n    ```\n\n    End footnote\n\nSeperate paragraph here to show that this isn't part of the footnote.\nOutput:\nThis is where you can place a footnote,1 sometimes multiple can be placed in one sentence2\nSeperate paragraph here to show that this isn’t part of the footnote.\n\n\n3.4.14 Conclusion\nMarkdown is a versatile markup language that simplifies writing for the web in a way that is readable and convenient. Today, it is widely used to present the work that is being done in a variety of areas, and Markdown provides a clean way to organize the content being presented and structure the documents well.\n\n\n3.4.15 Further Reading\n\nQuarto Markdown Basics\nThe Complete Guide to Markdown\nMarkdown: Synatx by John Gruber\n\n\n\n\n\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#footnotes",
    "href": "03-quarto.html#footnotes",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "This is a footnote.↩︎\nThis is a long footnote, which can have paragraphs.\nMake sure to use the four spaces to inent so that the subsequent paragraphs belong to the same footnote.\n{code can also be inserted in here}\nEnd footnote↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "04-python.html",
    "href": "04-python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#the-python-world",
    "href": "04-python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#standard-library",
    "href": "04-python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#important-libraries",
    "href": "04-python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ 2.82006715, -3.9178885 ,  4.39633472,  8.50748384, -1.61381509,\n        4.26662995,  1.58397951,  5.7061026 ,  3.58507023,  3.6764123 ])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.0976614 , 0.03338489, 0.08335193, 0.02655367, 0.06631446,\n       0.08494229, 0.0991976 , 0.0649292 , 0.09220445, 0.09135004])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#writing-a-function",
    "href": "04-python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n7.75 μs ± 403 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.92 μs ± 69.8 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n62.9 μs ± 4.15 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(n_doors, n_trials):\n    doors = np.arange(1, n_doors + 1)\n    prize = np.random.choice(doors, size=n_trials)\n    player = np.random.choice(doors, size=n_trials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(n_trials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(n_trials)])\n    return {'noswitch': np.sum(prize == player),\n               'switch': np.sum(prize == player2)}\n\nTest it out with 3 doors.\n\nmontyhall(3, 1000)\n\n{'noswitch': np.int64(349), 'switch': np.int64(651)}\n\n\nThen with 4 doors\n\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(248), 'switch': np.int64(389)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.\n\n4.4.1.1 Faster version\nThis one avoid loops.\n\nimport numpy as np\nfrom typing import Dict, Optional\n\ndef montyhall_fast(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Run a Monty Hall simulation with `n_doors` doors and `n_trials` repetitions.\n    The host always opens exactly one empty door that is neither the prize door\n    nor the player's initial choice.\n\n    Parameters\n    ----------\n    n_doors : int\n        Total number of doors in the game (must be &gt;= 3).\n    n_trials : int\n        Number of independent trials to simulate.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    Dict[str, int]\n        A dictionary with counts of wins under two strategies:\n        - 'noswitch': staying with the initial choice\n        - 'switch'  : switching after the host reveals one empty door\n\n    Examples\n    --------\n    &gt;&gt;&gt; results = montyhall_fast(3, 100000, seed=42)\n    &gt;&gt;&gt; results\n    {'noswitch': 33302, 'switch': 66698}\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initial random assignments\n    prize = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n    player = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n\n    host = np.empty(n_trials, dtype=np.int32)\n\n    # Case 1: player == prize → host excludes only 'player'\n    same = prize == player\n    if np.any(same):\n        k = rng.integers(n_doors - 1, size=np.sum(same), dtype=np.int32)\n        p = player[same]\n        host[same] = k + (k &gt;= p)\n\n    # Case 2: player != prize → host excludes 'player' and 'prize'\n    diff = ~same\n    if np.any(diff):\n        a = np.minimum(player[diff], prize[diff])\n        b = np.maximum(player[diff], prize[diff])\n        k = rng.integers(n_doors - 2, size=np.sum(diff), dtype=np.int32)\n        host[diff] = k + (k &gt;= a) + (k &gt;= b)\n\n    # Player switches: exclude 'player' and 'host'\n    a2 = np.minimum(player, host)\n    b2 = np.maximum(player, host)\n    k2 = rng.integers(n_doors - 2, size=n_trials, dtype=np.int32)\n    player2 = k2 + (k2 &gt;= a2) + (k2 &gt;= b2)\n\n    return {\n        \"noswitch\": int((prize == player).sum()),\n        \"switch\": int((prize == player2).sum()),\n    }\n\nAnother faster version uses Numba just-in-time (JIT) compiler Python, which translates a subset of Python and Numpy into fast machine code via LLVM at runtime. A decorator is added to the numeric functions and, after a one-time compile on the first call (“warm-up”), later calls run much faster. One can toggle parallel=True for multi-core speedups.\n\nfrom typing import Dict, Optional\nimport numpy as np\nfrom numba import njit, prange\n\n# --- internal helpers (compiled) ---\n\n@njit\ndef _seed_numba(seed: int) -&gt; None:\n    # Seed the RNG used inside Numba-compiled code\n    np.random.seed(seed)\n\n@njit\ndef _trial_once(n_doors: int) -&gt; (int, int):\n    \"\"\"\n    Run one Monty Hall trial (host opens exactly one empty door).\n    Returns (noswitch_win, switch_win) as 0/1 ints.\n    \"\"\"\n    prize  = np.random.randint(0, n_doors)\n    player = np.random.randint(0, n_doors)\n\n    # host picks an empty door not equal to prize or player\n    if player == prize:\n        # exclude only 'player' (size n_doors-1)\n        k = np.random.randint(0, n_doors - 1)\n        host = k + (1 if k &gt;= player else 0)\n    else:\n        # exclude 'player' and 'prize' (size n_doors-2)\n        a = player if player &lt; prize else prize\n        b = prize if prize &gt; player else player\n        k = np.random.randint(0, n_doors - 2)\n        host = k + (1 if k &gt;= a else 0) + (1 if k &gt;= b else 0)\n\n    # player switches: choose uniformly from doors != player and != host\n    a2 = player if player &lt; host else host\n    b2 = host if host &gt; player else player\n    k2 = np.random.randint(0, n_doors - 2)\n    player2 = k2 + (1 if k2 &gt;= a2 else 0) + (1 if k2 &gt;= b2 else 0)\n\n    noswitch_win = 1 if prize == player  else 0\n    switch_win   = 1 if prize == player2 else 0\n    return noswitch_win, switch_win\n\n@njit(parallel=True)\ndef _run_parallel(n_doors: int, n_trials: int) -&gt; (int, int):\n    ns = np.zeros(n_trials, dtype=np.int64)\n    sw = np.zeros(n_trials, dtype=np.int64)\n    for i in prange(n_trials):\n        a, b = _trial_once(n_doors)\n        ns[i] = a\n        sw[i] = b\n    return int(ns.sum()), int(sw.sum())\n\n@njit\ndef _run_serial(n_doors: int, n_trials: int) -&gt; (int, int):\n    noswitch = 0\n    switch   = 0\n    for _ in range(n_trials):\n        a, b = _trial_once(n_doors)\n        noswitch += a\n        switch   += b\n    return noswitch, switch\n\n# --- public API ---\n\ndef montyhall_numba(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None,\n    parallel: bool = True,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Monty Hall (host opens one empty door) using Numba-compiled loops.\n\n    Parameters\n    ----------\n    n_doors : int\n        Number of doors (&gt;= 3).\n    n_trials : int\n        Number of trials to simulate.\n    seed : int, optional\n        Seed for reproducibility.\n    parallel : bool, default True\n        Use a parallel loop over trials (multi-core).\n\n    Returns\n    -------\n    Dict[str, int]\n        {'noswitch': ..., 'switch': ...}\n    \"\"\"\n    if n_doors &lt; 3:\n        raise ValueError(\"n_doors must be &gt;= 3\")\n\n    if seed is not None:\n        _seed_numba(int(seed))  # seed the compiled RNG\n\n    ns, sw = (_run_parallel(n_doors, n_trials)\n              if parallel else\n              _run_serial(n_doors, n_trials))\n    return {\"noswitch\": ns, \"switch\": sw}\n\nIt wins when n_trials is very large (e.g., 10–100M) where loop overhead is amortized and you give it many threads, or the design avoids massive temporaries (not much in this example).\nLet’s see their time comparison.\n\nimport timeit\n\n# --- Timing function ---\ndef benchmark(n_doors: int = 3, n_trials: int = 1_000_00, seed: int = 42) -&gt; None:\n    print(f\"n_doors={n_doors}, n_trials={n_trials}\")\n    # Ensure deterministic where possible\n    np.random.seed(seed)\n\n    # Time original\n    t_orig = min(timeit.repeat(lambda: montyhall(n_doors, n_trials),\n                               repeat=3, number=1))\n    print(f\"Original (lists): {t_orig:.4f}s\")\n\n    # Time vectorized\n    t_vec = min(timeit.repeat(lambda: montyhall_fast(n_doors, n_trials, seed),\n                              repeat=3, number=1))\n    print(f\"Vectorized NumPy: {t_vec:.4f}s\")\n\n    # Time Numba serial (compile excluded by earlier warm-up)\n    t_numba_ser = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=False), repeat=3, number=1))\n    print(f\"Numba serial:     {t_numba_ser:.4f}s\")\n\n    # Time Numba parallel\n    t_numba_par = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=True), repeat=3, number=1))\n    print(f\"Numba parallel:   {t_numba_par:.4f}s\")\n\n\nbenchmark(n_doors = 4, n_trials = 1000_000)\n\nn_doors=4, n_trials=1000000\nOriginal (lists): 38.3069s\nVectorized NumPy: 0.0758s\nNumba serial:     0.0575s\n\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nNumba parallel:   0.0305s",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#variables-versus-objects",
    "href": "04-python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4861643520\n4861643520\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4560407312\n4645278256\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#number-representation",
    "href": "04-python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#sec-python-venv",
    "href": "04-python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "05-visualization.html",
    "href": "05-visualization.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Grammar of Graphics with Plotnine\nThis is section was written by Sonia Lucey. Sonia was ..",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visualization.html#grammar-of-graphics-with-plotnine",
    "href": "05-visualization.html#grammar-of-graphics-with-plotnine",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Subsection 1\n\n\n5.1.2 Subsection 2\n\n\n5.1.3 Further Reading",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html",
    "href": "06-manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#introduction",
    "href": "06-manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#example-nyc-crash-data",
    "href": "06-manipulation.html#example-nyc-crash-data",
    "title": "6  Data Manipulation",
    "section": "6.2 Example: NYC Crash Data",
    "text": "6.2 Example: NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of August 31, 2025, on September 11, 2025, in CSC format.\n\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyc_crashes_lbdwk_2025.csv'\ndf = pd.read_csv(file_path,\n                 dtype={'LATITUDE': np.float32,\n                        'LONGITUDE': np.float32,\n                        'ZIP CODE': str})\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Check for missing values\ndf.isnull().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           284\nzip_code                          284\nlatitude                           12\nlongitude                          12\nlocation                           12\non_street_name                    456\ncross_street_name                 587\noff_street_name                  1031\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1       9\ncontributing_factor_vehicle_2     355\ncontributing_factor_vehicle_3    1358\ncontributing_factor_vehicle_4    1447\ncontributing_factor_vehicle_5    1474\ncollision_id                        0\nvehicle_type_code_1                17\nvehicle_type_code_2               475\nvehicle_type_code_3              1363\nvehicle_type_code_4              1452\nvehicle_type_code_5              1474\ndtype: int64\n\n\nTake a peek at the first five rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n08/31/2025\n12:49\nQUEENS\n11101\n40.753113\n-73.933701\n(40.753113, -73.9337)\n30 ST\n39 AVE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4838875\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n1\n08/31/2025\n15:30\nMANHATTAN\n10022\n40.760601\n-73.964317\n(40.7606, -73.96432)\nE 59 ST\n2 AVE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4839110\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n2\n08/31/2025\n19:00\nNaN\nNaN\n40.734234\n-73.722748\n(40.734234, -73.72275)\nCROSS ISLAND PARKWAY\nHILLSIDE AVENUE\nNaN\n...\nUnspecified\nUnspecified\nNaN\nNaN\n4838966\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n3\n08/31/2025\n1:19\nBROOKLYN\n11220\n40.648075\n-74.007034\n(40.648075, -74.007034)\nNaN\nNaN\n4415 5 AVE\n...\nUnspecified\nNaN\nNaN\nNaN\n4838563\nSedan\nE-Bike\nNaN\nNaN\nNaN\n\n\n4\n08/31/2025\n2:41\nMANHATTAN\n10036\n40.756561\n-73.986107\n(40.75656, -73.98611)\nW 43 ST\nBROADWAY\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4838922\nStation Wagon/Sport Utility Vehicle\nBike\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nA quick summary of the data types of the columns:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1487 entries, 0 to 1486\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1487 non-null   object \n 1   crash_time                     1487 non-null   object \n 2   borough                        1203 non-null   object \n 3   zip_code                       1203 non-null   object \n 4   latitude                       1475 non-null   float32\n 5   longitude                      1475 non-null   float32\n 6   location                       1475 non-null   object \n 7   on_street_name                 1031 non-null   object \n 8   cross_street_name              900 non-null    object \n 9   off_street_name                456 non-null    object \n 10  number_of_persons_injured      1487 non-null   int64  \n 11  number_of_persons_killed       1487 non-null   int64  \n 12  number_of_pedestrians_injured  1487 non-null   int64  \n 13  number_of_pedestrians_killed   1487 non-null   int64  \n 14  number_of_cyclist_injured      1487 non-null   int64  \n 15  number_of_cyclist_killed       1487 non-null   int64  \n 16  number_of_motorist_injured     1487 non-null   int64  \n 17  number_of_motorist_killed      1487 non-null   int64  \n 18  contributing_factor_vehicle_1  1478 non-null   object \n 19  contributing_factor_vehicle_2  1132 non-null   object \n 20  contributing_factor_vehicle_3  129 non-null    object \n 21  contributing_factor_vehicle_4  40 non-null     object \n 22  contributing_factor_vehicle_5  13 non-null     object \n 23  collision_id                   1487 non-null   int64  \n 24  vehicle_type_code_1            1470 non-null   object \n 25  vehicle_type_code_2            1012 non-null   object \n 26  vehicle_type_code_3            124 non-null    object \n 27  vehicle_type_code_4            35 non-null     object \n 28  vehicle_type_code_5            13 non-null     object \ndtypes: float32(2), int64(9), object(18)\nmemory usage: 325.4+ KB\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Drop the redundant `latitute` and `longitude` columns\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nLet’s get some basic frequency tables of borough and zip_code, whose values could be used to check their validity against the legitmate values.\n\n# Frequency table for 'borough' without filling missing values\nborough_freq = df['borough'].value_counts(dropna=False).reset_index()\nborough_freq.columns = ['borough', 'count']\n\n# Frequency table for 'zip_code' without filling missing values\nzip_code_freq = df['zip_code'].value_counts(dropna=False).reset_index()\nzip_code_freq.columns = ['zip_code', 'count']\nzip_code_freq\n\n\n\n\n\n\n\n\nzip_code\ncount\n\n\n\n\n0\nNaN\n284\n\n\n1\n11207\n33\n\n\n2\n11203\n29\n\n\n3\n11212\n23\n\n\n4\n11233\n21\n\n\n...\n...\n...\n\n\n159\n11379\n1\n\n\n160\n10007\n1\n\n\n161\n10308\n1\n\n\n162\n11362\n1\n\n\n163\n11694\n1\n\n\n\n\n164 rows × 2 columns\n\n\n\nA comprehensive list of ZIP codes by borough can be obtained, for example, from the New York City Department of Health’s UHF Codes. We can use this list to check the validity of the zip codes in the data.\n\n# List of valid NYC ZIP codes compiled from UHF codes\n# Define all_valid_zips based on the earlier extracted ZIP codes\nall_valid_zips = {\n    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,\n    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,\n    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,\n    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,\n    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,\n    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,\n    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,\n    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,\n    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,\n    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,\n    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,\n    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,\n    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,\n    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,\n    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,\n    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,\n    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,\n    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,\n    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,\n    10306, 10307, 10308, 10309, 10312\n}\n\n    \n# Convert set to list of strings\nall_valid_zips = list(map(str, all_valid_zips))\n\n# Identify invalid ZIP codes (including NaN)\ninvalid_zips = df[\n    df['zip_code'].isna() | ~df['zip_code'].isin(all_valid_zips)\n    ]['zip_code']\n\n# Calculate frequency of invalid ZIP codes\ninvalid_zip_freq = invalid_zips.value_counts(dropna=False).reset_index()\ninvalid_zip_freq.columns = ['zip_code', 'frequency']\n\ninvalid_zip_freq\n\n\n\n\n\n\n\n\nzip_code\nfrequency\n\n\n\n\n0\nNaN\n284\n\n\n1\n10000\n4\n\n\n2\n10065\n3\n\n\n3\n10075\n2\n\n\n4\n11430\n1\n\n\n\n\n\n\n\nAs it turns out, the collection of valid NYC zip codes differ from different sources. From United States Zip Codes, 10065 appears to be a valid NYC zip code. Under this circumstance, it might be safer to not remove any zip code from the data.\nTo be safe, let’s concatenate valid and invalid zips.\n\n# Convert invalid ZIP codes to a set of strings\ninvalid_zips_set = set(invalid_zip_freq['zip_code'].dropna().astype(str))\n\n# Convert all_valid_zips to a set of strings (if not already)\nvalid_zips_set = set(map(str, all_valid_zips))\n\n# Merge both sets\nmerged_zips = invalid_zips_set | valid_zips_set  # Union of both sets\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nnp.array([missing_cooccur, total_missing_zip_code, total_missing_borough])\n\narray([284, 284, 284])\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use module Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nget_zip_code(latitude, longitude)\n\n'11101'\n\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#accessing-census-data",
    "href": "06-manipulation.html#accessing-census-data",
    "title": "6  Data Manipulation",
    "section": "6.3 Accessing Census Data",
    "text": "6.3 Accessing Census Data\nThe U.S. Census Bureau provides extensive demographic, economic, and social data through multiple surveys, including the decennial Census, the American Community Survey (ACS), and the Economic Census. These datasets offer valuable insights into population trends, economic conditions, and community characteristics at multiple geographic levels.\nThere are multiple ways to access Census data. For example:\n\nCensus API: The Census API allows programmatic access to various datasets. It supports queries for different geographic levels and time periods.\ndata.census.gov: The official web interface for searching and downloading Census data.\nIPUMS USA: Provides harmonized microdata for longitudinal research. Available at IPUMS USA.\nNHGIS: Offers historical Census data with geographic information. Visit NHGIS.\n\nIn addition, Python tools simplify API access and data retrieval.\n\n6.3.1 Python Tools for Accessing Census Data\nSeveral Python libraries facilitate Census data retrieval:\n\ncensus: A high-level interface to the Census API, supporting ACS and decennial Census queries. See census on PyPI.\ncensusdis: Provides richer functionality: automatic discovery of variables, geographies, and datasets. Helpful if you don’t want to manually look up variable codes. See censusdis on PyPI.\nus: Often used alongside census libraries to handle U.S. state and territory information (e.g., FIPS codes). See us on PyPI.\n\n\n\n6.3.2 Zip-Code Level for NYC Crash Data\nNow that we have NYC crash data, we might want to analyze patterns at the zip-code level to understand whether certain demographic or economic factors correlate with traffic incidents. While the crash dataset provides details about individual accidents, such as location, time, and severity, it does not contain contextual information about the neighborhoods where these crashes occur.\nTo perform meaningful zip-code-level analysis, we need additional data sources that provide relevant demographic, economic, and geographic variables. For example, understanding whether high-income areas experience fewer accidents, or whether population density influences crash frequency, requires integrating Census data. Key variables such as population size, median household income, employment rate, and population density can provide valuable context for interpreting crash trends across different zip codes.\nSince the Census Bureau provides detailed estimates for these variables at the zip-code level, we can use the Census API or other tools to retrieve relevant data and merge it with the NYC crash dataset. To access the Census API, you need an API key, which is free and easy to obtain. Visit the Census API Request page and submit your email address to receive a key. Once you have the key, you must include it in your API requests to access Census data. The following demonstration assumes that you have registered, obtained your API key, and saved it in a file called censusAPIkey.txt.\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nimport io\n\napi_key = open(\"censusAPIkey.txt\").read().strip()\nc = Census(api_key)\n\nSuppose that we want to get some basic info from ACS data of the year of 2024 for all the NYC zip codes. The variable names can be found in the ACS variable documentation.\n\nACS_YEAR = 2024\nACS_DATASET = \"acs/acs5\"\n\n# Important ACS variables (including land area for density calculation)\nACS_VARIABLES = {\n    \"B01003_001E\": \"Total Population\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B02001_002E\": \"White Population\",\n    \"B02001_003E\": \"Black Population\",\n    \"B02001_005E\": \"Asian Population\",\n    \"B15003_022E\": \"Bachelor’s Degree Holders\",\n    \"B15003_025E\": \"Graduate Degree Holders\",\n    \"B23025_002E\": \"Labor Force\",\n    \"B23025_005E\": \"Unemployed\",\n    \"B25077_001E\": \"Median Home Value\"\n}\n\n# Convert set to list of strings\nmerged_zips = list(map(str, merged_zips))\n\nLet’s set up the query to request the ACS data, and process the returned data.\n\nacs_data = c.acs5.get(\n    list(ACS_VARIABLES.keys()), \n    {'for': f'zip code tabulation area:{\",\".join(merged_zips)}'}\n    )\n\n# Convert to DataFrame\ndf_acs = pd.DataFrame(acs_data)\n\n# Rename columns\ndf_acs.rename(columns=ACS_VARIABLES, inplace=True)\ndf_acs.rename(columns={\"zip code tabulation area\": \"ZIP Code\"}, inplace=True)\n\nWe could save the ACS data df_acs in feather format (see next Section).\n\ndf_acs.to_feather(\"data/acs2023.feather\")\n\nThe population density could be an important factor for crash likelihood. To obtain the population densities, we need the areas of the zip codes. The shape files can be obtained from NYC Open Data.\n\nimport requests\nimport zipfile\nimport geopandas as gpd\n\n# Define the NYC MODZCTA shapefile URL and extraction directory\nshapefile_url = \"https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile\"\nextract_dir = \"tmp/MODZCTA_Shapefile\"\n\n# Create the directory if it doesn't exist\nos.makedirs(extract_dir, exist_ok=True)\n\n# Step 1: Download and extract the shapefile\nprint(\"Downloading MODZCTA shapefile...\")\nresponse = requests.get(shapefile_url)\nwith zipfile.ZipFile(io.BytesIO(response.content), \"r\") as z:\n    z.extractall(extract_dir)\n\nprint(f\"Shapefile extracted to: {extract_dir}\")\n\nDownloading MODZCTA shapefile...\nShapefile extracted to: tmp/MODZCTA_Shapefile\n\n\nNow we process the shape file to calculate the areas of the polygons.\n\n# Step 2: Automatically detect the correct .shp file\nshapefile_path = None\nfor file in os.listdir(extract_dir):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join(extract_dir, file)\n        break  # Use the first .shp file found\n\nif not shapefile_path:\n    raise FileNotFoundError(\"No .shp file found in extracted directory.\")\n\nprint(f\"Using shapefile: {shapefile_path}\")\n\n# Step 3: Load the shapefile into GeoPandas\ngdf = gpd.read_file(shapefile_path)\n\n# Step 4: Convert to CRS with meters for accurate area calculation\ngdf = gdf.to_crs(epsg=3857)\n\n# Step 5: Compute land area in square miles\ngdf['land_area_sq_miles'] = gdf['geometry'].area / 2_589_988.11\n# 1 square mile = 2,589,988.11 square meters\n\nprint(gdf[['modzcta', 'land_area_sq_miles']].head())\n\nUsing shapefile: tmp/MODZCTA_Shapefile/geo_export_3838b8cb-aa31-44be-8860-873410381e0a.shp\n  modzcta  land_area_sq_miles\n0   10001            1.153516\n1   10002            1.534509\n2   10003            1.008318\n3   10026            0.581848\n4   10004            0.256876\n\n\nLet’s export this data frame for future usage in feather format (see next Section).\n\ngdf[['modzcta', 'land_area_sq_miles']].to_feather('data/nyc_zip_areas.feather')\n\nNow we are ready to merge the two data frames.\n\n# Merge ACS data (`df_acs`) directly with MODZCTA land area (`gdf`)\ngdf = gdf.merge(df_acs, left_on='modzcta', right_on='ZIP Code', how='left')\n\n# Calculate Population Density (people per square mile)\ngdf['popdensity_per_sq_mile'] = (\n    gdf['Total Population'] / gdf['land_area_sq_miles']\n    )\n\n# Display first few rows\nprint(gdf[['modzcta', 'Total Population', 'land_area_sq_miles',\n    'popdensity_per_sq_mile']].head())\n\n  modzcta  Total Population  land_area_sq_miles  popdensity_per_sq_mile\n0   10001           29079.0            1.153516            25209.019713\n1   10002           75517.0            1.534509            49212.471465\n2   10003           53825.0            1.008318            53380.992071\n3   10026           37113.0            0.581848            63784.749994\n4   10004            3875.0            0.256876            15085.082190\n\n\nSome visualization of population density.\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Set up figure and axis\nfig, ax = plt.subplots(figsize=(10, 12))\n\n# Plot the choropleth map\ngdf.plot(column='popdensity_per_sq_mile', \n         cmap='viridis',  # Use a visually appealing color map\n         linewidth=0.8, \n         edgecolor='black',\n         legend=True,\n         legend_kwds={'label': \"Population Density (per sq mile)\",\n             'orientation': \"horizontal\"},\n         ax=ax)\n\n# Add a title\nax.set_title(\"Population Density by ZCTA in NYC\", fontsize=14)\n\n# Remove axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#cross-platform-data-format-arrow",
    "href": "06-manipulation.html#cross-platform-data-format-arrow",
    "title": "6  Data Manipulation",
    "section": "6.4 Cross-platform Data Format Arrow",
    "text": "6.4 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\nThe following code processes the raw data in CSV format and write out in Arrow format.\n\n# File paths\ncsv_file = 'data/nyc_crashes_lbdwk_2025.csv'\nfeather_file = 'tmp/nyc_crashes_lbdwk_2025.feather'\n\nimport pandas as pd\n\n# Move 'crash_datetime' to the first column\ndf = df[['crash_datetime'] + df.drop(columns=['crash_datetime']).columns.tolist()]\n\ndf['zip_code'] = df['zip_code'].astype(str).str.rstrip('.0')\n\ndf = df.sort_values(by='crash_datetime')\n\ndf.to_feather(feather_file)\n\nLet’s compare the file sizes of the feather format and the CSV format.\n\nimport os\n\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nCSV file size: 0.27 MB\nFeather file size: 0.13 MB\n\n\nRead the feather file back in:\n#| eval: false\ndff = pd.read_feather(feather_file)\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-exploration.html",
    "href": "07-exploration.html",
    "title": "7  Data Exploration",
    "section": "",
    "text": "7.1 Introduction\nData exploration is the disciplined process that connects raw records to credible analysis. Its goals are to identify data quality issues, understand what variables mean operationally, and generate falsifiable claims that later analysis can scrutinize. The work is iterative: initial checks surface inconsistencies or gaps; targeted cleaning follows; then renewed examination tests whether earlier conclusions still hold. Reproducibility is non-negotiable, so every step should live in code with a brief log explaining what changed and why. Critically, this phase is not confirmatory inference. Instead, it frames questions clearly, proposes measurable definitions, and records assumptions that later sections will test formally. Scope of this chapter.\nWe will develop practical habits for high-quality exploration. First, we present cleaning principles: consistency of formats and units, completeness and missing-data mechanisms, accuracy and duplicates, and integrity across related fields, together with clear documentation. Next, we practice numerically driven summaries: distributional statistics, grouped descriptives, cross-tabulations, and simple association checks that reveal promising relationships. Finally, we show how to state hypotheses correctly—with the null representing no effect or independence—and run appropriate tests in Python (ANOVA or Kruskal–Wallis for group means, Welch’s t-test or Mann–Whitney for two groups, OLS slope tests with robust errors, and Pearson or Spearman correlations), pairing p-values with effect sizes and intervals.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#getting-to-know-data",
    "href": "07-exploration.html#getting-to-know-data",
    "title": "7  Data Exploration",
    "section": "7.2 Getting to Know Data",
    "text": "7.2 Getting to Know Data\nAny analysis begins by becoming familiar with the dataset. This involves learning what the observations represent, what types of variables are recorded, and whether the structure meets the expectations of tidy data. Before turning to a specific example, we highlight general principles.\n\nUnits of observation. Each row of a dataset should correspond to a single unit, such as an individual, transaction, or property sale. Misalignment of units often leads to errors in later analysis.\nVariables and their types. Columns record attributes of units. These may be continuous measurements, counts, ordered categories, or nominal labels. Recognizing the correct type is essential because it dictates which summary statistics and hypothesis tests are appropriate.\nTidy data principles. In a tidy format, each row is one observation and each column is one variable. When data are stored otherwise, reshaping is necessary before analysis can proceed smoothly.\n\nWe now illustrate these ideas using a reduced version of the Ames Housing dataset (De Cock, 2009), which is available directly from OpenML. With a filtered subset of ~1460 observations, it drops older sales, keeps only certain years, and removes some variables to make modeling cleaner for beginners.\n\nimport openml\n\n# Load Ames Housing (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\n\n# Basic dimensions\ndf.shape\n\n(1460, 81)\n\n\nThe dataset contains nearly three thousand house sales and eighty variables.\nIt is useful to view the first few rows to confirm the structure.\n\n# First few rows\ndf.head()\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNone\nReg\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNone\nReg\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNone\nIR1\nLvl\nAllPub\n...\n0\nNone\nNone\nNone\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\nEach row corresponds to one property sale, while columns record attributes such as lot size, neighborhood, and sale price.\nUnderstanding whether variables are numeric, categorical, or temporal guides later exploration and cleaning.\n\n# Dtypes in pandas\ndf.dtypes.value_counts()\n\nobject     43\nint64      22\nuint8      13\nfloat64     3\nName: count, dtype: int64\n\n\n\n# Example: show a few variables with types\ndf.dtypes.head(10)\n\nId               int64\nMSSubClass       uint8\nMSZoning        object\nLotFrontage    float64\nLotArea          int64\nStreet          object\nAlley           object\nLotShape        object\nLandContour     object\nUtilities       object\ndtype: object\n\n\nMost features are numeric or categorical. Some, such as YearBuilt, are integers but represent calendar years.\nThe outcome of interest is the sale price.\n\n# The default target from OpenML\ndataset.default_target_attribute\n\n'SalePrice'\n\n\nNumeric summaries highlight scale and possible outliers.\n\n# Summary statistics for numeric columns\ndf.describe().T.head(10)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nId\n1460.0\n730.500000\n421.610009\n1.0\n365.75\n730.5\n1095.25\n1460.0\n\n\nMSSubClass\n1460.0\n56.897260\n42.300571\n20.0\n20.00\n50.0\n70.00\n190.0\n\n\nLotFrontage\n1201.0\n70.049958\n24.284752\n21.0\n59.00\n69.0\n80.00\n313.0\n\n\nLotArea\n1460.0\n10516.828082\n9981.264932\n1300.0\n7553.50\n9478.5\n11601.50\n215245.0\n\n\nOverallQual\n1460.0\n6.099315\n1.382997\n1.0\n5.00\n6.0\n7.00\n10.0\n\n\nOverallCond\n1460.0\n5.575342\n1.112799\n1.0\n5.00\n5.0\n6.00\n9.0\n\n\nYearBuilt\n1460.0\n1971.267808\n30.202904\n1872.0\n1954.00\n1973.0\n2000.00\n2010.0\n\n\nYearRemodAdd\n1460.0\n1984.865753\n20.645407\n1950.0\n1967.00\n1994.0\n2004.00\n2010.0\n\n\nMasVnrArea\n1452.0\n103.685262\n181.066207\n0.0\n0.00\n0.0\n166.00\n1600.0\n\n\nBsmtFinSF1\n1460.0\n443.639726\n456.098091\n0.0\n0.00\n383.5\n712.25\n5644.0\n\n\n\n\n\n\n\nCategorical summaries reveal balance among levels.\n\n# Frequency counts for categorical columns\ndf['Neighborhood'].value_counts().head()\n\nNeighborhood\nNAmes      225\nCollgCr    150\nOldTown    113\nEdwards    100\nSomerst     86\nName: count, dtype: int64\n\n\n\n# Another example\ndf['GarageType'].value_counts(dropna=False)\n\nGarageType\nAttchd     870\nDetchd     387\nBuiltIn     88\nNone        81\nBasment     19\nCarPort      9\n2Types       6\nName: count, dtype: int64\n\n\nSimple checks can detect implausible combinations.\n\n# Houses should not be sold before built\n(df['YrSold'] &lt; df['YearBuilt']).sum()\n\nnp.int64(0)\n\n\n\n# Garage year built should not precede house year built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\nnp.int64(9)\n\n\nThese checks confirm that while the dataset is well curated, certain quirks require careful interpretation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#data-cleaning-principles",
    "href": "07-exploration.html#data-cleaning-principles",
    "title": "7  Data Exploration",
    "section": "7.3 Data Cleaning Principles",
    "text": "7.3 Data Cleaning Principles\nExploration begins with data cleaning. The purpose is not to modify values casually but to identify issues, understand their sources, and decide on a transparent response. The following principles provide structure.\nConsistency. Variables should follow the same format and unit across all records. Dates should have a common representation, categorical labels should not differ by spelling, and measurements should use the same scale.\nCompleteness. Missing values are unavoidable. It is important to determine whether they arise from data entry errors, survey nonresponse, or structural absence. For example, a missing value in FireplaceQu often indicates that a house has no fireplace rather than missing information.\nAccuracy. Values should be plausible. Obvious errors include negative square footage or sale years in the future. Duplicate records also fall under this category.\nIntegrity. Relationships between variables should be logically consistent. A house cannot be sold before it was built. If related totals exist, the sum of parts should match the total.\nTransparency. All cleaning decisions should be recorded. Reproducibility requires that another analyst can understand what was changed and why.\n\n7.3.1 Illustration with Ames Housing\nWe apply these principles to the Ames dataset. The first step is to inspect missing values.\n\n# Count missing values in each column\ndf.isna().sum().sort_values(ascending=False).head(15)\n\nPoolQC          1453\nMiscFeature     1406\nAlley           1369\nFence           1179\nFireplaceQu      690\nLotFrontage      259\nGarageFinish      81\nGarageQual        81\nGarageYrBlt       81\nGarageType        81\nGarageCond        81\nBsmtExposure      38\nBsmtFinType2      38\nBsmtCond          37\nBsmtFinType1      37\ndtype: int64\n\n\nSeveral variables, such as PoolQC, MiscFeature, and Alley, contain many missing entries. Documentation shows that these are structural, indicating the absence of the feature.\n\n# Example: check PoolQC against PoolArea\n(df['PoolQC'].isna() & (df['PoolArea'] &gt; 0)).sum()\n\nnp.int64(0)\n\n\nThe result is zero, confirming that missing PoolQC means no pool.\nConsistency can be checked by reviewing categorical labels.\n\n# Distinct values in Exterior1st\ndf['Exterior1st'].unique()\n\narray(['VinylSd', 'MetalSd', 'Wd Sdng', 'HdBoard', 'BrkFace', 'WdShing',\n       'CemntBd', 'Plywood', 'AsbShng', 'Stucco', 'BrkComm', 'AsphShn',\n       'Stone', 'ImStucc', 'CBlock'], dtype=object)\n\n\nIf spelling variants are detected, they should be harmonized.\nAccuracy checks involve searching for implausible values.\n\n# Negative or zero living area\n(df['GrLivArea'] &lt;= 0).sum()\n\nnp.int64(0)\n\n\nIntegrity checks verify logical relationships.\n\n# Houses sold before they were built\n(df['YrSold'] &lt; df['YearBuilt']).sum()\n\nnp.int64(0)\n\n\n\n# Garage built before house built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\nnp.int64(9)\n\n\nThese checks help identify issues to document and, if appropriate, correct in a reproducible way.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#common-exploration-practices",
    "href": "07-exploration.html#common-exploration-practices",
    "title": "7  Data Exploration",
    "section": "7.4 Common Exploration Practices",
    "text": "7.4 Common Exploration Practices\nAfter establishing data cleaning principles, the next step is to compute summaries that describe the distributions of variables and their relationships. This section avoids graphics, relying instead on tables and statistics.\n\n7.4.1 Univariate summaries\nSimple statistics reveal scale, central tendency, and variability.\n\n# Sale price distribution\ndf['SalePrice'].describe()\n\ncount      1460.000000\nmean     180921.195890\nstd       79442.502883\nmin       34900.000000\n25%      129975.000000\n50%      163000.000000\n75%      214000.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n\n\nThe sale price is right-skewed, with a mean higher than the median.\n\n# Lot area distribution\ndf['LotArea'].describe()\n\ncount      1460.000000\nmean      10516.828082\nstd        9981.264932\nmin        1300.000000\n25%        7553.500000\n50%        9478.500000\n75%       11601.500000\nmax      215245.000000\nName: LotArea, dtype: float64\n\n\nLot size shows extreme variation, indicating possible outliers.\n\n\n7.4.2 Grouped summaries\nComparisons across categories highlight differences in central tendency.\n\n# Mean sale price by neighborhood\ndf.groupby('Neighborhood')['SalePrice'].mean().sort_values().head()\n\nNeighborhood\nMeadowV     98576.470588\nIDOTRR     100123.783784\nBrDale     104493.750000\nBrkSide    124834.051724\nEdwards    128219.700000\nName: SalePrice, dtype: float64\n\n\nNeighborhoods differ substantially in average sale price.\n\n# Median sale price by overall quality\ndf.groupby('OverallQual')['SalePrice'].median()\n\nOverallQual\n1      50150.0\n2      60000.0\n3      86250.0\n4     108000.0\n5     133000.0\n6     160000.0\n7     200141.0\n8     269750.0\n9     345000.0\n10    432390.0\nName: SalePrice, dtype: float64\n\n\nHigher quality ratings correspond to higher prices.\n\n\n7.4.3 Cross-tabulations\nCross-tabulations summarize associations between categorical variables.\n\nimport pandas as pd\n# Neighborhood by garage type\npd.crosstab(df['Neighborhood'], df['GarageType']).head()\n\n\n\n\n\n\n\nGarageType\n2Types\nAttchd\nBasment\nBuiltIn\nCarPort\nDetchd\n\n\nNeighborhood\n\n\n\n\n\n\n\n\n\n\nBlmngtn\n0\n17\n0\n0\n0\n0\n\n\nBlueste\n0\n2\n0\n0\n0\n0\n\n\nBrDale\n0\n2\n0\n0\n0\n13\n\n\nBrkSide\n0\n3\n0\n1\n0\n44\n\n\nClearCr\n0\n24\n0\n1\n0\n2\n\n\n\n\n\n\n\nSome garage types are common only in specific neighborhoods.\n\n\n7.4.4 Correlations\nCorrelation coefficients capture linear associations between numeric variables.\n\n# Correlation between living area and sale price\ndf[['GrLivArea','SalePrice']].corr()\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\nGrLivArea\n1.000000\n0.708624\n\n\nSalePrice\n0.708624\n1.000000\n\n\n\n\n\n\n\n\n# Correlation between lot area and sale price\ndf[['LotArea','SalePrice']].corr()\n\n\n\n\n\n\n\n\nLotArea\nSalePrice\n\n\n\n\nLotArea\n1.000000\n0.263843\n\n\nSalePrice\n0.263843\n1.000000\n\n\n\n\n\n\n\nLiving area is strongly correlated with price, while lot area shows a weaker association.\n\n\n7.4.5 Conditional summaries\nExamining distributions within subgroups can surface interaction patterns.\n\n# Average sale price by house style\ndf.groupby('HouseStyle')['SalePrice'].mean().sort_values()\n\nHouseStyle\n1.5Unf    110150.000000\nSFoyer    135074.486486\n1.5Fin    143116.740260\n2.5Unf    157354.545455\nSLvl      166703.384615\n1Story    175985.477961\n2Story    210051.764045\n2.5Fin    220000.000000\nName: SalePrice, dtype: float64\n\n\nHouse style is another factor associated with variation in price.\nThese practices provide a numerical portrait of the data, guiding later steps where hypotheses will be stated explicitly and tested with appropriate statistical methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#forming-and-testing-hypotheses",
    "href": "07-exploration.html#forming-and-testing-hypotheses",
    "title": "7  Data Exploration",
    "section": "7.5 Forming and Testing Hypotheses",
    "text": "7.5 Forming and Testing Hypotheses\nExploration becomes more rigorous when we state hypotheses formally and run appropriate tests. The null hypothesis always represents no effect, no difference, or no association. The alternative expresses the presence of an effect. The examples below use the Ames Housing data.\n\n7.5.1 Neighborhood and sale price\nHypothesis: - H0: The mean sale prices are equal across neighborhoods. - H1: At least one neighborhood has a different mean.\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\n\nsub = df[['SalePrice','Neighborhood']].dropna()\nmodel = smf.ols('SalePrice ~ C(Neighborhood)', data=sub).fit()\nanova_lm(model, typ=2)\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(Neighborhood)\n5.023606e+12\n24.0\n71.784865\n1.558600e-225\n\n\nResidual\n4.184305e+12\n1435.0\nNaN\nNaN\n\n\n\n\n\n\n\nANOVA tests equality of group means. If significant, post-hoc comparisons can identify which neighborhoods differ.\n\n\n7.5.2 Year built and sale price\nHypothesis: - H0: The slope for YearBuilt is zero; no linear relationship. - H1: The slope is not zero.\n\nmodel = smf.ols('SalePrice ~ YearBuilt', data=df).fit(cov_type='HC3')\nmodel.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.53e+06\n1.36e+05\n-18.667\n0.000\n-2.8e+06\n-2.26e+06\n\n\nYearBuilt\n1375.3735\n68.973\n19.941\n0.000\n1240.189\n1510.558\n\n\n\n\n\nThe regression slope test checks whether newer houses tend to sell for more.\n\n\n7.5.3 Lot area and sale price\nHypothesis: - H0: The population correlation is zero. - H1: The correlation is not zero.\n\nfrom scipy import stats\nsub = df[['LotArea','SalePrice']].dropna()\nstats.pearsonr(sub['LotArea'], sub['SalePrice'])\n\nPearsonRResult(statistic=np.float64(0.2638433538714058), pvalue=np.float64(1.123139154918551e-24))\n\n\nA Pearson correlation tests linear association. A Spearman rank correlation can be used when distributions are skewed.\n\n\n7.5.4 Fireplaces and sale price\nHypothesis: - H0: The mean sale price is the same for houses with and without a fireplace. - H1: The mean sale prices differ.\n\nsub = df[['SalePrice','Fireplaces']].dropna()\nsub['has_fp'] = (sub['Fireplaces'] &gt; 0).astype(int)\n\ng1 = sub.loc[sub['has_fp']==1, 'SalePrice']\ng0 = sub.loc[sub['has_fp']==0, 'SalePrice']\n\nstats.ttest_ind(g1, g0, equal_var=False)\n\nTtestResult(statistic=np.float64(21.105376324953664), pvalue=np.float64(4.666259945494159e-84), df=np.float64(1171.6295727321062))\n\n\nWelch’s t-test compares means when variances differ.\n\n\n7.5.5 Garage type and neighborhood\nHypothesis: - H0: Garage type and neighborhood are independent. - H1: Garage type and neighborhood are associated.\n\nimport pandas as pd\nct = pd.crosstab(df['GarageType'], df['Neighborhood'])\nstats.chi2_contingency(ct)\n\nChi2ContingencyResult(statistic=np.float64(794.6871326886048), pvalue=np.float64(5.179037846292559e-100), dof=120, expected_freq=array([[7.39666425e-02, 8.70195794e-03, 6.52646846e-02, 2.08846991e-01,\n        1.17476432e-01, 6.43944888e-01, 2.21899927e-01, 3.39376360e-01,\n        3.43727339e-01, 1.26178390e-01, 5.22117476e-02, 1.91443075e-01,\n        9.52864394e-01, 3.91588107e-02, 3.17621465e-01, 1.78390138e-01,\n        3.35025381e-01, 4.39448876e-01, 8.70195794e-02, 3.08919507e-01,\n        2.52356780e-01, 3.74184191e-01, 1.08774474e-01, 1.65337201e-01,\n        4.78607687e-02],\n       [1.07251632e+01, 1.26178390e+00, 9.46337926e+00, 3.02828136e+01,\n        1.70340827e+01, 9.33720087e+01, 3.21754895e+01, 4.92095722e+01,\n        4.98404641e+01, 1.82958666e+01, 7.57070341e+00, 2.77592458e+01,\n        1.38165337e+02, 5.67802756e+00, 4.60551124e+01, 2.58665700e+01,\n        4.85786802e+01, 6.37200870e+01, 1.26178390e+01, 4.47933285e+01,\n        3.65917331e+01, 5.42567078e+01, 1.57722988e+01, 2.39738941e+01,\n        6.93981146e+00],\n       [2.34227701e-01, 2.75562001e-02, 2.06671501e-01, 6.61348803e-01,\n        3.72008702e-01, 2.03915881e+00, 7.02683104e-01, 1.07469181e+00,\n        1.08846991e+00, 3.99564902e-01, 1.65337201e-01, 6.06236403e-01,\n        3.01740392e+00, 1.24002901e-01, 1.00580131e+00, 5.64902103e-01,\n        1.06091371e+00, 1.39158811e+00, 2.75562001e-01, 9.78245105e-01,\n        7.99129804e-01, 1.18491661e+00, 3.44452502e-01, 5.23567803e-01,\n        1.51559101e-01],\n       [1.08484409e+00, 1.27628716e-01, 9.57215373e-01, 3.06308920e+00,\n        1.72298767e+00, 9.44452502e+00, 3.25453227e+00, 4.97751994e+00,\n        5.04133430e+00, 1.85061639e+00, 7.65772299e-01, 2.80783176e+00,\n        1.39753445e+01, 5.74329224e-01, 4.65844815e+00, 2.61638869e+00,\n        4.91370558e+00, 6.44525018e+00, 1.27628716e+00, 4.53081943e+00,\n        3.70123278e+00, 5.48803481e+00, 1.59535896e+00, 2.42494561e+00,\n        7.01957941e-01],\n       [1.10949964e-01, 1.30529369e-02, 9.78970268e-02, 3.13270486e-01,\n        1.76214648e-01, 9.65917331e-01, 3.32849891e-01, 5.09064540e-01,\n        5.15591008e-01, 1.89267585e-01, 7.83176215e-02, 2.87164612e-01,\n        1.42929659e+00, 5.87382161e-02, 4.76432197e-01, 2.67585207e-01,\n        5.02538071e-01, 6.59173314e-01, 1.30529369e-01, 4.63379260e-01,\n        3.78535170e-01, 5.61276287e-01, 1.63161711e-01, 2.48005801e-01,\n        7.17911530e-02],\n       [4.77084844e+00, 5.61276287e-01, 4.20957215e+00, 1.34706309e+01,\n        7.57722988e+00, 4.15344453e+01, 1.43125453e+01, 2.18897752e+01,\n        2.21704133e+01, 8.13850616e+00, 3.36765772e+00, 1.23480783e+01,\n        6.14597534e+01, 2.52574329e+00, 2.04865845e+01, 1.15061639e+01,\n        2.16091371e+01, 2.83444525e+01, 5.61276287e+00, 1.99253082e+01,\n        1.62770123e+01, 2.41348803e+01, 7.01595359e+00, 1.06642495e+01,\n        3.08701958e+00]]))\n\n\nA chi-square test checks for association between two categorical variables.\n\n\n\n7.5.6 Quick reference table for common tests\n\n\n\n\n\n\n\n\n\nSituation\nNull hypothesis\nTest\nPython tool\n\n\n\n\nk-group mean comparison\nAll group means equal\nOne-way ANOVA\nanova_lm\n\n\nk-group, nonparametric\nAll group distributions equal\nKruskal–Wallis\nstats.kruskal\n\n\nTwo means, unequal variance\nMeans equal\nWelch’s t-test\nstats.ttest_ind\n\n\nTwo groups, nonparametric\nDistributions equal\nMann–Whitney U\nstats.mannwhitneyu\n\n\nLinear relationship\nSlope = 0\nOLS slope test\nols + robust SE\n\n\nContinuous association\nCorrelation = 0\nPearson correlation\nstats.pearsonr\n\n\nMonotone association\nCorrelation = 0\nSpearman correlation\nstats.spearmanr\n\n\nCategorical association\nIndependence\nChi-square test\nstats.chi2_contingency\n\n\n\n\nThese examples illustrate how hypotheses guide exploration. Each test produces a statistic, a p-value, and often an effect size. Results are provisional and informal, but they shape which relationships merit deeper investigation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#iterative-nature-of-exploration",
    "href": "07-exploration.html#iterative-nature-of-exploration",
    "title": "7  Data Exploration",
    "section": "7.6 Iterative Nature of Exploration",
    "text": "7.6 Iterative Nature of Exploration\nExploration is rarely linear. Cleaning, summarizing, and testing feed back into each other. Each new discovery can prompt a return to earlier steps.\n\n7.6.1 Cycle of exploration\n\nInspect variables and detect anomalies.\nClean data based on what anomalies reveal.\nSummarize distributions and relationships.\nFormulate and test new hypotheses.\nRevisit cleaning when results suggest overlooked issues.\n\n\n\n7.6.2 Example: Garage year built\nInitial inspection may suggest that many values of GarageYrBlt are missing. Documentation indicates that missing means no garage.\n\n# Count missing garage years\ndf['GarageYrBlt'].isna().sum()\n\nnp.int64(81)\n\n\nWhen checking integrity, we may notice that some garage years precede the house year built.\n\n# Garage built before house built\n(df['GarageYrBlt'] &lt; df['YearBuilt']).sum()\n\nnp.int64(9)\n\n\nThis prompts a decision: treat as data entry error, keep with caution, or exclude in certain analyses.\n\n\n7.6.3 Example: Living area and sale price\nA strong correlation between GrLivArea and SalePrice may surface.\n\n# Correlation\nsub = df[['GrLivArea','SalePrice']].dropna()\nsub.corr()\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\nGrLivArea\n1.000000\n0.708624\n\n\nSalePrice\n0.708624\n1.000000\n\n\n\n\n\n\n\nIf a few extremely large houses are driving the correlation, it may be necessary to investigate further.\n\n# Identify extreme values\nsub[sub['GrLivArea'] &gt; 4000][['GrLivArea','SalePrice']]\n\n\n\n\n\n\n\n\nGrLivArea\nSalePrice\n\n\n\n\n523\n4676\n184750\n\n\n691\n4316\n755000\n\n\n1182\n4476\n745000\n\n\n1298\n5642\n160000\n\n\n\n\n\n\n\nThese observations may be genuine luxury properties, or they may distort summary statistics. The decision is context-dependent and should be documented.\nExploration is not a one-pass process. Findings in one step often require revisiting previous steps. Clear documentation ensures that these iterations are transparent and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "07-exploration.html#good-practices-in-data-exploration",
    "href": "07-exploration.html#good-practices-in-data-exploration",
    "title": "7  Data Exploration",
    "section": "7.7 Good Practices in Data Exploration",
    "text": "7.7 Good Practices in Data Exploration\nClear habits in exploration make later analysis more reliable and easier to share. The following practices help ensure quality and reproducibility.\n\n7.7.1 Reproducibility\n\nKeep all work in notebooks or scripts, never only in spreadsheets.\nEnsure that another analyst can rerun the code and obtain identical results.\n\n\n# Example: set a random seed for reproducibility\nimport numpy as np\nnp.random.seed(20250923)\n\n\n\n7.7.2 Documentation\n\nRecord cleaning decisions explicitly. For example, note that NA in PoolQC means no pool.\nKeep a running log of questions, anomalies, and decisions.\n\n\n# Example: create an indicator for presence of a pool\ndf['HasPool'] = df['PoolArea'] &gt; 0\n\n\n\n7.7.3 Balanced curiosity and rigor\n\nExploration can generate many possible stories. Avoid over-interpreting patterns before formal testing.\nDistinguish clearly between exploratory checks and confirmatory analysis.\n\n\n\n7.7.4 Effect sizes and intervals\n\nReport not only p-values but also effect sizes and confidence intervals.\nThis practice keeps focus on the magnitude of relationships.\n\n\n# Example: compute Cohen's d for fireplace vs no fireplace\nsub = df[['SalePrice','Fireplaces']].dropna()\nsub['has_fp'] = (sub['Fireplaces'] &gt; 0).astype(int)\n\nmean_diff = sub.groupby('has_fp')['SalePrice'].mean().diff().iloc[-1]\npooled_sd = sub.groupby('has_fp')['SalePrice'].std().mean()\ncohens_d = mean_diff / pooled_sd\ncohens_d\n\nnp.float64(1.144008229281349)\n\n\n\n\n7.7.5 Transparency\n\nShare both code and notes with collaborators.\nVersion control with Git helps track changes and decisions.\n\n\nGood practices keep exploration structured and reproducible. They also create a record of reasoning that improves collaboration and supports later analysis.\n\n\n\n\nDe Cock, D. (2009). Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. Journal of Statistics Education, 17(3), 1–13. https://doi.org/10.1080/10691898.2009.11889627",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "08-regression.html",
    "href": "08-regression.html",
    "title": "8  Regression Models",
    "section": "",
    "text": "8.1 Introduction\nRegression is a fundamental tool in data science and statistics for modeling relationships between variables. It provides a framework to explain how a response variable changes when one or more explanatory variables vary, and it serves as a foundation for prediction, interpretation, and decision making. Regression models are used in a wide range of applications, from estimating the effect of education on income to predicting housing prices based on property characteristics.\nThis chapter introduces regression through a unified set of examples using the Ames Housing dataset. The dataset contains detailed information about housing sales in Ames, Iowa. It has become a popular benchmark for regression tasks, replacing the older Boston Housing dataset due to its larger size and richer set of features. Throughout the chapter, we will use this dataset to illustrate concepts of regression modeling, including model formulation, fitting, diagnosis, and extensions such as regularization, GLM, and GAM. Because many variables in the dataset record absence with NA (for example, NA in the alley variable indicates no alley access), careful preprocessing is required before modeling.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#introduction",
    "href": "08-regression.html#introduction",
    "title": "8  Regression Models",
    "section": "",
    "text": "8.1.1 Ames Housing Data Cleaning\nThe Ames housing data will be used for illustration, but it requires careful preprocessing. A distinctive feature of this dataset is that many NA values do not represent missing information but instead denote the absence of a feature. Treating them as missing would discard useful signals, so they should be recoded explicitly.\nFirst, we retrieve the data to ensure reproducibility.\n\nimport openml\nimport pandas as pd\n\n# Load Ames Housing dataset (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\n\nFor categorical variables, NA often means that the property does not have the feature. For example, Alley is NA when no alley access exists, FireplaceQu is NA when no fireplace is present, and PoolQC is NA when the house does not have a pool. In these cases, NA should be replaced with an explicit category such as “None.”\n\nnone_cols = [\n    \"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\",\n    \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n    \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n    \"BsmtFinType2\", \"MasVnrType\"\n]\nfor col in none_cols:\n    df[col] = df[col].fillna(\"None\")\n\nFor numeric variables, NA may also indicate absence. Examples include GarageCars, GarageArea, and basement square footage variables. When no garage or basement is present, the correct encoding is zero. Thus, these columns should be filled with 0 rather than treated as missing.\n\nzero_cols = [\n    \"GarageCars\", \"GarageArea\",\n    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n    \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\",\n    \"MasVnrArea\"\n]\nfor col in zero_cols:\n    df[col] = df[col].fillna(0)\n\nFinally, some variables contain genuinely missing data. A common example is LotFrontage, which records the linear feet of street connected to a property. Here NA values reflect unavailable measurements. These can be imputed using summary statistics such as the median or by more advanced methods if desired.\n\ndf[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].median())\n\nThis structured cleaning step ensures that absence is distinguished from missingness, numeric zero values are meaningful, and true missing values are handled appropriately. Only after this preparation is the dataset ready for modeling.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#linear-regression-model",
    "href": "08-regression.html#linear-regression-model",
    "title": "8  Regression Models",
    "section": "8.2 Linear Regression Model",
    "text": "8.2 Linear Regression Model\nThe starting point of regression analysis is the specification of a model that links a response variable to one or more explanatory variables. In the simplest form, the relationship is described by a linear function plus an error term:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i,\n\\]\nwhere \\(Y_i\\) is the response for observation \\(i\\), \\(X_{ij}\\) are the explanatory variables, \\(\\beta_j\\) are unknown coefficients, and \\(\\\\varepsilon_i\\) is a random error term. The model asserts that systematic variation in the response is captured by a linear combination of predictors, while unsystematic variation is left to the error.\nFor linear regression to yield valid estimates and inference, several assumptions are commonly made. The form of the mean function is assumed linear in parameters. The error terms are assumed to have mean zero and constant variance, and to be independent across observations. When sample sizes are small, normality of the errors is sometimes assumed to justify exact inference. With larger samples, asymptotic results make this assumption less critical, and estimation of coefficients by least squares does not require it. Finally, explanatory variables should not be perfectly collinear. These assumptions guide model fitting and motivate the diagnostic checks that follow.\n\n8.2.1 Fitting\nFitting a regression model means finding estimates of the coefficients that make the model align with observed data. The most common approach is ordinary least squares, which minimizes the sum of squared residuals:\n\\[\nL(\\beta) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_{i1} - \\cdots - \\beta_p X_{ip})^2.\n\\]\nHere \\(L(\\beta)\\) is the loss function, measuring how far predictions are from observed responses. Minimizing this quadratic loss yields closed form solutions for the coefficient estimates when predictors are not perfectly collinear. Computationally, this involves solving the normal equations or using matrix decompositions such as QR or singular value decomposition, which provide stable and efficient solutions.\nThis framework also sets the stage for extensions. By modifying the loss function to include penalty terms, one obtains regularization methods such as ridge regression or the lasso. The optimization remains similar in spirit but balances data fit with model complexity. Later sections will show how these modifications improve prediction and interpretability when many predictors are involved.\nHousing prices are highly skewed, with a long right tail. To stabilize variance and make the model fit better, it is common to use the log of SalePrice as the response:\n\\[\nY_i = \\log(\\text{SalePrice}_i).\n\\]\nWe add this transformed response to the dataset.\n\nimport numpy as np\n\ndf[\"LogPrice\"] = np.log(df[\"SalePrice\"])\n\nMany studies and analyses of the Ames data have found certain variables to be consistently important for predicting sale price. These include OverallQual (overall material and finish quality), GrLivArea (above- ground living area), GarageCars (garage capacity), TotalBsmtSF (total basement area), YearBuilt (construction year), FullBath (number of full bathrooms), and KitchenQual (kitchen quality). We will focus on these predictors to illustrate model fitting.\nInstead of manually creating dummy variables, we can use the formula API from statsmodels, which handles categorical predictors internally.\n\nimport statsmodels.formula.api as smf\n\nformula = (\n    \"LogPrice ~ OverallQual + GrLivArea + GarageCars + \"\n    \"TotalBsmtSF + YearBuilt + FullBath + C(KitchenQual)\"\n)\n\nWe then fit the regression model directly with the formula.\n\nmodel = smf.ols(formula, data=df).fit()\n\nFinally, we examine the regression results, which highlight the most important predictors of log sale price.\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nLogPrice\nR-squared:\n0.819\n\n\nModel:\nOLS\nAdj. R-squared:\n0.818\n\n\nMethod:\nLeast Squares\nF-statistic:\n727.5\n\n\nDate:\nThu, 02 Oct 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n14:02:14\nLog-Likelihood:\n515.19\n\n\nNo. Observations:\n1460\nAIC:\n-1010.\n\n\nDf Residuals:\n1450\nBIC:\n-957.5\n\n\nDf Model:\n9\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n6.8033\n0.419\n16.219\n0.000\n5.980\n7.626\n\n\nC(KitchenQual)[T.Fa]\n-0.2146\n0.037\n-5.780\n0.000\n-0.287\n-0.142\n\n\nC(KitchenQual)[T.Gd]\n-0.0653\n0.020\n-3.257\n0.001\n-0.105\n-0.026\n\n\nC(KitchenQual)[T.TA]\n-0.1348\n0.023\n-5.931\n0.000\n-0.179\n-0.090\n\n\nOverallQual\n0.0882\n0.006\n15.841\n0.000\n0.077\n0.099\n\n\nGrLivArea\n0.0002\n1.34e-05\n17.987\n0.000\n0.000\n0.000\n\n\nGarageCars\n0.0822\n0.008\n10.056\n0.000\n0.066\n0.098\n\n\nTotalBsmtSF\n0.0001\n1.28e-05\n9.134\n0.000\n9.2e-05\n0.000\n\n\nYearBuilt\n0.0021\n0.000\n9.686\n0.000\n0.002\n0.003\n\n\nFullBath\n-0.0087\n0.012\n-0.727\n0.467\n-0.032\n0.015\n\n\n\n\n\n\n\n\nOmnibus:\n962.926\nDurbin-Watson:\n1.990\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n36561.376\n\n\nSkew:\n-2.522\nProb(JB):\n0.00\n\n\nKurtosis:\n26.991\nCond. No.\n2.57e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.57e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe output gives estimated coefficients, standard errors, and measures of fit. At this stage, several questions naturally arise:\n\nHow should we interpret a coefficient when the outcome is on the log scale? For instance, what does a one-unit increase in OverallQual imply for expected sale price?\nHow do we compare the importance of variables measured on different scales, such as square footage and construction year?\nWhat role do categorical variables like KitchenQual play, and how do we interpret their dummy coefficients relative to the baseline?\nWhich predictors are statistically significant, and does significance necessarily imply practical importance?\nHow well does the model explain variation in housing prices, and what limitations might remain?\n\nThese questions guide us in interpreting the fitted model and connect directly to the diagnostic checks discussed in the next section.\n\n\n8.2.2 Diagnosis\nOnce a regression model has been fitted, it is essential to examine whether the underlying assumptions hold and whether the model provides a useful description of the data. Diagnostics help identify potential problems such as non-linearity, heteroscedasticity, influential points, and violations of independence.\nThe first step is to examine residuals, defined as the difference between observed and fitted values:\n\\[\n\\hat{\\varepsilon}_i = Y_i - \\hat{Y}_i.\n\\]\nPlotting residuals against fitted values reveals whether variance is constant and whether systematic patterns remain.\n\nimport matplotlib.pyplot as plt\n\nfitted_vals = model.fittedvalues\nresiduals = model.resid\n\nplt.scatter(fitted_vals, residuals, alpha=0.5)\nplt.axhline(0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs Fitted\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother check is the distribution of residuals. A histogram or Q-Q plot can indicate whether residuals are approximately normal, which is most relevant for inference in small samples.\n\nplt.hist(residuals, bins=30, edgecolor=\"black\")\nplt.xlabel(\"Residuals\")\nplt.title(\"Histogram of residuals\")\nplt.show()\n\ninfluence = model.get_influence()\nstd_resid = influence.resid_studentized_internal\n\nimport statsmodels.api as sm\nsm.qqplot(std_resid, line=\"45\")\nplt.title(\"Q-Q plot of residuals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential observations can distort regression results. Leverage and Cook’s distance are standard measures to detect such points.\n\ncooks = influence.cooks_distance[0]\n\nplt.scatter(fitted_vals, cooks, alpha=0.5)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Cook's distance\")\nplt.title(\"Influence diagnostics\")\nplt.show()\n\n\n\n\n\n\n\n\nKey questions to raise at this stage are:\n\nDo residuals appear randomly scattered, suggesting the linear model is adequate?\nIs there evidence of non-constant variance or other systematic patterns?\nAre residuals approximately normal, and does this matter given the sample size?\nWhich points exert disproportionate influence on the fitted model?\n\nThese diagnostic tools guide improvements, such as transformations, adding interaction terms, or considering alternative modeling approaches.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#regularized-regression",
    "href": "08-regression.html#regularized-regression",
    "title": "8  Regression Models",
    "section": "8.3 Regularized Regression",
    "text": "8.3 Regularized Regression\n\n8.3.1 Motivation\nOrdinary least squares can perform poorly when there are many predictors or when predictors are highly correlated. In such cases, estimated coefficients become unstable and prediction accuracy suffers. Regularization introduces penalties on the size of coefficients, leading to simpler and more robust models.\nTo illustrate, we return to the Ames data. Suppose we fit a model with a large set of predictors. Ordinary least squares will attempt to explain every fluctuation in the data, potentially overfitting. A regularized approach reduces this risk by shrinking coefficients, improving out-of-sample prediction.\n\n\n8.3.2 Formulation\nThe penalized regression framework modifies the least squares objective:\n\\[\nL(\\beta) = \\sum_{i=1}^n (Y_i - X_i^\\top \\beta)^2 + \\lambda P(\\beta),\n\\]\nwhere \\(P(\\beta)\\) is a penalty function and \\(\\lambda\\) controls its strength.\nFor ridge regression the penalty is\n\\[\nP(\\beta) = \\sum_j \\beta_j^2,\n\\]\nwhich shrinks coefficients smoothly toward zero but never sets them exactly to zero. For lasso regression the penalty is\n\\[\nP(\\beta) = \\sum_j |\\beta_j|.\n\\]\nBecause the absolute value has a sharp corner at zero, lasso can shrink some coefficients exactly to zero. This property allows lasso to perform variable selection and estimation in a single step, producing sparse models in which unimportant predictors are excluded automatically. Elastic net combines both types of penalties.\n\n\n8.3.3 Algorithms\nRidge regression has a closed-form solution obtained by modifying the normal equations. Lasso and elastic net require iterative algorithms, with coordinate descent being the most widely used. In practice, these algorithms are efficient and scale well to high-dimensional data.\nWhen using scikit-learn, predictors must be numeric. Since the Ames data include categorical variables such as KitchenQual, we need to encode them. We use a OneHotEncoder inside a ColumnTransformer. This transforms categorical variables into binary indicator columns while keeping numeric variables unchanged. The drop=\"first\" option avoids perfect collinearity by omitting one reference category.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\n\nnumeric_features = [\n    \"OverallQual\", \"GrLivArea\", \"GarageCars\",\n    \"TotalBsmtSF\", \"YearBuilt\", \"FullBath\"\n]\ncategorical_features = [\"KitchenQual\"]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n    ]\n)\n\nX = df[numeric_features + categorical_features]\ny = df[\"LogPrice\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nridge = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", Ridge(alpha=10))\n]).fit(X_train, y_train)\n\nlasso = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", Lasso(alpha=0.1))\n]).fit(X_train, y_train)\n\n\n\n8.3.4 Solution Paths\nAs the penalty parameter \\(\\lambda\\) decreases, the behavior of coefficients changes. Ridge coefficients approach ordinary least squares estimates, while lasso coefficients enter the model sequentially, illustrating variable selection.\nBecause predictors must be on the same scale for a fair comparison, it is important to standardize the numeric variables before computing the lasso path. Without standardization, some coefficients can appear flat or dominate others due to differences in scale.\nIn our Ames example, we have six numeric predictors and three dummy variables for KitchenQual (since one category was dropped). This means we are estimating nine coefficients in total, excluding the intercept. All nine should appear in the solution path once variables are properly scaled.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import lasso_path\nimport numpy as np\n\n# Standardized predictors\nX_proc = preprocessor.fit_transform(X_train)\nscaler = StandardScaler(with_mean=False)\nX_proc_std = scaler.fit_transform(X_proc)\n\n# Compute lambda_max\nn_samples = X_proc_std.shape[0]\nlambda_max = np.max(np.abs(X_proc_std.T @ y_train)) / n_samples\n\n# Define a grid of lambda values\nalphas = np.logspace(np.log10(lambda_max), -3, 50)\n\n# Compute solution path\nalphas, coefs, _ = lasso_path(X_proc_std, y_train, alphas=alphas)\n\nplt.plot(alphas, coefs.T)\nplt.xscale(\"log\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Coefficients\")\nplt.title(\"Lasso solution paths starting at lambda_max\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals how each coefficient evolves as \\(\\lambda\\) changes. At large values of \\(\\lambda\\), coefficients are shrunk close to zero. As \\(\\lambda\\) decreases, more predictors enter the model. The intercept is not included in the path and should be ignored when interpreting these curves.\n\n\n8.3.5 Tuning Parameter Selection\nChoosing \\(\\lambda\\) is critical. Too large, and the model is oversmoothed; too small, and the penalty has little effect. This parameter controls the trade-off between model fit and complexity:\n\nWhen \\(\\lambda = 0\\), the model reduces to the unpenalized regression.\nAs \\(\\lambda \\to \\infty\\), coefficients shrink toward zero, increasing bias but reducing variance.\n\nChoosing \\(\\lambda\\) appropriately is crucial. A general principle is to define a selection criterion \\(C(\\lambda)\\), which measures the predictive or explanatory performance of the fitted model, and then select \\(\\hat{\\lambda} = \\arg\\min_{\\lambda} C(\\lambda)\\).\nCommon criteria:\n\n\\(R^2\\) on a validation set: select \\(\\lambda\\) that maximizes explained variance.\nInformation criteria (AIC, BIC): less common in practice for penalized regression.\nCross-validation (CV): partition data, fit on training folds, evaluate on holdout fold, average prediction error across folds.\n\nThe grid of candidate values is not arbitrary:\n\nMaximum value:\n\\[\n\\lambda_{\\max} = \\max_j \\tfrac{1}{n} |x_j^\\top y|,\n\\]\nwith standardized predictors. At this level, all coefficients are zero.\nMinimum value: \\[\n\\lambda_{\\min} = \\epsilon \\cdot \\lambda_{\\max},\n\\] with \\(\\epsilon = 10^{-3}\\) if \\(n &gt; p\\) and \\(\\epsilon = 10^{-2}\\) otherwise.\nGrid: values are log-spaced between \\(\\lambda_{\\max}\\) and \\(\\lambda_{\\min}\\) (default 100 points in scikit-learn).\n\nIn the Ames example, we can use \\(k\\)-fold cross-validation to evaluate ridge and lasso models.\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\n\n# Lasso with 10-fold CV\nlasso_cv = LassoCV(\n    cv=KFold(n_splits=10, shuffle=True, random_state=123),\n    random_state=123\n)\n\n# Pipeline with preprocessing + model\npipe = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", lasso_cv)\n])\n\npipe.fit(X, y)\n\n# Selected lambda\nbest_lambda = pipe.named_steps[\"model\"].alpha_\nprint(\"Best lambda (alpha) selected by CV:\", best_lambda)\n\nBest lambda (alpha) selected by CV: 0.0003263140336320298\n\n\nThis process identifies the tuning parameter that balances bias and variance most effectively, yielding a model that generalizes well beyond the training data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "08-regression.html#generalized-linear-models",
    "href": "08-regression.html#generalized-linear-models",
    "title": "8  Regression Models",
    "section": "8.4 Generalized Linear Models",
    "text": "8.4 Generalized Linear Models\n\n8.4.1 Introduction\nLinear regression is a powerful tool for continuous outcomes under Gaussian assumptions, but many response variables encountered in practice are not continuous or normally distributed. For example, an indicator of whether a house sale price is above the median is binary, the number of bathrooms is a count, and proportions such as the fraction of remodeled homes lie between 0 and 1. Using linear regression in these settings can yield nonsensical predictions (e.g., negative counts or probabilities outside the unit interval).\nGeneralized linear models (GLMs) extend linear regression to cover a wider range of outcomes. The key idea is to preserve the familiar linear predictor structure, while linking it to the mean of the outcome through a function that reflects the nature of the data. The formal framework was introduced by Nelder & Wedderburn (1972) and remains central in modern statistics. Today, GLMs are viewed more flexibly: the distributional assumption provides a convenient likelihood-based loss function, but in practice one can proceed with quasi-likelihood or even direct loss minimization without strict distributional commitment.\n\n\n8.4.2 Framework\nGLMs extend linear regression by introducing a link function between the linear predictor and the conditional mean of the response. In linear regression we write\n\\[\nY_i = X_i^\\top \\beta + \\varepsilon_i\n\\]\nwith mean zero error \\(\\varepsilon_i\\)’s.\nThe mean is simply \\(\\mu_i = X_i^\\top \\beta\\). In a GLM, we allow non-Gaussian outcomes by defining\n\\[\n\\eta_i = X_i^\\top \\beta, \\quad g(\\mu_i) = \\eta_i,\n\\]\nwhere \\(g(\\cdot)\\) is a monotone link function, \\(\\mu_i = \\mathbb{E}(Y_i)\\), and \\(\\beta\\) are regression coefficients. The coefficients maintain the same interpretation as in linear regression: a one-unit change in a predictor shifts the linear predictor \\(\\eta_i\\) by its coefficient, with an indirect effect on \\(\\mu_i\\) through the link.\nThe variance of \\(Y_i\\) depends on the mean: \\(\\text{Var}(Y_i) = V(\\mu_i)\n\\cdot \\phi\\), where \\(V(\\cdot)\\) is the variance function and \\(\\phi\\) is a dispersion parameter. This structure arises naturally from the exponential family, which provides a unifying framework for GLMs. While exact distributional assumptions can be specified, the mean–variance relationship is often sufficient.\n\n\n8.4.3 Special Cases\n\nLogistic regression\nFor binary outcomes,\n\\[\nY_i \\sim \\text{Bernoulli}(\\mu_i), \\qquad g(\\mu_i) = \\log \\frac{\\mu_i}{1-\\mu_i}.\n\\]\nThe coefficient \\(\\beta_j\\) quantifies the log-odds change of success for a one-unit increase in \\(x_{ij}\\), with other covariates held fixed.\nPoisson regression\nFor count data,\n\\[\nY_i \\sim \\text{Poisson}(\\mu_i), \\qquad g(\\mu_i) = \\log(\\mu_i).\n\\]\nThe coefficient \\(\\beta_j\\) is interpreted as the log rate ratio, where \\(\\exp(\\beta_j)\\) gives the multiplicative change in expected count for a one-unit increase in \\(x_{ij}\\).\nGaussian regression\nFor continuous responses,\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2), \\qquad g(\\mu_i) = \\mu_i.\n\\]\nThe coefficient \\(\\beta_j\\) represents the expected change in the response for a one-unit increase in \\(x_{ij}\\).\n\nThus, GLMs preserve the linear predictor while flexibly adapting the link and variance structure to suit binary, count, and continuous data.\n\n\n8.4.4 Fitting and Diagnosis\nEstimation in generalized linear models is typically carried out by maximum likelihood. The parameters \\(\\beta\\) are obtained by solving the score equations, which in practice are computed through numerical optimization. A common algorithm is iteratively reweighted least squares (IRLS), which updates coefficient estimates using weighted least squares until convergence. In Python, functions such as statsmodels.api.GLM or sklearn.linear_model.LogisticRegression implement this estimation automatically, providing coefficient estimates along with standard errors and confidence intervals when applicable.\nAfter fitting a GLM, model adequacy should be checked through diagnostics. Residuals such as deviance residuals or Pearson residuals can reveal lack of fit and highlight influential observations. Goodness of fit can also be assessed with deviance statistics, likelihood ratio tests, or pseudo-\\(R^2\\) measures. In Python, statsmodels provides methods like .resid_deviance, .resid_pearson, and influence statistics to assess model fit. Visual inspection through residual plots remains a practical tool to detect systematic deviations from model assumptions.\n\n\n8.4.5 Regularized GLM\nRegularization extends generalized linear models by adding a penalty to the log-likelihood, which stabilizes estimation in high-dimensional settings and enables variable selection. The optimization problem can be formulated as\n\\[\n\\hat{\\beta} = \\arg\\min_{\\beta} \\Big\\{ -\\ell(\\beta) +\n\\lambda P(\\beta) \\Big\\},\n\\]\nwhere \\(\\ell(\\beta)\\) is the log-likelihood, \\(P(\\beta)\\) is a penalty function, and \\(\\lambda\\) is a tuning parameter controlling the strength of shrinkage.\nCommon choices of \\(P(\\beta)\\) include:\n\nRidge (\\(L_2\\)): \\(\\sum_j \\beta_j^2\\).\nLasso (\\(L_1\\)): \\(\\sum_j |\\beta_j|\\).\nElastic Net: \\(\\alpha \\sum_j |\\beta_j| + (1-\\alpha)\\sum_j \\beta_j^2\\).\n\nThe fitting algorithm typically involves coordinate descent or gradient- based optimization methods, which are efficient for large-scale data and sparse solutions. For example, the glmnet algorithm uses cyclical coordinate descent with warm starts.\nSelection of the tuning parameter \\(\\lambda\\) is crucial. A standard approach is cross-validation, where data are split into folds, the model is fitted on training folds for a grid of \\(\\lambda\\) values, and performance is evaluated on validation folds. The \\(\\lambda\\) yielding the lowest prediction error is chosen, sometimes with an additional rule to prefer more parsimonious models (the “one standard error rule”).\nIn Python, sklearn.linear_model.LogisticRegressionCV or sklearn.linear_model.ElasticNetCV implement these ideas, providing automatic cross-validation for regularized GLMs.\nThe general workflow for fitting a regularized logistic regression model is:\n\nDefine predictors and outcome: choose relevant numeric and categorical features, and specify the binary response.\nPreprocess features: standardize numeric predictors with StandardScaler() and encode categorical predictors with OneHotEncoder().\nSet up the pipeline: combine preprocessing with the logistic regression model in a unified workflow.\nFit with cross-validation: use LogisticRegressionCV with lasso (L1) or elastic net penalties. Cross-validation automatically selects the tuning parameter \\(\\lambda\\).\nInspect coefficients: identify which predictors remain with nonzero coefficients, interpreting them as important contributors to the outcome.\nEvaluate performance: measure predictive accuracy or AUC (see chapter on classification) on a held-out test set to assess generalization.\n\nThis structured process ensures stability, interpretability, and good predictive performance when fitting regularized logistic models.\n\n\n8.4.6 Example: Regularized GLM with Ames Housing Data\nWe continue with the processed Ames housing dataset (OpenML id 42165) from earlier in the regression chapter. The task is to predict whether a home is “expensive” (above the median sale price) using selected predictors. We fit a logistic regression model with an \\(L_1\\) (lasso) penalty to enable variable selection.\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import Pipeline\n\n# Binary outcome: 1 if SalePrice &gt; median\nmedian_price = df[\"SalePrice\"].median()\ny = (df[\"SalePrice\"] &gt; median_price).astype(int)\n\n# Logistic regression with L1 penalty and cross-validation\nlogit_lasso_cv = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", LogisticRegressionCV(\n         Cs=20, cv=5, penalty=\"l1\", solver=\"saga\",\n         scoring=\"accuracy\", max_iter=5000,\n         random_state=0\n    ))\n])\n\nlogit_lasso_cv.fit(X, y)\n\n# Extract selected coefficients\nmodel = logit_lasso_cv.named_steps[\"model\"]\nfeature_names = (\n    numeric_features +\n    list(logit_lasso_cv.named_steps[\"preprocessor\"]\n        .named_transformers_[\"cat\"]\n        .get_feature_names_out(categorical_features))\n)\n\nprint(\"Selected coefficients (lasso):\")\nfor name, coef in zip(feature_names, model.coef_[0]):\n    print(f\"{name}: {coef:.4f}\")\n\nprint(\"\\nBest C (inverse of lambda):\", model.C_[0])\n\nSelected coefficients (lasso):\nOverallQual: 1.1998\nGrLivArea: 1.6516\nGarageCars: 0.6534\nTotalBsmtSF: 0.5812\nYearBuilt: 0.8845\nFullBath: 0.1628\nKitchenQual_Fa: -1.2791\nKitchenQual_Gd: -0.0467\nKitchenQual_TA: -0.9568\n\nBest C (inverse of lambda): 1.623776739188721\n\n\nThis example demonstrates how lasso-penalized logistic regression can be used within the GLM framework. The penalty shrinks coefficients toward zero, with some set exactly to zero if uninformative, thereby improving interpretability and predictive stability.\nWe can evaluate classification performance by computing the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Predictions\ny_pred = logit_lasso_cv.predict(X)\n\n# Confusion matrix\ncm = confusion_matrix(y, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\ndisplay_labels=[\"Not Expensive\", \"Expensive\"])\ndisp.plot(cmap=\"Blues\")\n\n\n\n\n\n\n\n\n\n\n\n\nNelder, J. A., & Wedderburn, R. W. M. (1972). Generalized linear models. Journal of the Royal Statistical Society Series A: Statistics in Society, 135(3), 370–384.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Models</span>"
    ]
  },
  {
    "objectID": "09-classification.html",
    "href": "09-classification.html",
    "title": "9  Classification",
    "section": "",
    "text": "9.1 Introduction to Classification\nClassification is one of the most widely used tasks in data science, concerned with predicting categorical outcomes rather than continuous quantities. Many real-world problems can be framed as classification, such as diagnosing a disease from medical records, determining whether a loan applicant is likely to default, or identifying spam emails. Compared with regression, which models numeric responses, classification methods aim to assign observations into predefined classes based on their features. Logistic regression, introduced in the previous chapter, provides a natural transition: it uses a regression framework to model the probability of class membership. In this chapter, we expand beyond logistic regression to study how classification models are evaluated and to introduce other methods developed for classification tasks.\nClassification problems arise when the outcome of interest is categorical rather than continuous. Instead of predicting a numerical quantity, the task is to assign each observation to one of several predefined classes. Examples include deciding whether an email is spam or not, predicting a patient’s disease status from clinical measures, or determining whether a financial transaction is fraudulent. These problems are ubiquitous across domains and often require different tools from those used in regression.\nA widely used dataset for illustrating binary classification is the Breast Cancer Wisconsin (Diagnostic) dataset. It contains information on 569 patients, each described by 30 numerical features computed from digitized images of fine needle aspirates of breast masses. These features summarize characteristics of the cell nuclei, such as radius, texture, perimeter, smoothness, and concavity, with versions capturing mean, variation, and extreme values. The outcome variable records whether the tumor is malignant or benign. Because the features are all numeric and the outcome is binary, this dataset provides an ideal setting for introducing classification methods and performance evaluation.\nBefore building classification models, it is useful to perform exploratory data analysis (EDA) to understand the structure of the data. We first load the dataset from scikit-learn.\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\n# Load dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf[\"diagnosis\"] = data.target\n\n# Map diagnosis: 0 = malignant, 1 = benign\ndf[\"diagnosis\"] = df[\"diagnosis\"].map({0: \"malignant\", 1: \"benign\"})\ndf.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ndiagnosis\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nmalignant\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nmalignant\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nmalignant\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nmalignant\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nmalignant\n\n\n\n\n5 rows × 31 columns\nWe check the class distribution to see whether the dataset is balanced.\ndf[\"diagnosis\"].value_counts()\n\ndiagnosis\nbenign       357\nmalignant    212\nName: count, dtype: int64\nWe can also examine summary statistics of the numeric features.\ndf.describe().T.head(10)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nmean radius\n569.0\n14.127292\n3.524049\n6.98100\n11.70000\n13.37000\n15.78000\n28.11000\n\n\nmean texture\n569.0\n19.289649\n4.301036\n9.71000\n16.17000\n18.84000\n21.80000\n39.28000\n\n\nmean perimeter\n569.0\n91.969033\n24.298981\n43.79000\n75.17000\n86.24000\n104.10000\n188.50000\n\n\nmean area\n569.0\n654.889104\n351.914129\n143.50000\n420.30000\n551.10000\n782.70000\n2501.00000\n\n\nmean smoothness\n569.0\n0.096360\n0.014064\n0.05263\n0.08637\n0.09587\n0.10530\n0.16340\n\n\nmean compactness\n569.0\n0.104341\n0.052813\n0.01938\n0.06492\n0.09263\n0.13040\n0.34540\n\n\nmean concavity\n569.0\n0.088799\n0.079720\n0.00000\n0.02956\n0.06154\n0.13070\n0.42680\n\n\nmean concave points\n569.0\n0.048919\n0.038803\n0.00000\n0.02031\n0.03350\n0.07400\n0.20120\n\n\nmean symmetry\n569.0\n0.181162\n0.027414\n0.10600\n0.16190\n0.17920\n0.19570\n0.30400\n\n\nmean fractal dimension\n569.0\n0.062798\n0.007060\n0.04996\n0.05770\n0.06154\n0.06612\n0.09744\nVisualization helps reveal differences between classes. For example, we can compare the distributions of a few key features by diagnosis.\nfrom plotnine import ggplot, aes, geom_histogram, facet_wrap, labs\n\nfeatures = [\"mean radius\", \"mean texture\", \"mean area\"]\n\nfor feature in features:\n    p = (\n        ggplot(df, aes(x=feature, fill=\"diagnosis\")) \n        + geom_histogram(bins=20, alpha=0.5, position=\"identity\")\n        + labs(title=feature)\n    )\n    p\nThese plots suggest that malignant and benign tumors differ in several features, such as mean radius and mean area. Such separation indicates that classification methods can be effective in distinguishing between the two groups.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "09-classification.html#evaluating-classifiers",
    "href": "09-classification.html#evaluating-classifiers",
    "title": "9  Classification",
    "section": "9.2 Evaluating Classifiers",
    "text": "9.2 Evaluating Classifiers\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\n\n9.2.1 Confusion Matrix\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity or recall.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour other rates with predicted (column) margins:\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n9.2.2 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n9.2.3 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n9.2.4 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n9.2.5 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n9.2.6 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\). The ROC curve always starts at (0, 0) and ends at (1, 1) because these points represent the extreme threshold settings of the classifier. When the threshold is so high that all predictions are negative, both TPR and the TPR are zero—corresponding to the point (0, 0). When the threshold is so low that all predictions are positive, both TPR and FPR are one—corresponding to the point (1, 1).\nBest classification passes \\((0, 1)\\). The ideal classifier would achieve a TPR of 1 while keeping the FPR at 0. This corresponds to the point (0, 1) in the ROC space. In practice, the closer a classifier’s ROC curve approaches this top-left corner, the better its discriminative performance.\nClassification by random guess gives the 45-degree line. For every threshold, the TPR equals the FPR, because the classifier is just as likely to label a negative instance as positive as it is to label a positive instance correctly. Thus, its points fall on the line where TPR = FPR. This diagonal serves as a baseline: a model whose ROC curve lies on or below this line has no discriminative ability, equivalent to random guessing. Any useful classifier should produce a curve that bows above the diagonal, showing higher TPRs than FPRs across thresholds. The greater the area between the ROC curve and the diagonal, the more informative the model.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results. A higher AUC indicates that the model ranks positive instances higher than negative ones more consistently. Thus, a larger AUC reflects stronger separability between the classes and a more powerful classifier. An AUC of 1 means perfect discrimination, whereas an AUC of 0.5 means random guessing.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n9.2.7 Breast Cancer Example\nSince logistic regression provides a natural starting point for classification, we will apply it to the breast cancer data using a subset of features for simplicity to illustrate the metrics.\nThe response variable y is a binary array indicating whether a tumor is malignant or benign.\n\ny = 0 corresponds to malignant (cancerous) tumors.\ny = 1 corresponds to benign (non-cancerous) tumors.\n\nThis encoding follows scikit-learn’s convention for the Wisconsin Diagnostic Breast Cancer dataset. The dataset includes 569 samples with 30 numeric features derived from digitized images of fine needle aspirate biopsies.\nWhen fitting a logistic regression model, the predicted probabilities (y_pred_prob) represent the estimated probability that a tumor is benign (1). Consequently, high predicted probabilities correspond to benign cases, and low probabilities indicate malignant ones. The ROC and AUC computations use these probabilities as scores for the positive class (benign).\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nX = df[[\"mean radius\", \"mean texture\", \"mean area\"]]\ny = df[\"diagnosis\"].map({\"malignant\":0, \"benign\":1})\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nmodel = LogisticRegression(max_iter=500)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix\n\narray([[ 52,  11],\n       [  5, 103]])\n\n\nWe can compute accuracy, precision, recall, and F1-score to evaluate performance.\n\nprint(classification_report(y_test, y_pred,\n                                target_names=[\"malignant\",\"benign\"]))\n\n              precision    recall  f1-score   support\n\n   malignant       0.91      0.83      0.87        63\n      benign       0.90      0.95      0.93       108\n\n    accuracy                           0.91       171\n   macro avg       0.91      0.89      0.90       171\nweighted avg       0.91      0.91      0.91       171\n\n\n\n\nMalignant tumors: Precision of 0.91 means that 91% of tumors predicted malignant were truly malignant. Recall of 0.83 shows that the model correctly identified 83% of actual malignant tumors but missed 17% (false negatives). The F1-score of 0.87 balances these two aspects.\nBenign tumors: Precision of 0.90 means 90% of predicted benign tumors were correct. Recall of 0.95 shows the model caught 95% of actual benign tumors, misclassifying only 5% as malignant. The F1-score of 0.93 reflects this strong performance.\nOverall: The accuracy of 0.91 indicates that about 91% of tumors were classified correctly. The macro average (simple mean across classes) is slightly lower than the weighted average, reflecting the imbalance in sample sizes. Since benign cases are more common, the weighted average leans closer to their stronger performance.\n\nThe model seems quite accurate overall, but performs better at identifying benign tumors than malignant ones. The relatively lower recall for malignant cases means that some malignant tumors were misclassified as benign. In medical applications, such false negatives are especially serious and motivate the use of evaluation metrics beyond accuracy alone.\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, geom_abline, labs, theme_minimal\n\n# Assume model and data are from previous logistic regression example\n# y_test: true labels (0/1)\n# y_pred_prob: predicted probabilities for the positive class\ny_pred_prob = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc_value = roc_auc_score(y_test, y_pred_prob)\n\n# Create ROC DataFrame\nroc_df = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n})\n\n# Plot ROC curve\n(\n    ggplot(roc_df, aes(x='False Positive Rate', y='True Positive Rate')) +\n    geom_line(color='blue') +\n    geom_abline(linetype='dashed') +\n    labs(\n        title=f'ROC Curve (AUC = {auc_value:.2f})',\n        x='False Positive Rate (1 - Specificity)',\n        y='True Positive Rate (Sensitivity)'\n    ) +\n    theme_minimal()\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "09-classification.html#tuning-regularized-logistic-models",
    "href": "09-classification.html#tuning-regularized-logistic-models",
    "title": "9  Classification",
    "section": "9.3 Tuning Regularized Logistic Models",
    "text": "9.3 Tuning Regularized Logistic Models\nThe logistic regression model with an L1 (lasso) penalty requires a tuning parameter controlling the strength of regularization. In scikit-learn, this parameter is expressed as \\(C = 1 / \\lambda\\). Smaller \\(C\\) values correspond to stronger penalties, shrinking more coefficients toward zero. The goal is to select \\(C\\) that optimizes a performance metric such as F1 or AUC via cross-validation.\n\nStep 1: Data and Setup\n\nWe continue with the breast cancer dataset.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, labs, scale_x_log10, theme_minimal\n\n# Load the data\nX, y = load_breast_cancer(return_X_y=True)\n\n\nStep 2: Automatic Selection of Candidate C Values\n\nRather than manually picking a grid, LogisticRegressionCV determines a data- dependent range of \\(C\\) values by examining the scale of the coefficients. It constructs a grid of regularization strengths internally based on the variance of the features and outcome, ensuring coverage from under-regularized to over- regularized regimes.\n\n# Use LogisticRegressionCV to determine reasonable C values\nauto_lasso = LogisticRegressionCV(\n    Cs=20,\n    penalty=\"l1\",\n    solver=\"saga\",\n    cv=5,\n    scoring=\"roc_auc\",\n    max_iter=5000,\n    n_jobs=-1\n)\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", auto_lasso)\n])\n\npipeline.fit(X, y)\n\n## set tested Cs\nC_values = np.logspace(-2, 1, 20)\n## C_values = auto_lasso.Cs_        \n\n\nStep 3: Cross-Validation for F1 and AUC\n\nWhile LogisticRegressionCV provides built-in AUC-based selection, we can also evaluate each \\(C\\) value using different metrics such as F1-score.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Evaluate across automatically chosen C values\nf1_scores = []\nauc_scores = []\nfor C in C_values:\n    model = LogisticRegression(\n        penalty=\"l1\", solver=\"saga\", C=C, max_iter=5000\n    )\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", model)\n    ])\n    f1_scores.append(cross_val_score(pipe, X, y, cv=cv, scoring=\"f1\").mean())\n    auc_scores.append(cross_val_score(pipe, X, y, cv=cv, scoring=\"roc_auc\").mean())\n\ncv_results = pd.DataFrame({\"C\": C_values, \"F1\": f1_scores, \"AUC\": auc_scores})\n\n\nStep 4: Identify the Optimal Regularization Strength\n\n\nbest_f1 = cv_results.loc[cv_results[\"F1\"].idxmax()]\nbest_auc = cv_results.loc[cv_results[\"AUC\"].idxmax()]\n\nprint(\"Best by F1:\", best_f1)\nprint(\"Best by AUC:\", best_auc)\n\nBest by F1: C      1.623777\nF1     0.979323\nAUC    0.994523\nName: 14, dtype: float64\nBest by AUC: C      0.784760\nF1     0.976486\nAUC    0.995644\nName: 12, dtype: float64\n\n\n\nStep 5: Visualization\n\n\n(\n    ggplot(cv_results.melt(id_vars=\"C\", var_name=\"Metric\", value_name=\"Score\"),\n           aes(x=\"C\", y=\"Score\", color=\"Metric\"))\n    + geom_line()\n    + scale_x_log10()\n    + labs(\n        title=\"Cross-Validation for Lasso Logistic Regression\",\n        x=\"C (Inverse Regularization Strength)\",\n        y=\"Mean 5-Fold Score\"\n    )\n    + theme_minimal()\n)\n\n\n\n\n\n\n\n\nThis approach avoids arbitrary grids. The smallest \\(C\\) corresponds to the strongest regularization (the simplest model), while the largest \\(C\\) allows almost no penalty (the most flexible model). The optimal \\(C\\) often lies between these extremes. A higher AUC indicates better class separation, whereas a higher F1-score indicates balanced precision and recall. Depending on the application, one metric may be prioritized over the other.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "10-supervised.html",
    "href": "10-supervised.html",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1 Decision Trees\nDecision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed (Breiman et al., 1984). Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#decision-trees",
    "href": "10-supervised.html#decision-trees",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1.1 Recursive Partition Algorithm\nThe core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node \\(m\\), the data subset is denoted as \\(Q_m\\) with a sample size of \\(n_m\\). The objective is to find a candidate split \\(\\theta\\), defined as a threshold for a given feature, that minimizes an impurity or loss measure \\(H\\).\nWhen a split is made at node \\(m\\), the data is divided into two subsets: \\(Q_{m,l}\\) (left node) with sample size \\(n_{m,l}\\), and \\(Q_{m,r}\\) (right node) with sample size \\(n_{m,r}\\). The split quality, measured by \\(G(Q_m, \\theta)\\), is given by:\n\\[\nG(Q_m, \\theta) = \\frac{n_{m,l}}{n_m} H(Q_{m,l}(\\theta)) +\n\\frac{n_{m,r}}{n_m} H(Q_{m,r}(\\theta)).\n\\]\nThe algorithm aims to identify the split that minimizes the impurity:\n\\[\n\\theta^* = \\arg\\min_{\\theta} G(Q_m, \\theta).\n\\]\nThis process is applied recursively at each child node until a stopping condition is met.\n\nStopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.\nPruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves generalization.\n\n\n\n10.1.2 Search Space for Possible Splits\nAt each node, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the feature values. For a given feature, the algorithm considers each of its unique value in the current node as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring effective partition.\nFormally, let the feature set be \\(\\{X_1, X_2, \\ldots, X_p\\}\\), where \\(p\\) is the total number of features, and let the unique values of feature \\(X_j\\) at node \\(m\\) be denoted by \\(\\{v_{m,j,1}, v_{m,j,2}, \\ldots, v_{m,j,k_{mj}}\\}\\). The search space at node \\(m\\) includes:\n\nFeature candidates: \\(\\{X_1, X_2, \\ldots, X_p\\}\\).\nThreshold candidates for \\(X_j\\): \\[\n\\left\\{ \\frac{v_{m,j,i} + v_{m,j,i+1}}{2} \\mid 1 \\leq i &lt; k_{mj} \\right\\}.\n\\]\n\nWhile the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.\n\n\n10.1.3 Metrics\n\n10.1.3.1 Classification\nIn classification, the split quality metric measures how pure the resulting nodes are after a split. A pure node contains observations that predominantly belong to a single class.\n\nGini Index: The Gini index measures node impurity by the probability that two observations randomly drawn from the node belong to different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node \\(m\\), the Gini index is \\[\nH(Q_m) = \\sum_{k=1}^{K} p_{mk} (1 - p_{mk})\n= 1 - \\sum_{k=1}^n p_{mk}^2,\n\\] where \\(p_{mk}\\) is the proportion of samples of class \\(k\\) at node \\(m\\); and \\(K\\) is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including sklearn.\nThe Gini index originates from the Gini coefficient, introduced by Corrado Gini in 1912 to quantify inequality in income distributions. In that context, the Gini coefficient measures how unevenly a quantity (such as wealth) is distributed across a population. Decision tree algorithms adapt this concept of inequality to measure the impurity of a node: instead of wealth, the distribution concerns class membership. A perfectly pure node, where all observations belong to the same class, represents complete equality and yields a Gini index of zero. As class proportions become more mixed, inequality in class membership increases, leading to higher impurity values. Thus, the Gini index used in decision trees can be viewed as a statistical measure of diversity or heterogeneity derived from Gini’s original work on inequality.\nEntropy (Information Gain): Derived from information theory, entropy quantifies the disorder of the data at a node. Lower entropy means higher purity. At node \\(m\\), it is defined as \\[\nH(Q_m) = - \\sum_{k=1}^{K} p_{mk} \\log p_{mk}.\n\\] Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.\nMisclassification Error: Misclassification error focuses on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node \\(m\\), it is defined as \\[\nH(Q_m) = 1 - \\max_k p_{mk},\n\\] where \\(\\max_k p_{mk}\\) is the largest proportion of samples belonging to any class \\(k\\).\n\n\n\n10.1.3.2 Regression Criteria\nIn regression, the goal is to minimize the spread or variance of the target variable within each node.\n\nMean Squared Error (MSE): MSE is the average squared difference between observed and predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} (y_i - \\bar{y}_m)^2,\n\\] where\n\n\\(y_i\\) is the actual value for sample \\(i\\);\n\\(\\bar{y}_m\\) is the mean value of the target at node \\(m\\);\n\\(n_m\\) is the number of samples at node \\(m\\).\n\nMSE works well when the target is continuous and normally distributed.\nHalf Poisson Deviance: Used for count target, the Poisson deviance measures the variance in the number of occurrences of an event. At node \\(m\\), it is \\[\nH(Q_m) = \\sum_{i=1}^{n_m} \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right),\n\\] where \\(\\hat{y}_i\\) is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.\nMean Absolute Error (MAE): MAE aims to minimize the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} |y_i - \\bar{y}_m|.\n\\] MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.\n\n\n\n\n10.1.4 Ames Housing Example\nThe Ames Housing data are used to illustrate a regression tree model for predicting log house price.\nAs before, we retrive the data from OpenML.\n\nimport openml\nimport pandas as pd\nimport numpy as np\n\n# Load Ames Housing dataset (OpenML ID 42165)\ndataset = openml.datasets.get_dataset(42165)\ndf, *_ = dataset.get_data()\ndf[\"LogPrice\"] = np.log(df[\"SalePrice\"])\n\nA decision tree partitions the feature space into regions where the average log price is relatively constant.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nimport plotnine as gg\nimport pandas as pd\n\nnumeric_features = [\n    \"OverallQual\", \"GrLivArea\", \"GarageCars\",\n    \"TotalBsmtSF\", \"YearBuilt\", \"FullBath\"\n]\ncategorical_features = [\"KitchenQual\"]\n\npreprocessor = ColumnTransformer([\n    (\"num\", \"passthrough\", numeric_features),\n    (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n])\n\nX = df[numeric_features + categorical_features]\ny = df[\"LogPrice\"]\n\ndepths = range(2, 11)\ncv_scores = [\n    cross_val_score(\n        Pipeline([\n            (\"pre\", preprocessor),\n            (\"model\", DecisionTreeRegressor(max_depth=d, random_state=0))\n        ]),\n        X, y, cv=5, scoring=\"r2\"\n    ).mean()\n    for d in depths\n]\n\nlist(zip(depths, cv_scores))\n\n[(2, np.float64(0.594134995704976)),\n (3, np.float64(0.6712972027857058)),\n (4, np.float64(0.7141792234890973)),\n (5, np.float64(0.748794485599919)),\n (6, np.float64(0.7587964739851765)),\n (7, np.float64(0.7343953839481492)),\n (8, np.float64(0.7186324525934304)),\n (9, np.float64(0.7112790937242873)),\n (10, np.float64(0.6938966980572193))]\n\n\nCross-validation identifies an appropriate tree depth that balances fit and generalization. A too-deep tree overfits, while a shallow tree misses structure.\n\ndt = Pipeline([\n    (\"pre\", preprocessor),\n    (\"model\", DecisionTreeRegressor(max_depth=4, random_state=0))\n])\n\ndt.fit(X, y)\ny_pred = dt.predict(X)\n\ndf_pred = pd.DataFrame({\"Observed\": y, \"Predicted\": y_pred})\n\n(gg.ggplot(df_pred, gg.aes(x=\"Observed\", y=\"Predicted\")) +\n gg.geom_point(alpha=0.5) +\n gg.geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n gg.labs(title=\"Decision Tree Regression on Ames Housing\",\n         x=\"Observed Log Price\", y=\"Predicted Log Price\"))\n\n\n\n\n\n\n\n\nThe plot shows predicted versus observed log prices. A well-fitted model has points close to the diagonal. The decision tree naturally captures nonlinear effects and interactions, though its predictions are piecewise constant, producing visible step patterns.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "10-supervised.html#gradient-boosted-models",
    "href": "10-supervised.html#gradient-boosted-models",
    "title": "10  Supervised Learning",
    "section": "10.2 Gradient-Boosted Models",
    "text": "10.2 Gradient-Boosted Models\nGradient boosting is a powerful ensemble technique in machine learning that combines multiple weak learners into a strong predictive model. Unlike bagging methods, which train models independently, gradient boosting fits models sequentially, with each new model correcting errors made by the previous ensemble (Friedman, 2001). While decision trees are commonly used as weak learners, gradient boosting can be generalized to other base models. This iterative method optimizes a specified loss function by repeatedly adding models designed to reduce residual errors.\n\n10.2.1 Introduction\nGradient boosting builds on the general concept of boosting, aiming to construct a strong predictor from an ensemble of sequentially trained weak learners. The weak learners are often shallow decision trees (stumps), linear models, or generalized additive models (Hastie et al., 2009). Each iteration adds a new learner focusing primarily on the data points poorly predicted by the existing ensemble, thereby progressively enhancing predictive accuracy.\nGradient boosting’s effectiveness stems from:\n\nError Correction: Each iteration specifically targets previous errors, refining predictive accuracy.\nWeighted Learning: Iteratively focuses more heavily on difficult-to-predict data points.\nFlexibility: Capable of handling diverse loss functions and various types of predictive tasks.\n\nThe effectiveness of gradient-boosted models has made them popular across diverse tasks, including classification, regression, and ranking. Gradient boosting forms the foundation for algorithms such as XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018), known for their high performance and scalability.\n\n\n10.2.2 Gradient Boosting Process\nGradient boosting builds an ensemble by iteratively minimizing the residual errors from previous models. This iterative approach optimizes a loss function, \\(L(y, F(x))\\), where \\(y\\) represents the observed target variable and \\(F(x)\\) the model’s prediction for a given feature vector \\(x\\).\nKey concepts:\n\nLoss Function: Guides model optimization, such as squared error for regression or logistic loss for classification.\nLearning Rate: Controls incremental updates, balancing training speed and generalization.\nRegularization: Reduces overfitting through tree depth limitation, subsampling, and L1/L2 penalties.\n\n\n10.2.2.1 Model Iteration\nThe gradient boosting algorithm proceeds as follows:\n\nInitialization: Define a base model \\(F_0(x)\\), typically the mean of the target variable for regression or the log-odds for classification.\nIterative Boosting: At each iteration \\(m\\):\n\nCompute pseudo-residuals representing the negative gradient of the loss function at the current predictions. For each observation \\(i\\): \\[\nr_i^{(m)} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F(x)=F_{m-1}(x)},\n\\] where \\(x_i\\) and \\(y_i\\) denote the feature vector and observed value for the \\(i\\)-th observation, respectively.\nFit a new weak learner \\(h_m(x)\\) to these residuals.\nUpdate the model: \\[\nF_m(x) = F_{m-1}(x) + \\eta \\, h_m(x),\n\\] where \\(\\eta\\) is a small positive learning rate (e.g., 0.01–0.1), controlling incremental improvement and reducing overfitting.\n\nFinal Model: After \\(M\\) iterations, the ensemble model is: \\[\n  F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta \\, h_m(x).\n  \\]\n\nStochastic gradient boosting is a variant that enhances gradient boosting by introducing randomness through subsampling at each iteration, selecting a random fraction of data points (typically 50%–80%) to fit the model (Friedman, 2002). This randomness helps reduce correlation among trees, improve model robustness, and reduce the risk of overfitting.\n\n\n\n10.2.3 Demonstration\n\n\n10.2.4 XGBoost: Extreme Gradient Boosting\nXGBoost is a scalable and efficient implementation of gradient-boosted decision trees (Chen & Guestrin, 2016). It has become one of the most widely used machine learning methods for structured data due to its high predictive performance, regularization capabilities, and speed. XGBoost builds an ensemble of decision trees in a stage-wise fashion, minimizing a regularized objective that balances training loss and model complexity.\nThe core idea of XGBoost is to fit each new tree to the gradient of the loss function with respect to the model’s predictions. Unlike traditional boosting algorithms like AdaBoost, which use only first-order gradients, XGBoost optionally uses second-order derivatives (Hessians), enabling better convergence and stability (Friedman, 2001).\nXGBoost is widely used in data science competitions and real-world applications. It supports regularization (L1 and L2), handles missing values internally, and is designed for distributed computing.\nXGBoost builds upon the same foundational idea as gradient boosted machines—sequentially adding trees to improve the predictive model— but introduces a number of enhancements:\n\n\n\n\n\n\n\n\nAspect\nTraditional GBM\nXGBoost\n\n\n\n\nImplementation\nBasic gradient boosting\nOptimized, regularized boosting\n\n\nRegularization\nShrinkage only\nL1 and L2 regularization\n\n\nLoss Optimization\nFirst-order gradients\nFirst- and second-order\n\n\nMissing Data\nRequires manual imputation\nHandled automatically\n\n\nTree Construction\nDepth-wise\nLevel-wise (faster)\n\n\nParallelization\nLimited\nBuilt-in\n\n\nSparsity Handling\nNo\nYes\n\n\nObjective Functions\nFew options\nCustom supported\n\n\nCross-validation\nExternal via GridSearchCV\nBuilt-in xgb.cv\n\n\n\nXGBoost is therefore more suitable for large-scale problems and provides better generalization performance in many practical tasks.\n\n\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984). Classification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5), 1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., & Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. Advances in Neural Information Processing Systems, 3146–3154.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. Advances in Neural Information Processing Systems, 6638–6648.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "11  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Positron, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a HTML.\nPrint the HTML file to a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhy is the command line interface if preferred among the professionals?\nWhat are the advantages of Linux over Windows?\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\nWhy do I require at least 10 commits for each assignment?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\nExplain in words which strategy is preferred. Use analytic results to help if you could derive them.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\nTest your function with \\((1, 3, 9, 10)\\), \\((4, 4, 10, 10)\\), \\((1, 5, 5, 5)\\), \\((3, 3, 7, 7)\\), \\((3, 3, 8, 8)\\), and $(1, 4, 5, 6).\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. The data of the Labor Day week, 2025, was downloaded by filtering with CRASH Date in between 11:59:59 PM August 30 and 12:00:00 AM September 7, available in data/nyc_crashes_lbdwk_2025.csv.\n\nRead the data into a Pandas data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data curator, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration In the cleaned crash data in feather format (thank Wilson Tang for his work, we have a variable zip_filled which is true if, in the raw data, a crash has nonmissing geocode but missing zip code, which was filled during the cleaning process. This variable could be viewed as a data quality metric.\n\nConstruct a contigency table for zip_filled and borough. Is the pattern of fillable zips the same across boroughs? Formulate a hypothesis and test it.\nConstruct an hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash Severity Modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Noise complaints in NYC The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with requests to NYPD on noise complaints that are created from 12:00:00 AM 06/29/2025 to 12:00:00 PM 07/05/2025. The subset is available in CSV format as data/nypd311w062925noise_by100625.csv. Read the data dictionary online to understand the meaning of the variables. For a clean workflow, this data is not to be uploaded to your GitHub repo. You will lose style points if you do. Instead, it should be placed in an untracked local folder data, so your analysis is reproducible as long as I have the same folder on my computer.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Justify and clean them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second; action_update_date after closed_date.\nSummarize your suggestions to the data curator in several bullet points.\n\nData exploration.\n\nIf we suspect that response time may depend on the time of day when a complaint is made, we can compare the response times for complaints submitted during nighttime and daytime. To do this, we can visualize the comparison by complaint type, borough, and weekday (vs weekend/holiday).\nPerform a formal hypothesis test to confirm the observations from your visualization. Formally state your hypotheses and summarize your conclusions in plain English.\nCreate a binary variable over2h to indicate that a service request took two hours or longer to close.\nDoes over2h depend on the complaint type, borough, or weekday (vs weekend/holiday)? State your hypotheses and summarize your conclusions in plain English.\n\nData analysis.\n\nThe addresses of NYC police precincts are stored in data/nypd_precincts.csv. Use geocoding tools to find their geocode (longitude and latitude) from the addresses.\nCreate a variable dist2pp which represent the distance from each request incidence to the nearest police precinct.\nAdd zip code level variables by merging with data from US Census.\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over2h for the noise complaints with the training data, using all the variables you can engineer from the available data. If you have tuning parameters, justify how they were selected.\nAssess the performance of your model in terms of commonly used metrics. Summarize your results to a New Yorker who is not data science savvy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\nAmerican Statistical Association (ASA). (2018). Ethical guidelines\nfor statistical practice.\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984).\nClassification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting\nsystem. Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nDe Cock, D. (2009). Ames, Iowa: Alternative to the\nBoston housing data as an end of semester regression\nproject. Journal of Statistics Education, 17(3), 1–13.\nhttps://doi.org/10.1080/10691898.2009.11889627\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient\nboosting machine. The Annals of Statistics, 29(5),\n1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational\nStatistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference, and prediction.\nSpringer.\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &\nLiu, T.-Y. (2017). LightGBM: A highly efficient gradient\nboosting decision tree. Advances in Neural Information Processing\nSystems, 3146–3154.\n\n\nNelder, J. A., & Wedderburn, R. W. M. (1972). Generalized linear\nmodels. Journal of the Royal Statistical Society Series A:\nStatistics in Society, 135(3), 370–384.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin,\nA. (2018). CatBoost: Unbiased boosting with categorical features.\nAdvances in Neural Information Processing Systems, 6638–6648.\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The\npractice of responsible data analysis and decision making. MIT\nPress.",
    "crumbs": [
      "References"
    ]
  }
]